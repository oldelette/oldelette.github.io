<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Kubectl - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{background:#fff;color:#333;display:block;overflow-x:auto;padding:.5em}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{background-color:#eaffea;color:#55a532}.hljs-deletion{background-color:#ffecec;color:#bd2c00}.hljs-link{text-decoration:underline}.markdown-body{word-wrap:break-word;font-size:16px;line-height:1.5}.markdown-body:after,.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;line-height:1;margin-left:-20px;padding-right:4px}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-bottom:16px;margin-top:0}.markdown-body hr{background-color:#e7e7e7;border:0;height:.25em;margin:24px 0;padding:0}.markdown-body blockquote{border-left:.25em solid #ddd;color:#777;font-size:16px;padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{background-color:#fcfcfc;border:1px solid;border-color:#ccc #ccc #bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb;color:#555;display:inline-block;font-size:11px;line-height:10px;padding:3px 5px;vertical-align:middle}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{font-weight:600;line-height:1.25;margin-bottom:16px;margin-top:24px}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{border-bottom:1px solid #eee;padding-bottom:.3em}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:#777;font-size:.85em}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{list-style-type:none;padding:0}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-bottom:0;margin-top:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{font-size:1em;font-style:italic;font-weight:700;margin-top:16px;padding:0}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{display:block;overflow:auto;width:100%;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{border:1px solid #ddd;padding:6px 13px}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{background-color:#fff;box-sizing:initial;max-width:100%}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{background-color:initial;max-width:none;vertical-align:text-top}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{border:1px solid #ddd;display:block;float:left;margin:13px 0 0;overflow:hidden;padding:7px;width:auto}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{clear:both;color:#333;display:block;padding:5px 0 0}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{background-color:#0000000a;border-radius:3px;font-size:85%;margin:0;padding:.2em 0}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{content:"\00a0";letter-spacing:-.2em}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{background:#0000;border:0;font-size:100%;margin:0;padding:0;white-space:pre;word-break:normal}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{background-color:#f7f7f7;border-radius:3px;font-size:85%;line-height:1.45;overflow:auto;padding:16px}.markdown-body pre code,.markdown-body pre tt{word-wrap:normal;background-color:initial;border:0;display:inline;line-height:inherit;margin:0;max-width:auto;overflow:visible;padding:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{font-size:12px;line-height:1;overflow:hidden;padding:5px;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{background:#fff;border:0;padding:10px 8px 9px;text-align:right}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:#f8f8f8;border-top:0;font-weight:700}.news .alert .markdown-body blockquote{border:0;padding:0 0 0 40px}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{cursor:default!important;float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle}.markdown-body{max-width:758px;overflow:visible!important;padding-bottom:40px;padding-top:40px;position:relative}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{border-right:3px solid #6ce26c!important;box-sizing:initial;color:#afafaf!important;cursor:default;display:inline-block;min-width:20px;padding:0 8px 0 0;position:relative;text-align:right;z-index:4}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-bottom:none;border-left:none;border-top:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-collapse:inherit!important;border-spacing:0}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{margin-bottom:unset;overflow:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p:last-child{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{background-color:inherit;border-radius:0;overflow:visible;text-align:center;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{height:100%;max-width:100%}.markdown-body pre>code.wrap{word-wrap:break-word;white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap}.markdown-body .alert>p:last-child,.markdown-body .alert>ul:last-child{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{background-color:#000;background-position:50%;background-repeat:no-repeat;background-size:contain;cursor:pointer;display:table;overflow:hidden;text-align:center}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{object-fit:contain;width:100%;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{height:100%;left:0;position:absolute;top:0;width:100%}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{color:#fff;height:auto;left:50%;opacity:.3;position:absolute;top:50%;transform:translate(-50%,-50%);transition:opacity .2s;width:auto;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{bottom:0;height:100%;left:0;position:absolute;right:0;top:0;width:100%}.figma{display:table;padding-bottom:56.25%;position:relative;width:100%}.figma iframe{border:1px solid #eee;bottom:0;height:100%;left:0;position:absolute;right:0;top:0;width:100%}.markmap-container{height:300px}.markmap-container>svg{height:100%;width:100%}.MJX_Assistive_MathML{display:none}#MathJax_Message{z-index:1000!important}.ui-infobar{color:#777;margin:25px auto -25px;max-width:760px;position:relative;z-index:2}.toc .invisable-node{list-style-type:none}.ui-toc{bottom:20px;position:fixed;z-index:998}.ui-toc.both-mode{margin-left:8px}.ui-toc.both-mode .ui-toc-label{border-bottom-left-radius:0;border-top-left-radius:0;height:40px;padding:10px 4px}.ui-toc-label{background-color:#e6e6e6;border:none;color:#868686;transition:opacity .2s}.ui-toc .open .ui-toc-label{color:#fff;opacity:1;transition:opacity .2s}.ui-toc-label:focus{background-color:#ccc;color:#000;opacity:.3}.ui-toc-label:hover{background-color:#ccc;opacity:1;transition:opacity .2s}.ui-toc-dropdown{margin-bottom:20px;margin-top:20px;max-height:70vh;max-width:45vw;overflow:auto;padding-left:10px;padding-right:10px;text-align:inherit;width:25vw}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{letter-spacing:.0029em;padding-right:0}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{color:#767676;display:block;font-size:13px;font-weight:500;padding:4px 20px}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{background-color:initial;border-left:1px solid #000;color:#000;padding-left:19px;text-decoration:none}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{border-left:none;border-right:1px solid #000;padding-right:19px}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{background-color:initial;border-left:2px solid #000;color:#000;font-weight:700;padding-left:18px}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{border-left:none;border-right:2px solid #000;padding-right:18px}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{font-size:12px;font-weight:400;padding-bottom:1px;padding-left:30px;padding-top:1px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{font-size:12px;font-weight:400;padding-bottom:1px;padding-left:40px;padding-top:1px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{font-weight:500;padding-left:28px}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{font-weight:500;padding-left:38px}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{max-height:70vh;max-width:15vw;overflow:auto;position:fixed;top:0}.back-to-top,.expand-toggle,.go-to-bottom{color:#999;display:block;font-size:12px;font-weight:500;margin-left:10px;margin-top:10px;padding:4px 10px}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{background-position:50%;background-repeat:no-repeat;background-size:cover;border-radius:50%;display:block;height:20px;margin-bottom:2px;margin-right:5px;margin-top:2px;width:20px}.ui-user-icon.small{display:inline-block;height:18px;margin:0 0 .2em;vertical-align:middle;width:18px}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-more-info{color:#888;cursor:pointer;vertical-align:middle}.ui-more-info .fa{font-size:16px}.ui-connectedGithub,.ui-published-note{color:#888}.ui-connectedGithub{line-height:23px;white-space:nowrap}.ui-connectedGithub a.file-path{color:#888;padding-left:22px;text-decoration:none}.ui-connectedGithub a.file-path:active,.ui-connectedGithub a.file-path:hover{color:#888;text-decoration:underline}.ui-connectedGithub .fa{font-size:20px}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}.selectable{-webkit-user-select:text;-o-user-select:text;user-select:text}.inline-spoiler-section{cursor:pointer}.inline-spoiler-section .spoiler-text{background-color:#333;border-radius:2px}.inline-spoiler-section .spoiler-text>*{opacity:0}.inline-spoiler-section .spoiler-img{filter:blur(10px)}.inline-spoiler-section.raw{background-color:#333;border-radius:2px}.inline-spoiler-section.raw>*{opacity:0}.inline-spoiler-section.unveil{cursor:auto}.inline-spoiler-section.unveil .spoiler-text{background-color:#3333331a}.inline-spoiler-section.unveil .spoiler-text>*{opacity:1}.inline-spoiler-section.unveil .spoiler-img{filter:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{color:#222;position:relative;z-index:1}.markdown-body.slides:before{background-color:currentColor;bottom:0;box-shadow:0 0 0 50vw;content:"";display:block;left:0;position:absolute;right:0;top:0;z-index:-1}.markdown-body.slides section[data-markdown]{background-color:#fff;margin-bottom:1.5em;position:relative;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{left:1em;max-height:100%;overflow:hidden;position:absolute;right:1em;top:50%;transform:translateY(-50%)}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{border:3px solid #777;content:"";height:1.5em;position:absolute;right:1em;top:-1.5em}.site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif}html[lang=zh-tw] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}svg{text-shadow:none}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true"><h6 id="tags-Work" data-id="tags-Work"><a class="anchor hidden-xs" href="#tags-Work" title="tags-Work"><span class="octicon octicon-link"></span></a><span>tags: </span><code>Work</code></h6><h1 id="Kubectl" data-id="Kubectl"><a class="anchor hidden-xs" href="#Kubectl" title="Kubectl"><span class="octicon octicon-link"></span></a><span>Kubectl</span></h1><p><a href="https://ithelp.ithome.com.tw/articles/10234562" target="_blank" rel="noopener"><span>從題目中學習k8s</span></a></p><h2 id="Tips" data-id="Tips"><a class="anchor hidden-xs" href="#Tips" title="Tips"><span class="octicon octicon-link"></span></a><span>Tips</span></h2><p><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14937836#overview" target="_blank" rel="noopener"><span>Certification Tip!</span></a><span>  - 需登入 udemy</span><br>
<a href="https://kubernetes.io/docs/reference/kubectl/conventions/" target="_blank" rel="noopener"><span>kubectl Usage Conventions</span></a></p><h3 id="查看-K8s-所有物件和它們的縮寫" data-id="查看-K8s-所有物件和它們的縮寫"><a class="anchor hidden-xs" href="#查看-K8s-所有物件和它們的縮寫" title="查看-K8s-所有物件和它們的縮寫"><span class="octicon octicon-link"></span></a><span>查看 K8s 所有物件和它們的縮寫</span></h3><div class="alert alert-info">
<p><span>kubectl api-resources</span></p>
</div><h3 id="Pod-amp-yaml" data-id="Pod-amp-yaml"><a class="anchor hidden-xs" href="#Pod-amp-yaml" title="Pod-amp-yaml"><span class="octicon octicon-link"></span></a><span>Pod &amp; yaml</span></h3><ul>
<li><span>創建出一個基本YAML，再去修改</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl run nginx-kusc00101 --image=nginx --restart=Never --dry-run=client -o yaml&gt; xxx.pod</span></p>
</div><p><span>–dry-run=client 參數用來預覽 而不會真正提交到 cluster 集群中</span></p><p><ins><span>–dry-run</span></ins><span>: By default as soon as the command is run, the resource will be created.</span><br>
<span>If you simply want to test your command , use the </span><ins><span>–dry-run=client</span></ins><span> option.</span><br>
<span>This will not create the resource, instead, tell you whether the resource can be created and if your command is right.</span></p><p><ins><span>-o yaml</span></ins><span>: This will output the resource definition in YAML format on screen.</span></p><h3 id="Deployment" data-id="Deployment"><a class="anchor hidden-xs" href="#Deployment" title="Deployment"><span class="octicon octicon-link"></span></a><span>Deployment</span></h3><ul>
<li><span>Create a deployment</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl create deployment --image=nginx nginx</span></p>
</div><ul>
<li><span>Generate Deployment YAML file (-o yaml). Don’t create it (–dry-run)</span><br>
<span>Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run) with 4 Replicas (–replicas=4)</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml</span><br>
<span>kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml &gt; nginx-deployment.yaml</span></p>
</div><h3 id="Service" data-id="Service"><a class="anchor hidden-xs" href="#Service" title="Service"><span class="octicon octicon-link"></span></a><span>Service</span></h3><ul>
<li><span>將Pod expose出去 (創建一個Service)</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl expose po &lt;pod-name&gt; --type=NodePort --name=&lt;svc-name&gt; --port=80</span><br>
<span>kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml</span></p>
</div><h2 id="Core-Concepts" data-id="Core-Concepts"><a class="anchor hidden-xs" href="#Core-Concepts" title="Core-Concepts"><span class="octicon octicon-link"></span></a><span>Core Concepts</span></h2><h3 id="1-Label" data-id="1-Label"><a class="anchor hidden-xs" href="#1-Label" title="1-Label"><span class="octicon octicon-link"></span></a><span>1. Label</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10236866" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/9pnxx4N.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>## 先透過kubectl run命令建立Pod基本YAML
kubectl run nginx-kusc00101 --image=nginx --restart=Never --dry-run=client -o yaml&gt; q1.pod
## 將YAML 修改 (新增下方圖片紅字內容)
## 創立物件
kubectl apply -f q1.pod
</code></pre><p><span>nodeSelector 是 Pod 到某 Node 上最簡單、直接的調度方式，直接利用 label選取即可</span></p><p><img src="https://i.imgur.com/sI3n5R6.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/rwzGjMY.png" alt="" loading="lazy"></p><p><span>是個Pod Scheduling 的概念題，也就是如何將 Pod運行在適合的 Node上</span></p><div class="alert alert-success">
<p><span>label 是一個 [key:value] 的結構，以 key 對應的 value 來區隔它們屬性的異同</span><br>
<span>label 可以被附加到各種 resource-objects 上，例如 Node、Pod 或 Service等…</span><br>
<span>一個 label 可以被增加到任意數量的物件上；一個物件也可以添加任意數量的 label</span></p>
<p><span>常用的 label 包含 release(版本)、environment(環境)、tier(架構)、partition(分區)、track(品質管控)等。</span></p>
</div><p><span>相關label指令</span></p><div class="alert alert-warning">
<p><span>查看Pod label</span><br>
<span>kubectl get po --show-labels</span><br>
<span>查看特定label的Pod</span><br>
<span>kubectl get po --selector &lt;key&gt;=&lt;value&gt;</span><br>
<span>kubectl get po --selector &lt;key1&gt;=&lt;value1&gt;,&lt;key2&gt;=&lt;value2&gt;</span><br>
<span>新增Pod label</span><br>
<span>kubectl label po &lt;pod-name&gt; &lt;key&gt;=&lt;value&gt;</span><br>
<span>刪除Pod label</span><br>
<span>kubectl label po &lt;pod-name&gt; &lt;key&gt;-</span></p>
</div><p><span>想要删除一個 Label，只需在命令最後指定 Label 的 key 名稱與一個減號相連即可：</span></p><p><img src="https://i.imgur.com/6RSRY2i.png" alt="" loading="lazy"></p><h3 id="2-Deployment-amp-ReplicaSet-amp-Replication-Controller" data-id="2-Deployment-amp-ReplicaSet-amp-Replication-Controller"><a class="anchor hidden-xs" href="#2-Deployment-amp-ReplicaSet-amp-Replication-Controller" title="2-Deployment-amp-ReplicaSet-amp-Replication-Controller"><span class="octicon octicon-link"></span></a><span>2. Deployment &amp; ReplicaSet &amp; Replication Controller</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10237456" target="_blank" rel="noopener"><span>Ref1 從題目中學習k8s</span></a><br>
<a href="https://stackoverflow.com/questions/73814500/record-has-been-deprecated-then-what-is-the-alternative" target="_blank" rel="noopener"><span>Ref2 --record has been deprecated, then what is the alternative</span></a></p><p><img src="https://i.imgur.com/JnZHgeD.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>sudo kubectl create deploy nginx-app --image=nginx:1.11.0-alpine --replicas 3
sudo kubectl set image deployment nginx-app nginx=nginx:1.11.3-alpine --&gt; 升版
sudo kubectl annotate deployment nginx-app kubernetes.io/change-cause="version change to 1.11.0 to 1.11.3" --overwrite=true
sudo kubectl rollout history deployment nginx-app
sudo kubectl rollout undo deployment nginx-app --&gt; 退版
</code></pre><p><img src="https://i.imgur.com/leII1mB.png" alt="" loading="lazy"></p><p><span>考點是 rolling update 和 rollout，也就是K8s中重要的版本控制、版本升級與版本回滾 (版本是指 image 的版本)</span></p><p><span>在K8s中有兩種版本升級的 strategies</span></p><ul>
<li><ins><span>recreate</span></ins><br>
<span>recreate方法很直覺，就是直接將所有Pod一次升級，一次刪除所有舊版Pod，再一次創建所有新版Pod，缺點是更新過程會暫時中斷服務</span></li>
<li><ins><span>rolling update</span></ins><br>
<span>rolling update則是將舊版Pod一個一個刪除，再一個一個創建新的，可以保證更新期間提供的服務不會中斷。</span></li>
</ul><div class="alert alert-success">
<p><span>創建 Deployment 時不必特別指定 update strategy type</span><br>
<span>K8s中 default的策略就是 rolling update (可以用 kubectl describe deploy 命令查看)</span></p>
</div><h4 id="Deployment部署" data-id="Deployment部署"><a class="anchor hidden-xs" href="#Deployment部署" title="Deployment部署"><span class="octicon octicon-link"></span></a><span>Deployment部署</span></h4><p><span>Deployment 可以達成以下幾件事情：</span></p><ul>
<li><span>部署一個應用服務 (application)</span></li>
<li><span>協助 applications 升級到某個特定版本</span></li>
<li><span>服務升級過程中做到無停機服務遷移 (zero downtime deployment)</span></li>
<li><span>可以 Rollback 到先前版本</span></li>
</ul><table>
<thead>
<tr>
<th><span>Deployment相關指令</span></th>
<th><span>指令功能</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>kubectl get deployments</span></td>
<td><span>取得目前Kubernetes中的deployments的資訊</span></td>
</tr>
<tr>
<td><span>kubectl get rs</span></td>
<td><span>取得目前Kubernetes中的 Replication Set 的資訊</span></td>
</tr>
<tr>
<td><span>kubectl describe deploy &lt;deployment-name&gt;</span></td>
<td><span>取得特定deployment的詳細資料</span></td>
</tr>
<tr>
<td><span>kubectl set image deploy/ &lt;deployment-name&gt; &lt;pod-name&gt;: &lt;image-path&gt;:&lt;version&gt;</span></td>
<td><span>將 deployment 管理的 pod 升級到特定image版本</span></td>
</tr>
<tr>
<td><span>kubectl edit deploy &lt;deployment-name&gt;</span></td>
<td><span>編輯特定 deployment 物件</span></td>
</tr>
<tr>
<td><span>kubectl rollout status deploy &lt;deployment-name&gt;</span></td>
<td><span>查詢目前某deployment升級狀況</span></td>
</tr>
<tr>
<td><span>kubectl rollout history deploy &lt;deployment-name&gt;</span></td>
<td><span>查詢目前某deployment升級的歷史紀錄</span></td>
</tr>
<tr>
<td><span>kubectl rollout undo deploy &lt;deployment-name&gt;</span></td>
<td><span>回滾 Pod到先前一個版本</span></td>
</tr>
<tr>
<td><span>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=n</span></td>
<td><span>回滾 Pod到某個特定版本</span></td>
</tr>
</tbody>
</table><h4 id="ReplicaSet-複製集" data-id="ReplicaSet-複製集"><a class="anchor hidden-xs" href="#ReplicaSet-複製集" title="ReplicaSet-複製集"><span class="octicon octicon-link"></span></a><span>ReplicaSet 複製集</span></h4><p><img src="https://i.imgur.com/sb9Q0FU.png" alt="" loading="lazy"><br>
<span>圖中可以看到一個 Deployment 掌管一或多個 ReplicaSet，而一個 ReplicaSet 掌管一或多個 Pod</span></p><div class="alert alert-info">
<p><span>Kubernetes 官方强烈建議避免直接使用 ReplicaSet，應該通過 Deployment 來建立 RS 和Pod</span></p>
</div><p><span>ReplicaSet 是用來確保在 k8s 中，在資源允許的前提下，指定的 pod 的數量會跟使用者所期望的一致，也就是所謂的 desired status</span></p><ul>
<li><ins><span>常用的 replicaset command</span></ins></li>
</ul><pre><code>kubectl create -f replicaset-definition.yml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset (Also deltes all underlying PODs)
kubectl replace -f replocaset-definition.yml
kubectl scale -replicas=6 -f replocaset-definition.yml (擴展 replicas)
kubectl edit rs new-replica-set (編輯 rs)
</code></pre><p><span>[Ref- ReplicaSet 介紹]</span><br>
<img src="https://i.imgur.com/DIm2ybF.png" alt="" loading="lazy"></p><pre><code># my-replica-sets.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replica-set
spec:
  replicas: 3
  selector:
    matchLabels:
      env: dev
    matchExpressions:
      - {key: env, operator: In, values: [dev]}
      - {key: env, operator: NotIn, values: [prod]}
  template:
    metadata:
      labels:
        app: hello-pod-v1
        env: dev
        version: v1
    spec:
      containers:
      - name: my-pod
        image: zxcvbnius/docker-demo
        ports:
        - containerPort: 3000
</code></pre><ul>
<li>
<p><ins><span>spec.selector.matchLabels</span></ins><br>
<span>在 Replica Set的selector 裡面提供了 matchLabels，matchLabels的用法代表著等於(equivalent)，代表Pod的labels必須與matchLabels中指定的值相同，才算符合條件。</span></p>
</li>
<li>
<p><ins><span>spec.selector.matchExpressions</span></ins><br>
<span>matchExpressions 的用法較為彈性，每一筆條件主要由三個部分組成key, operator，value.</span><br>
<span>以 my-replica-sets.yaml 中敘述為例，我們指定Pod的條件為 1) env必須為dev 2) env不能為prod。</span><br>
<span>目前operator支援4種條件: In, NotIn, Exists, DoesNotExis</span></p>
</li>
</ul><h4 id="Replication-Controller" data-id="Replication-Controller"><a class="anchor hidden-xs" href="#Replication-Controller" title="Replication-Controller"><span class="octicon octicon-link"></span></a><span>Replication Controller</span></h4><div class="alert alert-success">
<p><span>Replication Controller 就是 Kubernetes 上用來管理 Pod 的數量以及狀態的 controller</span></p>
<ul>
<li><span>每個 Replication Controller 都有屬於自己的 yaml 檔</span></li>
<li><span>在 Replication Controller 設定檔中可以指定同時有多少個相同的 Pods 運行在 Kubernetes Cluster上</span></li>
<li><span>當某一 Pod 發生 crash, failed，而終止運行時，Replication Controller會幫我們自動偵測，並且自動創建一個新的Pod，確保 Pod運行的數量與設定檔的指定的數量相同</span></li>
</ul>
</div><p><span>透過 Replica 設定我們就可以快速產生一樣內容的 Pod</span><br>
<span>舉例來說：今天設定了 replica: 2 就代表會產生兩個內容一樣的 Pod 出來。</span><br>
<img src="https://i.imgur.com/k5LOx1S.png" alt="" loading="lazy"></p><pre><code>kubectl create -f q3.yaml
kubectl get replicationcontroller
</code></pre><p><img src="https://i.imgur.com/GuGoOFi.png" alt="" loading="lazy"></p><pre><code># q3.yaml
apiVersion: v1                                                      
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: test
      name: test
    spec:
      containers:
      - image: nginx:1.11.0-alpine
        name: test
  replicas: 3
</code></pre><ul>
<li><ins><span>spec.replicas &amp; spec.selector</span></ins><br>
<span>在spec.replicas中，我們必須定義 Pod的數量，以及在spec.selector中指定我們要選擇的Pod的條件 (labels)</span></li>
<li><ins><span>spec.template</span></ins><br>
<span>在spec.template 中我們會去定義pod的資訊，包含Pod的labels以及Pod中要運行的container。</span></li>
</ul><h3 id="3-Service" data-id="3-Service"><a class="anchor hidden-xs" href="#3-Service" title="3-Service"><span class="octicon octicon-link"></span></a><span>3. Service</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10238233" target="_blank" rel="noopener"><span>Ref 從題目中學習k8s</span></a><br>
<img src="https://i.imgur.com/rxhUfNu.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>## 透過kubectl expose命令創建Service
sudo kubectl expose pod kubernetes-demo-pod --type=NodePort --port=80
</code></pre><p><a href="https://hackmd.io/@tienyulin/kubernetes-service-deployment-ingress" target="_blank" rel="noopener"><span>Ref - Pod 進階應用 : Service、Deployment、Ingress</span></a></p><div class="alert alert-success">
<p><span>Service 是一個抽象化的物件，它定義了一群的 Pod 和存取這些 Pod 的規則</span><br>
<span>簡單來說就是要讓使用者和 Kubernetes Cluster 進行隔離，使用者只需要告訴 Service 我的請求而不需要知道誰會幫我處理也不需要知道處理的過程和細節，Service 收到請求後就會依照定義的規則將請求送到對應的 Pod</span><br>
<span>Pod 和 Pod 之間的也會透過 Service 來存取。</span></p>
</div><p><img src="https://i.imgur.com/XGdNEJC.png" alt="" loading="lazy"></p><h4 id="kubectl-expose" data-id="kubectl-expose"><a class="anchor hidden-xs" href="#kubectl-expose" title="kubectl-expose"><span class="octicon octicon-link"></span></a><span>kubectl expose</span></h4><p><span>該指令可以幫我們創建一個新的 Kubernetes Service 物件，來讓Kubernetes Cluster中運行的 Pod與外部互相溝通</span></p><pre><code>kubectl expose pod &lt;pod name&gt; --type=&lt;service type&gt; --name=&lt;service name&gt; \
--port=&lt;port&gt; --target-port=&lt;target port&gt;
</code></pre><h4 id="Service-Nodeport" data-id="Service-Nodeport"><a class="anchor hidden-xs" href="#Service-Nodeport" title="Service-Nodeport"><span class="octicon octicon-link"></span></a><span>Service Nodeport</span></h4><p><span>Service 的類型:</span></p><div class="alert alert-success">
<ul>
<li><span>ClusterIP : 預設類型,使用 Cluster 內部的 IP 來暴露 Service，因此只能在 Cluster 內部存取。</span></li>
<li><span>NodePort : 使用 Node 的 IP 和 Port 來暴露 Service，在 Cluster 外可以透過 NodeIP: NodePort 來存取 Service。</span><br>
<span>而 Cluster 內部所需的 Cluster IP 會自動被建立。</span></li>
<li><span>LoadBalancer : 使用雲端供應商提供的 LoadBalancer 來開放 Service，會自動建立相對應的 NodePort 和 Cluster IP。</span></li>
<li><span>ExternalName : 將 Service 關聯到 ExternalName，也就是 Domain。</span><br>
<span>例如 : </span><a href="http://foo.sample.com" target="_blank" rel="noopener"><span>foo.sample.com</span></a><span>。</span></li>
</ul>
</div><p><img src="https://i.imgur.com/B5u9XNy.png" alt="" loading="lazy"></p><ul>
<li><ins><span>apiVersion</span></ins><br>
<span>Service使用的Kubernetes API 是 v1版本號</span></li>
<li><ins><span>spec.type</span></ins><br>
<span>可以指定 Service的型別，可以是 NodePort 或是 LoadBalancer 等等…</span></li>
<li><ins><span>spec.ports.port</span></ins><br>
<span>創建 Service 的 Cluster IP，是哪個port number去對應到targetPort</span></li>
<li><ins><span>spec.ports.nodePort</span></ins><br>
<span>可以指定 Node 物件是哪一個 port numbrt，去對應到targetPort，若是在Service的設定檔中沒有指定的話，Kubernetes會隨機幫我們選一個port number</span></li>
<li><ins><span>spec.ports.targetPort</span></ins><br>
<span>targetPort 是我們指定的 Pod 的 port number，由於我們會在Pod中運行一個 port number 3000 的 web container，所以我們指定hello-service的特定 port number都可以導到該web container</span></li>
<li><ins><span>spec.ports.protocol</span></ins><br>
<span>目前 Service 支援TCP與UDP兩種protocl，預設為TCP</span></li>
<li><ins><span>spec.selector</span></ins><br>
<span>selector 會幫忙選擇 pod (use label)</span></li>
</ul><p><a href="https://ithelp.ithome.com.tw/articles/10194344" target="_blank" rel="noopener"><span>&lt;NodePort example&gt;</span></a></p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: hello-service
spec:
  type: NodePort
  ports:
  - port: 3000
    nodePort: 30390
    protocol: TCP
    targetPort: 3000
  selector:
    app: my-deployment
</code></pre><p><img src="https://i.imgur.com/e54szMJ.png" alt="" loading="lazy"></p><p><span>相關 service 指令</span></p><div class="alert alert-warning">
<p><span>kubectl expose pod kubernetes-pod --type=NodePort</span><br>
<span>## 查看建立的 Service</span><br>
<span>kubectl get services</span><br>
<span>## 查看 Service 的詳細資訊</span><br>
<span>kubectl describe services</span></p>
</div><h3 id="4-Namespace" data-id="4-Namespace"><a class="anchor hidden-xs" href="#4-Namespace" title="4-Namespace"><span class="octicon octicon-link"></span></a><span>4. Namespace</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10238974" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/JQeAd3s.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>kubectl create ns website-frontend
kubectl run jenkins --image=jenkins --restart=Never --dry-run=client -o yaml &gt; q4.yaml
kubectl apply -f q4.yaml -n website-frontend
</code></pre><div class="alert alert-success">
<p><span>藉由 namespace 可以作到 project 的 resources 的隔離，使不同的 project 可以有相同名字的 deployment, services.</span><br>
<span>可以當作在 cluster 又在加上一層 virtual cluster 的分割，這個好處是不需要多開實體的 cluster 裡面又包含了 master node 的相關服務</span></p>
<ul>
<li><span>同一個 namespace 的資源名稱是唯一性</span></li>
<li><span>不同 namespaces 的資源名稱可以相同</span></li>
</ul>
</div><p><img src="https://i.imgur.com/6nxaHQV.png" alt="" loading="lazy"></p><p><span>K8s 預設存在的 namespace 有三個:</span></p><ul>
<li><ins><span>default</span></ins><br>
<span>K8s集群建立時自動創建，一般對K8s物件的操作都在default namespace中完成</span></li>
<li><ins><span>kube-system</span></ins><br>
<span>K8s在集群建立時會為了某些目的創建一系列的Pod和Service，例如weave net CNI、DNS Service等，通常是一些維持集群運作的重要物件</span><br>
<span>為了預防這些物件不小心被使用者刪除或更改導致集群毀損，必須將這些物件獨立於其它物件，因此會將這些物件放置於kube-system這個namespace之下</span></li>
<li><ins><span>kube-public</span></ins><br>
<span>此namespace也是由K8s自動創建，當有些resources需要讓所有user都能access到時，會利用此namespace</span></li>
</ul><p><span>相關namespace指令</span></p><div class="alert alert-warning">
<p><span>## 查看&lt;namespace-name&gt;下的pod</span><br>
<span>kubectl get po --namespace=&lt;namespace-name&gt;</span><br>
<span>kubectl get po -n &lt;namespace-name&gt;</span></p>
<p><span>## 創建namespace</span><br>
<span>kubectl create ns &lt;namespace-name&gt;</span></p>
<p><span>## 查看所有namespace下的pod</span><br>
<span>kubectl get po -A (–all-namespaces)</span></p>
<p><span>## 刪除單一 Namespaces</span><br>
<span>kubectl delete namespaces &lt;namespace-name&gt;</span></p>
<p><span>##切換 Namespaces</span><br>
<span>kubectl config set-context $(kubectl config current-context) --namespace=dev</span><br>
<span>kubectl config view | grep namespace</span></p>
</div><ul>
<li><span>切換 namespace example</span><br>
<img src="https://i.imgur.com/eKAdWlx.png" alt="" loading="lazy"></li>
</ul><pre><code># my-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
</code></pre><ul>
<li><span>刪除 example</span><br>
<img src="https://i.imgur.com/wFGPdv2.png" alt="" loading="lazy"><br>
<img src="https://i.imgur.com/4pB0jdD.png" alt="" loading="lazy"></li>
</ul><h3 id="5-Imperative-vs-Declarative" data-id="5-Imperative-vs-Declarative"><a class="anchor hidden-xs" href="#5-Imperative-vs-Declarative" title="5-Imperative-vs-Declarative"><span class="octicon octicon-link"></span></a><span>5. Imperative vs Declarative</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10240455" target="_blank" rel="noopener"><span>Ref-資源定義與使用</span></a></p><p><span>&lt;資源對象管理方式&gt;</span><br>
<span>在 kubectl 中有三種方式管理資源</span></p><ul>
<li><ins><span>Imperative commands</span></ins><br>
<span>透過指令方式管理資源, 通常於開發或測試使用</span><br>
<span>ex: kubectl edit</span></li>
<li><ins><span>Imperative object configurations</span></ins><br>
<span>ex: 建立, 刪除, 替換</span></li>
</ul><pre><code>kubectl create -f FILE.yaml
kubectl delete -f FILE.yaml
kubectl replace -f FILE.yaml
</code></pre><ul>
<li><ins><span>Declarative object configurations</span></ins><br>
<span>使用檔案描述，底層由 kubernetes 維護，也就是其他未定義欄位。因此會針對先前修改過的欄位比較，並進行刪或更新動作。</span><br>
<strong><span>相較於 create 是去創建，而 apply 則是維護</span></strong></li>
</ul><p><img src="https://i.imgur.com/N2CNscT.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/ffuHIpJ.jpg" alt="" loading="lazy"></p><h2 id="Scheduling" data-id="Scheduling"><a class="anchor hidden-xs" href="#Scheduling" title="Scheduling"><span class="octicon octicon-link"></span></a><span>Scheduling</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section3-Scheduling/Kubernetes%2B-CKA-%2B0200%2B-%2BScheduling.pdf" target="_blank" rel="noopener"><span>Scheduling.pdf</span></a></p><h3 id="0-Scheduling---nodeName-amp-nodeSelectors" data-id="0-Scheduling---nodeName-amp-nodeSelectors"><a class="anchor hidden-xs" href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0-Scheduling---nodeName-amp-nodeSelectors"><span class="octicon octicon-link"></span></a><span>0. Scheduling - nodeName &amp; nodeSelectors</span></h3><p><span>Scheduling就是決定Pod在建立時，會被安排到哪個Node上 (cluster 集群預設是 </span><strong><span>Scheduler</span></strong><span>來負責這件事)</span></p><ul>
<li><ins><span>Manual Scheduling</span></ins><br>
<span>會需要手動做 Scheduling 的情況為集群中沒有Scheduler時</span><br>
<span>如果沒有Sheduler，準備建立的Pod之狀態會停留在Pending，等待系統將其綁定到某個節點上運行</span></li>
</ul><div class="alert alert-success">
<p><span>kubectl get pods --namespace kube-system --&gt; 用這個指令看是否缺少 sheduler</span></p>
</div><h4 id="nodeName" data-id="nodeName"><a class="anchor hidden-xs" href="#nodeName" title="nodeName"><span class="octicon octicon-link"></span></a><span>nodeName</span></h4><h5 id="方法一-Binding" data-id="方法一-Binding"><a class="anchor hidden-xs" href="#方法一-Binding" title="方法一-Binding"><span class="octicon octicon-link"></span></a><span>方法一: Binding</span></h5><p><span>使用Binding來幫它綁定到某個Node上</span></p><pre><code>apiVersion: v1
kind: Binding
metadata:
  name: nginx # 會自動找這個名字的物件
target:
  apiVersion: v1
  kind: Node
  name: node01
</code></pre><p><span>這邊 </span><a href="http://metadata.name" target="_blank" rel="noopener"><span>metadata.name</span></a><span> 所設定的物件名稱，系統會自動根據這個名稱去尋找，找到這個名稱的物件並綁定到target所指定的Node</span></p><h5 id="方法二-Edit-pod-yaml" data-id="方法二-Edit-pod-yaml"><a class="anchor hidden-xs" href="#方法二-Edit-pod-yaml" title="方法二-Edit-pod-yaml"><span class="octicon octicon-link"></span></a><span>方法二: Edit pod yaml</span></h5><p><span>在原本的 YAML中，使用 </span><strong><span>nodeName</span></strong><span> 來指定Pod到某個Node.</span><br>
<span>(要注意的是，Pod其實是無法直接更新的，一定要透過刪除再重建來更新)</span></p><p><img src="https://i.imgur.com/wecbjPH.png" alt="" loading="lazy"></p><h4 id="nodeSelectors" data-id="nodeSelectors"><a class="anchor hidden-xs" href="#nodeSelectors" title="nodeSelectors"><span class="octicon octicon-link"></span></a><span>nodeSelectors</span></h4><p><span>透過設置 nodeSelector 來達成 Node Affinity 的需求</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    run: test
  name: test
spec:
  containers:
  - image: nginx
    name: test
  nodeSelector:
    size: Large 
</code></pre><h3 id="1-Taints-amp-Tolerations" data-id="1-Taints-amp-Tolerations"><a class="anchor hidden-xs" href="#1-Taints-amp-Tolerations" title="1-Taints-amp-Tolerations"><span class="octicon octicon-link"></span></a><span>1. Taints &amp; Tolerations</span></h3><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section3-Scheduling/Udemy%2BKubernetes%2Btaints-tolerations.pdf" target="_blank" rel="noopener"><span>taints-tolerations.pdf</span></a></p><p><span>taint 跟 node affinity 雖然都是屬於 scheduling 的一部份，但要達成的目的其實完全相反：</span></p><div class="alert alert-info">
<p><span>node affinity (親和性調度)：設計如何讓 pod 被分派到某個 worker node</span><br>
<span>taint (汙點)：設計讓 pod 如何不要被分派到某個 worker node</span></p>
</div><ul>
<li><ins><span>Taints (污點)</span></ins><span>: 是讓 Node 能排斥特定類型的 Pod</span></li>
<li><ins><span>Tolerations (容忍度)</span></ins><span>: 是應用在Pod上的屬性，允許(但不硬性要求)Pod 調度到帶有與之匹配的Taints 的 Node 上.</span></li>
</ul><hr><p><img src="https://i.imgur.com/Hia2ShD.png" alt="" loading="lazy"></p><p><span>pod D 有標示 Tolerations: app=blue 僅代表可以接受的 node (非強制)</span><br>
<span>以這張圖為例子, pod D 不一定只能在有標示 Taints: app=blue 上的 node1 執行, 他也可在無任何限制下的 node3 上執行</span></p><pre><code>kubectl describe nodes &lt;node-name&gt; | grep -i taints
</code></pre><p><img src="https://i.imgur.com/Nk2x2Tg.png" alt="" loading="lazy"></p><p><ins><span>Master node</span></ins><span>: 基於安全性考量情況下，k8s 中 master node是不會部署pod在上面</span><br>
<span>(可以看到有一個 Taint setting 是說 Noschedule on </span><a href="http://node-role.kubernetes.io/master" target="_blank" rel="noopener"><span>node-role.kubernetes.io/master</span></a><span>)</span></p><hr><p><span>每個 taint 都有以下 3 個屬性：</span></p><ul>
<li><span>Key</span></li>
<li><span>Value</span></li>
<li><span>Effect: NoSchedule &amp; PreferNoSchedule &amp; NoExecute</span></li>
</ul><pre><code>#增加 taint
kubectl taint nodes node1 key=value:tain-effect
#移除 taint (在 taint 的 Key:Effect 後面加上 -)
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
</code></pre><p><img src="https://i.imgur.com/hDK3XH8.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/j1bXG49.png" alt="" loading="lazy"></p><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: test
spec:
  containers:
  - image: nginx
    name: test
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
</code></pre><h4 id="TaintsTolerations-and-Node-Affinity" data-id="TaintsTolerations-and-Node-Affinity"><a class="anchor hidden-xs" href="#TaintsTolerations-and-Node-Affinity" title="TaintsTolerations-and-Node-Affinity"><span class="octicon octicon-link"></span></a><span>Taints/Tolerations and Node Affinity</span></h4><p><span>有時候想要的效果需要組合 Taints/Tolerations + Node Affinity 才能辦到</span></p><hr><p><img src="https://i.imgur.com/WtcLJjT.png" alt="" loading="lazy"></p><hr><h3 id="2-Affinity-amp-Anti-Affinity" data-id="2-Affinity-amp-Anti-Affinity"><a class="anchor hidden-xs" href="#2-Affinity-amp-Anti-Affinity" title="2-Affinity-amp-Anti-Affinity"><span class="octicon octicon-link"></span></a><span>2. Affinity &amp; Anti-Affinity</span></h3><p><span>Affinity/Anti-Affinity 指的是親和性調度與反親和性調度，也就是滿足特定條件後，(不)將Pod調度到特定群集上</span><br>
<span>Node Affinity 和 Node Selector不同的是，它會有更多細緻的操作，你可以把Node Selector看成是簡易版的 Node Affinity</span></p><div class="alert alert-info">
<p><span>nodeAffinity有兩種策略</span></p>
<ul>
<li><span>1.preferredDuringSchedulingIgnoredDuringExecution: 軟策略（可以不滿足)</span></li>
<li><span>2.requiredDuringSchedulingIgnoredDuringExecution: 硬策略（一定要滿足）</span></li>
<li><span>requiredDuringSchedulingRequiredDuringExecution (planned)</span></li>
</ul>
</div><p><span>上面策略可以分成4段來看</span></p><table>
<thead>
<tr>
<th></th>
<th><span>DuringScheduling</span></th>
<th><span>DuringExecution</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>Type1</span></td>
<td><span>Required</span></td>
<td><span>Ignored</span></td>
</tr>
<tr>
<td><span>Type2</span></td>
<td><span>Preferred</span></td>
<td><span>Ignored</span></td>
</tr>
<tr>
<td><span>Type3</span></td>
<td><span>Required</span></td>
<td><span>Required</span></td>
</tr>
</tbody>
</table><ul>
<li><span>左右兩邊內容相同</span><br>
<img src="https://i.imgur.com/wuf4Bn4.png" alt="" loading="lazy"></li>
</ul><hr><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/" target="_blank" rel="noopener"><span>Schedule a Pod using required node affinity</span></a><br>
<span>做題目的時候，先利用 </span><a href="https://kubernetes.io/docs/home/" target="_blank" rel="noopener"><span>k8s.io document</span></a><span> 去搜尋 example 來改</span></p><pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
</code></pre><h3 id="3-Kubernetes-Resources---RequestLimit" data-id="3-Kubernetes-Resources---RequestLimit"><a class="anchor hidden-xs" href="#3-Kubernetes-Resources---RequestLimit" title="3-Kubernetes-Resources---RequestLimit"><span class="octicon octicon-link"></span></a><span>3. Kubernetes Resources - Request/Limit</span></h3><p><span>Kubernetes 需要考慮如何在優先度和公平性的前提下提供資源的利用率，為了實現資源被有效調度和分配時同時提高資源利用率</span><br>
<span>Kubernetes 提供了 Request/Limit 兩種限制類型讓我們對資源進行分配。</span></p><div class="alert alert-success">
<ul>
<li><ins><span>request</span></ins><br>
<span>容器使用的最小資源要求，做為容器調度時資源分配的判斷依賴</span><br>
<span>只有當前節點上可分配的資源量 &gt;= request 時才允許將容器調度到該節點上。</span></li>
<li><ins><span>limit</span></ins><br>
<span>容器能使用的最大值。</span><br>
<span>設置為 0 表示對使用的資源不做限制，可以無限使用</span></li>
</ul>
</div><p><span>下面的 Pod 含有兩個容器.</span><br>
<span>1.每個容器都有 resource requests, 0.25 cpu 以及 64MiB 記憶體</span><br>
<span>2.每個容器都有 resource limits 0.5 cpu 以及 128 MiB 記憶體</span><br>
<span>總體來說, 這個 Pod resource limits 1 cpu 以及 256 MiB 記憶體</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
</code></pre><h4 id="Exceed-limit" data-id="Exceed-limit"><a class="anchor hidden-xs" href="#Exceed-limit" title="Exceed-limit"><span class="octicon octicon-link"></span></a><span>Exceed limit</span></h4><ul>
<li><span>throttle (節流)</span></li>
<li><span>terminate (終止)</span></li>
</ul><p><img src="https://i.imgur.com/LF00LLU.png" alt="" loading="lazy"></p><h4 id="Increase-pod-resource" data-id="Increase-pod-resource"><a class="anchor hidden-xs" href="#Increase-pod-resource" title="Increase-pod-resource"><span class="octicon octicon-link"></span></a><span>Increase pod resource</span></h4><div class="alert alert-info">
<p><span>The status OOMKilled indicates that it is failing because the pod ran out of memory.</span></p>
</div><p><span>$ k describe pods elephant</span><br>
<span>下面這張圖清楚表示 pod 會死掉的原因, 因為 elephant pod 需要 15Mi 的 memory, 但是 limit 是 10Mi, 所以導致 OOMKilled.</span><br>
<img src="https://i.imgur.com/7H3gNUW.png" alt="" loading="lazy"></p><p><span>要調整 Memory Limit 到 20Mi</span></p><pre><code>(o) kubectl replace --force -f /tmp/kubectl-edit-996911536.yaml
(x) kubectl edit pod elephant --&gt; 不能直接調整 running pod resource limit
</code></pre><p><img src="https://i.imgur.com/95LbxBq.png" alt="" loading="lazy"></p><h3 id="4-Demonsets" data-id="4-Demonsets"><a class="anchor hidden-xs" href="#4-Demonsets" title="4-Demonsets"><span class="octicon octicon-link"></span></a><span>4. Demonsets</span></h3><p><span>DamonSet 會確保在所有(或是特定)節點上，一定運行著指定的一個Pod，並且每當有新的Node加入Cluster時，DaemonSet會為他們新增這指定的一個Pod，同時只要有Node被移除Cluster外，在這Node上的指定Pod也會被移除</span></p><p><img src="https://i.imgur.com/CXSKKpi.png" alt="" loading="lazy"></p><p><span>應用場景:</span></p><ul>
<li><span>Monitoring (Prometheu, Datadog)</span></li>
<li><span>Log Viewer (fluent-bi, fluentd, logstash)</span></li>
<li><span>Storage (glusterd, ceph)</span></li>
</ul><pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
</code></pre><ul>
<li><ins><span>k describe daemonsets --namespace=kube-system</span></ins><br>
<span>觀察有多少 pod scheduled by DaemonSet kube-proxy --&gt; 1個</span></li>
</ul><p><img src="https://i.imgur.com/pBA7ap7.png" alt="" loading="lazy"></p><ul>
<li><ins><span>daemonset create</span></ins></li>
</ul><pre><code>kubectl create deployment elasticsearch --namespace=kube-system \
--image=registry.k8s.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml &gt; q1.yaml
</code></pre><p><span>創建 daemonset yaml (因為 kubectl 沒有 kubectl create daemonset command 所以先用 kubectl create deployment 再去做修正)</span></p><h3 id="5-Static-pod" data-id="5-Static-pod"><a class="anchor hidden-xs" href="#5-Static-pod" title="5-Static-pod"><span class="octicon octicon-link"></span></a><span>5. Static pod</span></h3><p><span>Static Pod 不需依靠Controller Plane的物件，所以可以透過 Static Pod創建屬於自己 Node中的controller plane物件</span><br>
<span>ex: Control Plan e中的 controller.yaml, etcd.yaml等等，其實都算是一種 Static Pod</span></p><p><span>Pod依創建方式可分為兩種</span></p><ul>
<li><span>Static Pod</span></li>
<li><span>kube-apiserver</span></li>
</ul><pre><code>kubectl get po -A --&gt; 只要Pod name後面跟著 &lt;Control Plane-Name&gt; 的，就都是Static Pod
</code></pre><p><span>下面這張圖有4個 static pod</span><br>
<img src="https://i.imgur.com/ewdSnzZ.png" alt="" loading="lazy"></p><h4 id="Static-Pod-Path" data-id="Static-Pod-Path"><a class="anchor hidden-xs" href="#Static-Pod-Path" title="Static-Pod-Path"><span class="octicon octicon-link"></span></a><span>Static Pod Path</span></h4><p><span>Static Pod 的創建方式是由 kubelet 定期掃描特定目錄下的 YAML file 來創建</span></p><ul>
<li><span>/var/lib/kubelet/config.yaml</span></li>
</ul><pre><code>下圖紅色圈起來的 /etc/kubernetes/manifests 路徑就是創建 Static Pod 的特定目錄
</code></pre><p><img src="https://i.imgur.com/qZ0cpLe.png" alt="" loading="lazy"></p><h4 id="Create-statis-pod" data-id="Create-statis-pod"><a class="anchor hidden-xs" href="#Create-statis-pod" title="Create-statis-pod"><span class="octicon octicon-link"></span></a><span>Create statis pod</span></h4><p><img src="https://i.imgur.com/frLT5gU.png" alt="" loading="lazy"></p><p><span>注意 --command 一定要放在命令最後面</span></p><pre><code>kubectl run static-busybox --image=busybox --dry-run=client -o yaml \ 
--command -- sleep 1000 &gt; q1.yaml
mv q1.yaml /etc/kubernetes/manifests/
</code></pre><p><span>建立好後，把 ymal 放到 Static Pod 的特定目錄, 他就會自動建立好 statis pod</span></p><p><span>注意 restartPolicy: Never 會自動重啟</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox:1.28.4
    name: static-busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
</code></pre><hr><h2 id="Logging-amp-Monitoring" data-id="Logging-amp-Monitoring"><a class="anchor hidden-xs" href="#Logging-amp-Monitoring" title="Logging-amp-Monitoring"><span class="octicon octicon-link"></span></a><span>Logging &amp; Monitoring</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section4-Loggin%20%26%20Monitoring/Kubernetes%2B-CKA-%2B0300%2B-%2BLogging-Monitoring.pdf" target="_blank" rel="noopener"><span>Logging-Monitoring.pdf</span></a></p><h3 id="1-K8s-Monitor-Cluster-Components" data-id="1-K8s-Monitor-Cluster-Components"><a class="anchor hidden-xs" href="#1-K8s-Monitor-Cluster-Components" title="1-K8s-Monitor-Cluster-Components"><span class="octicon octicon-link"></span></a><span>1. K8s Monitor Cluster Components</span></h3><p><span>K8s本身就具備一些基本的伺服器監控工具，例如：</span></p><ul>
<li><span>K8s Dashboard：插件工具，展示每個 K8s 集群上的資源利用情況，也是實現資源和環境管理與交互的主要工具。</span></li>
<li><span>Pod liveness probe：Container健康狀態診斷工具。</span></li>
<li><span>Kubelet：每個 Node 上都運行著 Kubelet，監控Container的運行情況。 Kubelet 也是 Control Plane 與各個 Node 通信的渠道。</span></li>
</ul><h3 id="2-Metric-Server" data-id="2-Metric-Server"><a class="anchor hidden-xs" href="#2-Metric-Server" title="2-Metric-Server"><span class="octicon octicon-link"></span></a><span>2. Metric Server</span></h3><p><a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener"><span>metrics-server Github</span></a></p><p><span>直接將 Github 上的 open spurce pull 到本地端並創建其中物件</span></p><pre><code>git clone https://github.com/kubernetes-sigs/metrics-server.git
or
kubectl apply -f https://github.com/kubernetes-sigs/metrics- \
server/releases/download/v0.3.6/components.yaml
</code></pre><p><span>該 YAML會於 kube-system namespace創建一個Deployment</span><br>
<img src="https://i.imgur.com/dTo7Qx1.png" alt="" loading="lazy"></p><pre><code>kubectl top no &lt;node-name&gt;
</code></pre><p><img src="https://i.imgur.com/UMx7Pol.png" alt="" loading="lazy"></p><p><span>Node 中是透過kubelet 來管理 Node 的運作，kubelet需要透過 apiserver 接收Control Plane下達的命令在 Node中 運行 Pod.</span><br>
<span>而 kubelet 中其實還包含一個 subcomponent，稱為 cAdvisor 或是 Container Advisor</span><br>
<span>cAdvisor 負責從 Pod 中擷取performance metrics，並將收集到的數據以metrics-api的形式，透過Summary API expose 給 Metric-Server</span></p><p><img src="https://i.imgur.com/lWPWbWR.png" alt="" loading="lazy"></p><ul>
<li><span>Kubernetes log</span></li>
</ul><pre><code>Kubernetes logs -f 
</code></pre><h2 id="Application-Lifecycle-Managment" data-id="Application-Lifecycle-Managment"><a class="anchor hidden-xs" href="#Application-Lifecycle-Managment" title="Application-Lifecycle-Managment"><span class="octicon octicon-link"></span></a><span>Application Lifecycle Managment</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section5-Application%20Lifecycle%20Management/Kubernetes%2B-CKA-%2B0400%2B-%2BApplication%2BLifecycle%2BManagement.pdf" target="_blank" rel="noopener"><span>Application+Lifecycle+Management.pdf</span></a></p><h3 id="1-Rolling-Update-amp-Rollbacks" data-id="1-Rolling-Update-amp-Rollbacks"><a class="anchor hidden-xs" href="#1-Rolling-Update-amp-Rollbacks" title="1-Rolling-Update-amp-Rollbacks"><span class="octicon octicon-link"></span></a><span>1. Rolling Update &amp; Rollbacks</span></h3><p><span>Deployment 這個物件裡面會包含了 ReplicaSet，然後再透過 ReplicaSet 來掌管 Pod 運行數Kubernetes 中回滾機制 (Rolling Update / Back)，就是透過 ReplicasSet 來達到</span></p><ul>
<li><span>每次 Rolling Update (更新)都會產生新的 ReplicaSet 來管控 Pod，可以把它想成產生一個新版本</span></li>
<li><span>Rollbacks (回滾) 則是把現有版本切換到上一個或指定的版本</span></li>
</ul><p><img src="https://i.imgur.com/1AZqz5W.png" alt="" loading="lazy"></p><div class="alert alert-success">
<p><span>kubectl rollout status &lt;deployment&gt;  --&gt; 查看歷史 rollout 狀態</span><br>
<span>kubectl rollout history &lt;deployment&gt;  --&gt; 查看歷史 rollout 紀錄</span><br>
<span>kubectl rollout undo &lt;deployment&gt;  --&gt; roll back 版本</span><br>
<span>kubectl rollout pause &lt;deployment&gt;  --&gt; 暫停資源</span><br>
<span>kubectl rollout resume &lt;deployment&gt;  --&gt; 恢復暫停之資源</span></p>
</div><h4 id="Deployment-Strategy" data-id="Deployment-Strategy"><a class="anchor hidden-xs" href="#Deployment-Strategy" title="Deployment-Strategy"><span class="octicon octicon-link"></span></a><span>Deployment Strategy</span></h4><p><span>K8s 中有兩種版本升級的strategies，一種是recreate，一種為rolling update.</span></p><ul>
<li><ins><span>recreate</span></ins><br>
<span>直接將所有 Pod一次升級，一次刪除所有舊版Pod，再一次創建所有新版Pod，缺點是更新過程會暫時中斷服務；</span></li>
<li><ins><span>rolling update</span></ins><br>
<span>將舊版 Pod 一個一個刪除，再一個一個創建新的，可以保證更新期間提供的服務不會中斷</span></li>
</ul><p><img src="https://i.imgur.com/ZIL9m62.png" alt="" loading="lazy"></p><h4 id="Rolling-Update" data-id="Rolling-Update"><a class="anchor hidden-xs" href="#Rolling-Update" title="Rolling-Update"><span class="octicon octicon-link"></span></a><span>Rolling Update</span></h4><p><span>有三種方式來進行滾動升級 (以升級 docker image 為例)</span><br>
<ins><span>–record 參數:</span></ins><br>
<span>這參數主要是告知 Kubernetes 紀錄此次下達的指令，能清楚不同的版本(revision) 間做了什麼操作</span></p><ul>
<li><span>set image</span></li>
</ul><pre><code>kubectl set image deployment &lt;deployment&gt; &lt;container&gt;=&lt;image&gt; --record
</code></pre><ul>
<li><span>replace</span><br>
<span>修改 xxx.yaml 內的 image 版本</span></li>
</ul><pre><code>kubectl replace -f &lt;yaml&gt; --record
</code></pre><ul>
<li><span>edit</span></li>
</ul><pre><code>kubectl edit deployment &lt;deployment&gt; --record
</code></pre><hr><p><img src="https://i.imgur.com/YaRxY5J.jpg" alt="" loading="lazy"></p><h3 id="2-ConfigMap" data-id="2-ConfigMap"><a class="anchor hidden-xs" href="#2-ConfigMap" title="2-ConfigMap"><span class="octicon octicon-link"></span></a><span>2. ConfigMap</span></h3><p><span>ConfigMap 與 Pod 可以單獨存在於 k8s 叢集中，當 Pod 需要使用 ConfigMap 時才需要將 ConfigMap 掛載到 Pod 內使用</span></p><div class="alert alert-info">
<p><span>Decoupled (解耦)</span></p>
<ul>
<li><span>便於管理</span></li>
<li><span>高彈性：易於掛載不同的 ConfigMap 到 Pod 內使用</span></li>
<li><span>一處編輯多處使用：同一個 ConfigMap 可掛載到多個 Pod 使用</span></li>
</ul>
</div><p><span>k8s 中的 ConfigMap/Secret 可以接受兩種來源:</span></p><ul>
<li><span>–from-literal</span></li>
</ul><pre><code>kubectl create configmap myconfig --from-literal=k1=v1 --from-literal=k2=v2
</code></pre><p><img src="https://i.imgur.com/eVf7tI7.png" alt="" loading="lazy"></p><ul>
<li><span>–from-file</span></li>
</ul><pre><code>kubectl create configmap myconfigfromkey --from-file=fromfilekey=.env
</code></pre><p><img src="https://i.imgur.com/ReZgUp4.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Configmap in Pods 有三種型式</span></li>
</ul><p><img src="https://i.imgur.com/ebjgXcy.png" alt="" loading="lazy"></p><h4 id="Configmap-in-Pods" data-id="Configmap-in-Pods"><a class="anchor hidden-xs" href="#Configmap-in-Pods" title="Configmap-in-Pods"><span class="octicon octicon-link"></span></a><span>Configmap in Pods</span></h4><p><span>將 ConfigMap 內的 key 直掛在 Pod 環境變數的例子</span></p><pre><code>apiVersion: v1
kind: Pod 
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  containers:
  - image: nginx
    name: test
    command: ["/bin/sh", "-c", "env"]
    resources: {}
    env:
    - name: MY_CONFIG_KEY  &lt;=== 指定一個新的環境變數名稱為 MY_CONFIG_KEY
      valueFrom:
        configMapKeyRef:  &lt;=== MY_CONFIG_KEY 將會參考 myconfig 內的 k1 值
          name: myconfig  &lt;=== myconfig 是上面宣告的 ConfigMap 物件
          key: k1
  dnsPolicy: ClusterFirst
  restartPolicy: Never
</code></pre><hr><h3 id="3-Secret" data-id="3-Secret"><a class="anchor hidden-xs" href="#3-Secret" title="3-Secret"><span class="octicon octicon-link"></span></a><span>3. Secret</span></h3><p><span>Secrets 是 Kubernetes 提供開發者存放敏感資訊的方式</span><br>
<span>像是密碼、OAuth tokens 及 ssh keys 等等… 將這些資訊存在放 Secret 中比直接放在 Pod YAML 或 Image中更加安全和靈活</span></p><div class="alert alert-success">
<p><span>Remember that secrets encode data in </span><strong><span>base64 format</span></strong><span>.</span><br>
<span>Anyone with the base64 encoded secret can easily decode it.</span></p>
</div><ul>
<li><ins><span>kubectl apply -f test_secret.yaml</span></ins>
<ul>
<li><span>echo -n “xx” | base64</span></li>
<li><span>echo -n “xx” | base64 --decode</span></li>
</ul>
</li>
</ul><pre><code># test_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd29yZA== 
</code></pre><h4 id="Secret-in-Pods--1" data-id="Secret-in-Pods--1"><a class="anchor hidden-xs" href="#Secret-in-Pods--1" title="Secret-in-Pods--1"><span class="octicon octicon-link"></span></a><span>Secret in Pods -1</span></h4><p><span>下面兩種方式 valueFrom &amp; envFrom</span></p><p><span>考試的時候, 可以去 </span><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/" target="_blank" rel="noopener"><span>kubernetes.io</span></a><span> document 搜尋 example</span><br>
<span>secret --&gt; Using Secrets as environment variables --&gt; </span><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data" target="_blank" rel="noopener"><span>Define container environment variables using Secret data</span></a></p><h5 id="valueFrom" data-id="valueFrom"><a class="anchor hidden-xs" href="#valueFrom" title="valueFrom"><span class="octicon octicon-link"></span></a><span>valueFrom</span></h5><ul>
<li><ins><span>將Secret作為容器的環境變數</span></ins><br>
<span>直接在 Pod 中以 spec.env.valueFrom.secretKeyRef 欄位參考到 Secret 即可</span></li>
</ul><pre><code>sudo kubectl apply -f secret_test.yaml
sudo kubectl exec -it secret-env-pod sh
</code></pre><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: secret-env-pod                                                     
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: test-secret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: test-secret
            key: password
  restartPolicy: Never
</code></pre><ul>
<li><span>創建完成後，進入pod 並echo 即可看到 Secret 的設置結果</span><br>
<img src="https://i.imgur.com/8dDcH3e.png" alt="" loading="lazy"></li>
</ul><h5 id="envFrom" data-id="envFrom"><a class="anchor hidden-xs" href="#envFrom" title="envFrom"><span class="octicon octicon-link"></span></a><span>envFrom</span></h5><ul>
<li><span>dotfile-secret  為 kubectl secret name</span></li>
</ul><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: secret-env-pod                                                         
spec:
  containers:
  - name: secret-env-pod
    image: redis
    envFrom:
      - secretRef:
          name: dotfile-secret 
</code></pre><p><img src="https://i.imgur.com/dIA1Amm.png" alt="" loading="lazy"></p><h5 id="Secret-in-Pods---Volume" data-id="Secret-in-Pods---Volume"><a class="anchor hidden-xs" href="#Secret-in-Pods---Volume" title="Secret-in-Pods---Volume"><span class="octicon octicon-link"></span></a><span>Secret in Pods - Volume</span></h5><p><span>將 Secret 製作為一個檔案，Pod 以 Volume的形式將此檔案掛載(mount)到容器上</span></p><ul>
<li><ins><span>先用 kubectl 創建一個基礎的 yaml</span></ins></li>
</ul><pre><code>sudo kubectl run nginx --image=nginx --restart=Never \ 
--dry-run=client -o yaml &gt; secret_volume.yaml
</code></pre><h4 id="Question" data-id="Question"><a class="anchor hidden-xs" href="#Question" title="Question"><span class="octicon octicon-link"></span></a><span>Question</span></h4><p><a href="https://ithelp.ithome.com.tw/articles/10239675" target="_blank" rel="noopener"><span>Ref</span></a></p><p><img src="https://i.imgur.com/fkVjs2S.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><ol>
<li><span>創建一個Secret</span></li>
<li><span>創建兩個Pod (kubectl run 先建立兩個基礎 pod yaml)</span></li>
<li><span>其中一個Podmount此Secret，另一個Pod將Secret作為環境變數使用</span></li>
</ol><pre><code>1. sudo kubectl create secret generic super-secret \ 
--from-literal=credential=alice --from-literal=username=bob
2-1. kubectl run pod-secrets-via-file --image=redis --dry-run=client -o yaml &gt; q5-1-pod.yaml
2-2. kubectl run pod-secrets-via-env --image=redis --dry-run=client -o yaml &gt; q5-2-pod.yaml
3.kubectl apply -f q5-1-pod.yaml q5-2-pod.yaml
</code></pre><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q5.yaml" target="_blank" rel="noopener"><span>q5-1-pod.yaml</span></a><br>
<a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q5-2.yaml" target="_blank" rel="noopener"><span>q5-2-pod.yaml</span></a></p><p><img src="https://i.imgur.com/kDNbK8K.png" alt="" loading="lazy"></p><hr><h2 id="Cluster-Maintenance" data-id="Cluster-Maintenance"><a class="anchor hidden-xs" href="#Cluster-Maintenance" title="Cluster-Maintenance"><span class="octicon octicon-link"></span></a><span>Cluster Maintenance</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section6-Cluster%20Maintenance/Kubernetes-CKA-0500-Cluster%2BMaintenance-v1.2.pdf" target="_blank" rel="noopener"><span>Kubernetes-CKA-0500-Cluster+Maintenance-v1.2.pdf</span></a></p><h3 id="0-Kubernets-Software-Versions" data-id="0-Kubernets-Software-Versions"><a class="anchor hidden-xs" href="#0-Kubernets-Software-Versions" title="0-Kubernets-Software-Versions"><span class="octicon octicon-link"></span></a><span>0. Kubernets Software Versions</span></h3><ul>
<li><span>檢查版本</span></li>
</ul><pre><code>kubectl version --client
</code></pre><p><span>Kubernetes的版本v1.25.2可以分為三個部分：1、25、2，我們分別稱為Major、Minor、Patch。</span></p><ul>
<li><span>Major：主要版本</span></li>
<li><span>Minor：次要，代表特點和功能上的更新</span></li>
<li><span>Patch：補丁，代表修復Bug</span></li>
</ul><p><img src="https://i.imgur.com/S7vTbad.png" alt="" loading="lazy"></p><p><span>Kubernetes的各個元件，如：kube-apiserver、Controller-manager等，彼此的版本都有相依性，也就是版本不能相差太多</span></p><p><span>Kubernetes的更新週期是三個月，也就是每三個月會推出新版本</span></p><p><img src="https://i.imgur.com/nF8pJbC.png" alt="" loading="lazy"></p><h3 id="1-Cluster-Upgrade" data-id="1-Cluster-Upgrade"><a class="anchor hidden-xs" href="#1-Cluster-Upgrade" title="1-Cluster-Upgrade"><span class="octicon octicon-link"></span></a><span>1. Cluster Upgrade</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10301864" target="_blank" rel="noopener"><span>Upgrade Strategy</span></a></p><p><span>Cluster Upgrade主要有三種策略，三者的共通點是必須先更新 Master Node：</span></p><ol>
<li><ins><span>Strategy-1:</span></ins><span> 一次同時更新全部的 Worker Node。</span></li>
<li><ins><span>Strategy-2:</span></ins><span> 一次更新一個 Worker Node，更新成功後再更新下一個Worker Node，直到全部Worker Node更新完成。</span></li>
<li><ins><span>Strategy-3:</span></ins><span> 一次加入一個新版本的 Worker Node，加入後再剔除一個舊版本的 Worker Node，直到全部 Worker Node更新完成。</span></li>
</ol><hr><div class="alert alert-success">
<p><span>分成 Upgrading control plane nodes 跟 Upgrade worker nodes 兩部分</span></p>
</div><p><span>兩種 Node 的升級步驟都可以參考 </span><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/" target="_blank" rel="noopener"><span>k8s document</span></a></p><h3 id="11-Upgrading-control-plane-nodes" data-id="11-Upgrading-control-plane-nodes"><a class="anchor hidden-xs" href="#11-Upgrading-control-plane-nodes" title="11-Upgrading-control-plane-nodes"><span class="octicon octicon-link"></span></a><span>1.1 Upgrading control plane nodes</span></h3><p><strong><ins><span>step0:</span></ins><span> Check the current version</span></strong></p><p><img src="https://i.imgur.com/3PX8baW.png" alt="" loading="lazy"></p><pre><code>apt update
apt-cache madison kubeadm
# find the latest 1.26 version in the list
# it should look like 1.26.x-00, where x is the latest patch
</code></pre><p><strong><ins><span>step1:</span></ins></strong><br>
<span>檢查可以更新到哪個版本，輸入下面的指令 kubeadm 會告訴你目前可以升級到什麼版本</span></p><pre><code>kubeadm upgrade plan
</code></pre><p><span>目前版本: v1.25.0</span><br>
<span>遠端版本 (remote version): v1.26.1</span></p><p><img src="https://i.imgur.com/3rtBcKq.png" alt="" loading="lazy"></p><p><strong><ins><span>step2:</span></ins><span> upgrage kubeadm version</span></strong></p><p><span>replace x in 1.26.0-00 with the latest patch version</span></p><pre><code>kubeadm version
apt-get update &amp;&amp; apt-get install -y kubeadm=1.26.0-00 &amp;&amp; apt-mark hold kubeadm
</code></pre><p><img src="https://i.imgur.com/BHg4ATH.png" alt="" loading="lazy"></p><pre><code>sudo kubeadm upgrade apply v1.26.0
</code></pre><p><span>成功把 kubeadm 從 v1.25.0 升級到 v1.26.0</span><br>
<img src="https://i.imgur.com/ZHLwXLz.png" alt="" loading="lazy"></p><p><strong><ins><span>step3:</span></ins><span> Upgrade kubelet and kubectl</span></strong></p><pre><code>sudo apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubelet=1.26.x-00 kubectl=1.26.x-00 &amp;&amp; apt-mark hold kubelet kubectl
</code></pre><p><img src="https://i.imgur.com/zBVNnBA.png" alt="" loading="lazy"></p><p><strong><ins><span>step4:</span></ins><span> Restart the kubelet:</span></strong><br>
<span>重啟kubelet，使其更新生效</span></p><pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre><p><strong><ins><span>step5:</span></ins><span> Check the upgrade version</span></strong></p><p><span>檢查 control plane nodes 版本是否有確實升級</span></p><p><img src="https://i.imgur.com/X1QjFOg.png" alt="" loading="lazy"></p><p><strong><ins><span>step6:</span></ins><span> Uncordon the node</span></strong></p><p><span>升級完後, 把 control plane nodes 調回 schedulable</span></p><pre><code># replace &lt;node-to-uncordon&gt; with the name of your node
kubectl uncordon &lt;node-to-uncordon&gt;
</code></pre><p><img src="https://i.imgur.com/0E5fLFC.png" alt="" loading="lazy"></p><hr><h3 id="12-Upgrade-worker-nodes" data-id="12-Upgrade-worker-nodes"><a class="anchor hidden-xs" href="#12-Upgrade-worker-nodes" title="12-Upgrade-worker-nodes"><span class="octicon octicon-link"></span></a><span>1.2 Upgrade worker nodes</span></h3><p><span>The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time, without compromising the minimum required capacity for running your workloads.</span></p><p><strong><ins><span>step0:</span></ins><span> ssh worker node</span></strong></p><pre><code>ssh &lt;woker-node-ip&gt;
</code></pre><p><img src="https://i.imgur.com/CrKsiCN.png" alt="" loading="lazy"></p><p><strong><ins><span>step1:</span></ins><span> Upgrade kubeadm</span></strong></p><pre><code>apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubeadm=1.26.0-00 &amp;&amp; apt-mark hold kubeadm
</code></pre><p><strong><span>++step2:++kubeadm upgrade</span></strong></p><p><img src="https://i.imgur.com/0lrKS2Y.png" alt="" loading="lazy"></p><p><strong><span>++step3:++Upgrade kubelet and kubectl</span></strong></p><pre><code>apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubelet=1.26.0-00 kubectl=1.26.0-00 &amp;&amp; apt-mark hold kubelet kubectl
</code></pre><p><strong><ins><span>step4:</span></ins><span> Restart the kubelet:</span></strong><br>
<span>重啟kubelet，使其更新生效</span></p><pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre><p><strong><ins><span>step5:</span></ins><span> Check the upgrade version</span></strong></p><p><span>檢查 worker nodes 版本是否有確實升級</span></p><p><img src="https://i.imgur.com/X1QjFOg.png" alt="" loading="lazy"></p><p><strong><ins><span>step6:</span></ins><span> Uncordon the node</span></strong></p><p><span>升級完後, 把 worker nodes 調回 schedulable</span></p><p><img src="https://i.imgur.com/8tZnUwS.png" alt="" loading="lazy"></p><hr><h3 id="2-Node-Maintenance" data-id="2-Node-Maintenance"><a class="anchor hidden-xs" href="#2-Node-Maintenance" title="2-Node-Maintenance"><span class="octicon octicon-link"></span></a><span>2. Node Maintenance</span></h3><p><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller" target="_blank" rel="noopener"><span>Node Controller</span></a><span> 是 Kubernetes 中用來管理 Node 的一個物件</span></p><p><span>kubectl drain 代表將該 Node 狀態變更為維護模式，如此在該 Node 上面的 Pod，就會轉移到其他 Node 上.</span><br>
<span>若不是透過 Replication Controller 或是 DaemonSet 等創建好的 Pod，則該指令會失敗，除非加上 --force</span></p><ul>
<li><ins><span>從 Kubernetes Cluster 移除 node</span></ins></li>
</ul><pre><code>kubectl drain {node_name}
kubectl drain {node_name} --force

kubectl cordon {node_name}   --&gt; 標記 Node 為不可 Schedule
kubectl uncordon {node_name} --&gt; 恢復 Node
</code></pre><p><span>如果在 Node 上，有daemonset 類型的pod，不可以被刪除，需要加上下面的參數</span></p><ul>
<li><ins><span>–ignore-daemonsets</span></ins></li>
</ul><pre><code>kubectl drain node01 --ignore-daemonsets
</code></pre><p><img src="https://i.imgur.com/JSYb37l.png" alt="" loading="lazy"></p><p><span>完成後可以看到 node01 的 Status 為 </span><strong><span>SchedulingDisabled</span></strong><br>
<img src="https://i.imgur.com/y2Yvtu1.png" alt="" loading="lazy"></p><p><span>如果想要讓他恢復可被調度，使用 </span><strong><span>uncordon</span></strong><span> 後可以看到 Status 變回 Ready</span><br>
<img src="https://i.imgur.com/Uch5ONU.png" alt="" loading="lazy"></p><h3 id="3-Backup-amp-Restore-Methods" data-id="3-Backup-amp-Restore-Methods"><a class="anchor hidden-xs" href="#3-Backup-amp-Restore-Methods" title="3-Backup-amp-Restore-Methods"><span class="octicon octicon-link"></span></a><span>3. Backup &amp; Restore Methods</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10301931" target="_blank" rel="noopener"><span>Ref: ETCD與集群的備份與還原</span></a></p><p><span>Kubernetes Cluster Object 的備份方式有兩種</span></p><h4 id="1-儲存-YAML-檔" data-id="1-儲存-YAML-檔"><a class="anchor hidden-xs" href="#1-儲存-YAML-檔" title="1-儲存-YAML-檔"><span class="octicon octicon-link"></span></a><span>1. </span><ins><span>儲存 YAML 檔</span></ins></h4><p><span>備份所有 namespace 中的 deploy, service 狀態</span><br>
<code>    sudo kubectl get all -A -o yaml &gt; all-deploy-service.yaml    </code></p><h4 id="2-將ETCD-Cluster-使用etcdctl備份成快照檔" data-id="2-將ETCD-Cluster-使用etcdctl備份成快照檔"><a class="anchor hidden-xs" href="#2-將ETCD-Cluster-使用etcdctl備份成快照檔" title="2-將ETCD-Cluster-使用etcdctl備份成快照檔"><span class="octicon octicon-link"></span></a><span>2. </span><ins><span>將ETCD Cluster 使用etcdctl備份成快照檔</span></ins></h4><p><span>若發生問題需要還原時，再將快照檔還原</span></p><div class="alert alert-success">
<p><span>&lt;ETCD Cluster&gt;</span><br>
<span>Etcd 是 Kubernetes Cluster 中的一個十分重要的元件，用於保存 Cluster 所有的網路配置和對象的狀態訊息</span></p>
</div><p><span>etcd 的備份有兩種方式：</span></p><ul>
<li><ins><span>Built-in snapshot</span></ins><br>
<span>etcd 支持內建 snapshot，因此備份 etcd集群很容易。</span><br>
<span>可以使用etcdctl snapshot save 命令從集群內物件中獲取，也可以從當前未被 etcd process 使用的 etcd 資料目錄中複製 member/snap/db 文件。</span></li>
<li><ins><span>Volume snapshot</span></ins><br>
<span>如果 etcd 在支持備份的 Volume（例如Amazon Elastic Block Store）上運行，可以通過獲取存 Volume 的 snapshot來備份。</span></li>
</ul><h3 id="31-etcdctl-command" data-id="31-etcdctl-command"><a class="anchor hidden-xs" href="#31-etcdctl-command" title="31-etcdctl-command"><span class="octicon octicon-link"></span></a><span>3.1 etcdctl command</span></h3><ul>
<li><strong><ins><span>查看 etcd 的細節</span></ins></strong></li>
</ul><pre><code>k describe pod etcd-controlplane -n kube-system
</code></pre><ul>
<li><strong><ins><span>讓 etcdctl command 生效</span></ins></strong></li>
</ul><pre><code>ETCDCTL_API=3 etcdctl snapshot
export ETCDCTL_API=3
</code></pre><p><img src="https://i.imgur.com/To7DiXB.png" alt="" loading="lazy"></p><ul>
<li><strong><ins><span>Store 狀態</span></ins></strong><br>
<span>使用 etcdctl 保存到 /opt/snapshot-pre-boot.db</span></li>
</ul><pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
</code></pre><p><img src="https://i.imgur.com/ASx50RY.png" alt="" loading="lazy"></p><ul>
<li><strong><ins><span>Restore snapshot</span></ins></strong></li>
</ul><p><span>使用保存的 /opt/snapshot-pre-boot.db snapshot 去還原</span></p><pre><code>etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db
</code></pre><p><img src="https://i.imgur.com/w86Ycef.png" alt="" loading="lazy"></p><p><span>修改 /etc/kubernetes/manifests/etcd.yaml 文件內容</span></p><pre><code>  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
</code></pre><h2 id="Security" data-id="Security"><a class="anchor hidden-xs" href="#Security" title="Security"><span class="octicon octicon-link"></span></a><span>Security</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section7-Security/Kubernetes%2B-CKA-%2B0600%2B-%2BSecurity.pdf" target="_blank" rel="noopener"><span>Security.pdf</span></a></p><h3 id="1-Kubernetes-API-Server-Authentication" data-id="1-Kubernetes-API-Server-Authentication"><a class="anchor hidden-xs" href="#1-Kubernetes-API-Server-Authentication" title="1-Kubernetes-API-Server-Authentication"><span class="octicon octicon-link"></span></a><span>1. Kubernetes API Server Authentication</span></h3><p><span>kube-apiserver 是 kubernetes 的網關性質的元件，是 kubernetes cluster 資源操作的唯一入口</span><br>
<span>因此像是認證與授權等一些過程很明顯是要基於這個 kube-apiserve 元件</span><br>
<span>kubernetes cluster 的所有操作基本上都是通過 apiserver 這個元件進行的，它提供 HTTP Restful 形式的 API 供 cluster 内外 client 端使用</span></p><p><span>三步驟</span></p><ol>
<li><span>Authentication: 認證解决的問題是辨認用户的身份</span></li>
<li><span>Authorization: 授權是明確定義 user 具有哪些權限</span></li>
<li><span>Admission Control: 准入控制是作用於 kubernetes 中的資源對象</span></li>
</ol><p><img src="https://i.imgur.com/8U2SDMN.png" alt="" loading="lazy"></p><p><span>Kubernetes 中有幾種驗證方式：</span></p><ul>
<li><span>Certificate</span></li>
<li><span>Token</span></li>
<li><span>OpenID</span></li>
<li><span>Web Hook</span></li>
</ul><hr><ol>
<li><span>Identify the certificate file used for the </span><ins><span>kube-api server</span></ins><br>
<span>–&gt; /etc/kubernetes/pki/apiserver.crt</span></li>
<li><span>Identify the Certificate file used to authenticate kube-apiserver as </span><ins><span>a client to ETCD Server</span></ins><br>
<span>–&gt; /etc/kubernetes/pki/apiserver-etcd-client.crt</span></li>
<li><span>Identify the key used to authenticate kubeapi-server to the kubelet server</span><br>
<span>–&gt; /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></li>
</ol><ul>
<li><ins><span>/etc/kubernetes/manifests/kube-apiserver.yaml</span></ins></li>
</ul><p><img src="https://i.imgur.com/HhqHNNR.png" alt="" loading="lazy"></p><ol start="4">
<li><span>What is the Common Name (CN) configured on the Kube API Server Certificate?</span><br>
<span>–&gt; kube-apiserver</span></li>
</ol><pre><code>openssl x509 -in file-path.crt -text -noout
</code></pre><p><img src="https://i.imgur.com/mrutoM1.png" alt="" loading="lazy"></p><h3 id="2-Authorization" data-id="2-Authorization"><a class="anchor hidden-xs" href="#2-Authorization" title="2-Authorization"><span class="octicon octicon-link"></span></a><span>2. Authorization</span></h3><p><span>通過了 Authentication (身份認證)後，那僅能代表當前的使用者允許與 Kubernetes API Server 溝通，至於該使用者是否有權限(Permission)請求什麼資源，就是定義在 Authorization</span></p><p><span>Authorization Mode 有以下幾種模式：</span></p><ul>
<li><span>Node</span></li>
<li><span>ABAC (Attribute-based access control)</span></li>
<li><span>RBAC (Role-Base Access Control) -</span><strong><span>Default in k8s</span></strong></li>
<li><span>Webhook</span></li>
</ul><hr><ul>
<li><ins><span>Check the kube-apiserver settings:</span></ins><br>
<span>查看現有 cluster 的 Authorization 方法</span></li>
</ul><pre><code>cat /etc/kubernetes/manifests/kube-apiserver.yaml
</code></pre><p><img src="https://i.imgur.com/nC9N5G1.png" alt="" loading="lazy"></p><h4 id="21-Node" data-id="21-Node"><a class="anchor hidden-xs" href="#21-Node" title="21-Node"><span class="octicon octicon-link"></span></a><span>2.1 Node</span></h4><p><span>這是為了授權在每一個 node 上的 kubelet 所發出的 API request 所設計出來的，讓 kubelet 的 API request 可進行特定的權限控制</span></p><p><img src="https://i.imgur.com/ejswpYT.png" alt="" loading="lazy"></p><h4 id="22-ABAC" data-id="22-ABAC"><a class="anchor hidden-xs" href="#22-ABAC" title="22-ABAC"><span class="octicon octicon-link"></span></a><span>2.2 ABAC</span></h4><p><ins><span>Attribute-based access control</span></ins></p><p><span>此種方式就是在 master node 上保留一份 policy 文件，指定不同的使用者對於 resource 的存取權限，不彈性也不容易擴充，修改了 policy 文件之後還需要重新啟動 master node</span></p><p><img src="https://i.imgur.com/2v4K66G.png" alt="" loading="lazy"></p><h4 id="23-RBAC" data-id="23-RBAC"><a class="anchor hidden-xs" href="#23-RBAC" title="23-RBAC"><span class="octicon octicon-link"></span></a><span>2.3 RBAC</span></h4><p><ins><span>Role-Base Access Control</span></ins></p><p><span>RBAC API 中定義了 resource target，用來描述使用者以及 resource 之間的權限關係：</span></p><ul>
<li><span>Role：定義在特定 namespace 下的 resource 的存取權限</span></li>
<li><span>RoleBinding： 設定哪些使用者(or service account)與 role 綁定而擁有存取權限</span></li>
<li><span>ClusterRole：定義在整個 k8s cluster 下的 resource 的存取權限</span></li>
<li><span>ClusterRoleBinding：設定哪些使用者(or service account) 與 role 綁定而擁有存取權限</span></li>
</ul><p><img src="https://i.imgur.com/Jw38R2Y.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/aB1ufdX.png" alt="" loading="lazy"></p><h4 id="24-Webhook" data-id="24-Webhook"><a class="anchor hidden-xs" href="#24-Webhook" title="24-Webhook"><span class="octicon octicon-link"></span></a><span>2.4 Webhook</span></h4><p><span>這個模式是管理者在外部提供 HTTPS 授權服務，並設定 API server 透過與外部服務互動的方式進行授權</span></p><p><img src="https://i.imgur.com/snqwQKD.png" alt="" loading="lazy"></p><h3 id="3-KubeConfig" data-id="3-KubeConfig"><a class="anchor hidden-xs" href="#3-KubeConfig" title="3-KubeConfig"><span class="octicon octicon-link"></span></a><span>3. KubeConfig</span></h3><p><a href="https://www.akiicat.com/2019/04/24/Kubernetes/setup-kubernetes-configuration/" target="_blank" rel="noopener"><span> </span><strong><span>Context</span></strong><span> </span></a></p><div class="alert alert-success">
<p><span>要存取某個 Kubernetes 的 cluster，必須先設定好 Kubernetes 的 context，context 裡面會描述要如何存取到你的 Kubernetes 的 cluster</span><br>
<span>在 Kubernetes 裡面，切換不同的 cluster 是以 context 為單位</span><br>
<span>一個 context 裡面必需要三個元件，分別是 User、Server、Certification.</span><br>
<span>這三個東西說起來也很直觀，有個使用者 (User) 必須要有憑證 (Certification) 才能連到某個 Cluster (Server)。</span></p>
</div><ul>
<li><span>底下是一個 Context 所包含的內容</span><br>
<img src="https://i.imgur.com/2TXQw1X.png" alt="" loading="lazy"></li>
</ul><p><span>設定檔會放置在 ~/.kube/config (mac or linux)，下面內容表示 kubectl 已連接到 kubernetes-admin</span></p><ul>
<li><span>列出目前 kubectl 的設定內容</span></li>
</ul><pre><code>kubectl config view --&gt; 取得設定檔
</code></pre><p><img src="https://i.imgur.com/0kWmVQN.png" alt="" loading="lazy"></p><hr><ul>
<li><ins><span>Config yaml</span></ins><br>
<img src="https://i.imgur.com/A1B0YdH.png" alt="" loading="lazy"></li>
</ul><hr><p><span>管理多個 k8s cluster</span></p><pre><code>kubectl config use-context [NAME]
kubectl config get-contexts --&gt; 查詢有哪些 cluster 可以切換
kubectl config current-context --&gt; 目前正在管理的 cluster
kubectl cluster-info --&gt; 取得 cluster狀態
</code></pre><p><img src="https://i.imgur.com/Hp0ktrj.png" alt="" loading="lazy"></p><h3 id="4-Role" data-id="4-Role"><a class="anchor hidden-xs" href="#4-Role" title="4-Role"><span class="octicon octicon-link"></span></a><span>4. Role</span></h3><p><span>Role 即代表了 namespace 下的資源的權限，ClusterRole 則是 Cluster 層級資源的權限</span><br>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-example" target="_blank" rel="noopener"><span>Role example</span></a></p><pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
</code></pre><p><span>rules包含三個子欄位</span></p><ul>
<li><ins><span>apiGroups</span></ins><br>
<span>要使用的 API Group name。若此欄空白，則預設為 core API</span></li>
<li><ins><span>resources</span></ins><br>
<span>要對甚麼 resource 進行設定</span></li>
<li><ins><span>verb</span></ins><br>
<span>允許對resources進行的操作</span></li>
</ul><pre><code>rules可以一次設定好幾個，以 YAML 陣列區隔即可
</code></pre><p><img src="https://i.imgur.com/MBnAcWz.png" alt="" loading="lazy"></p><hr><h5 id="Example" data-id="Example"><a class="anchor hidden-xs" href="#Example" title="Example"><span class="octicon octicon-link"></span></a><span>Example</span></h5><p><span>kube-proxy 這個 role 可以拿到 kube-proxy namespace 下的 configmaps</span></p><p><img src="https://i.imgur.com/YpH1ef5.png" alt="" loading="lazy"></p><h4 id="Question-1" data-id="Question-1"><a class="anchor hidden-xs" href="#Question-1" title="Question-1"><span class="octicon octicon-link"></span></a><span>Question 1</span></h4><p><img src="https://i.imgur.com/hhRbrAh.png" alt="" loading="lazy"></p><pre><code>k create role developer --verb=list,create,delete --resource=pods
k describe role developer
k create rolebinding dev-user-binding --role=developer --user=dev-user
k describe rolebindings dev-user-binding
</code></pre><p><img src="https://i.imgur.com/dNLmr6D.png" alt="" loading="lazy"></p><h4 id="Question-2" data-id="Question-2"><a class="anchor hidden-xs" href="#Question-2" title="Question-2"><span class="octicon octicon-link"></span></a><span>Question 2</span></h4><p><span>需要增加角色權限 (原本為 Forbidden)</span><br>
<img src="https://i.imgur.com/gm3UWfS.png" alt="" loading="lazy"></p><p><span>我們用 describe 去看，發現 developer role 可以看得 Resource Names 是 blue-app 而非題目想要我們看的資源 dark-blue-app, 所以這就是問題所在</span></p><p><img src="https://i.imgur.com/AZpdJkL.png" alt="" loading="lazy"></p><pre><code>k edit role developer -n blue
k --as dev-user get pods dark-blue-app -n blue
</code></pre><p><span>edit 進去把 rules 底下的 resourceNames 改成 dark-blue-app 即可</span><br>
<img src="https://i.imgur.com/IyV9sEG.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/TA3WMqK.png" alt="" loading="lazy"></p><hr><h3 id="5-Service-Account" data-id="5-Service-Account"><a class="anchor hidden-xs" href="#5-Service-Account" title="5-Service-Account"><span class="octicon octicon-link"></span></a><span>5. Service Account</span></h3><p><span>Kubernetes 的帳號有兩種類型，分別為：</span></p><ul>
<li>
<p><ins><span>使用者帳戶 (Normal Users)</span></ins><br>
<span>任何人想要連接並存取 Kubernetes 叢集，都需要先建立一個 “使用者帳戶” 並將憑證資訊提供給用戶端 (如: kubectl)，以便通過 Kubernetes 的 API server 的認證 (Authentication)</span><br>
<span>我覺得這個名字應該稱為 User Accounts 會比較好理解，但是 Kubernetes 官網稱一般使用者 (Normal Users)</span></p>
</li>
<li>
<p><ins><span>服務帳戶 (Service Accounts)</span></ins><br>
<span>任何跑在 Pod 裡面的容器想要存取 Kubernetes 的 API 伺服器 (kube-apiserver)，就需要先有一個 “服務帳戶” 綁定在 Pod 身上，然後以便通過 Kubernetes 的 API 伺服器的身份認證 (Authentication)</span></p>
</li>
</ul><pre><code>kubectl create serviceaccount dashboard-sa
kubectl get serviceaccount
kubectl describe serviceaccount dashboard-sa
</code></pre><p><span>當創建 pod 的時候，如果没有指定一個 service account，系统會自動的在與該 pod 相同的 namespace 下幫其指派一個 default service account</span><br>
<span>如果 describe pod 的原始 json 或 yaml 訊息（例如使用 kubectl get pods/podename -o yaml 命令），我們可以看到 spec.serviceAccountName 已经被設置為 automatically</span></p><pre><code>k get pods &lt;pod-name&gt; -o yaml
</code></pre><p><img src="https://i.imgur.com/h2LEhMA.png" alt="" loading="lazy"></p><p><span>這樣就可以 在 pod 中使用自動掛載的 service account 憑證來訪問 API.</span></p><ul>
<li><span>You can opt out of automounting API credentials on </span><ins><span>/var/run/secrets/kubernetes.io/serviceaccount/token</span></ins><span> for a service account by setting automountServiceAccountToken: false on the ServiceAccount</span></li>
</ul><p><span>選擇取消 service account 自訂掛載 API 憑證，只需在 service account 中設置 </span><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener"><span>automountServiceAccountToken: false</span></a></p><pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
...
</code></pre><ul>
<li><span>You can also opt out of automounting API credentials for a particular Pod:</span></li>
</ul><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
</code></pre><h3 id="6-Image-Sercurity" data-id="6-Image-Sercurity"><a class="anchor hidden-xs" href="#6-Image-Sercurity" title="6-Image-Sercurity"><span class="octicon octicon-link"></span></a><span>6. Image Sercurity</span></h3><p><span>如果今天要從本地端去抓取一個 private container registry，我們第一件要做的事情就是 docekr login.</span><br>
<span>對於 Kubernetes 來說，其會使用 secret 的特殊型態 docker-registry 作為登入任何 private container registry 的帳號密碼來源.</span></p><p><ins><span>這邊有兩種方式可以使用</span></ins></p><ul>
<li><span>第一種是先透過 docker login 登入，之後將登入後的設定檔案送給 Kubernetes secret 物件</span></li>
<li><span>第二種則是創建 Kubernetes secret 時使用明碼的帳號密碼</span></li>
</ul><pre><code>kubectl create secret
k create secret docker-registry --&gt; 從 private harbor 拉 image
</code></pre><p><img src="https://i.imgur.com/3PkNc8g.png" alt="" loading="lazy"></p><h4 id="Commandline-create-secret-object" data-id="Commandline-create-secret-object"><a class="anchor hidden-xs" href="#Commandline-create-secret-object" title="Commandline-create-secret-object"><span class="octicon octicon-link"></span></a><span>Commandline create secret object</span></h4><p><img src="https://i.imgur.com/lB2YhtN.png" alt="" loading="lazy"></p><pre><code>kubectl create secret docker-registry -h

k create secret docker-registry private-reg-cred \ 
--docker-server=myprivateregistry.com:5000 --docker-username=dock_user \ 
--docker-password=dock_password --docker-email=dock_user@myprivateregistry.com
</code></pre><p><img src="https://i.imgur.com/nI0u0P8.png" alt="" loading="lazy"></p><h4 id="Create-a-Pod-that-uses-your-Secret" data-id="Create-a-Pod-that-uses-your-Secret"><a class="anchor hidden-xs" href="#Create-a-Pod-that-uses-your-Secret" title="Create-a-Pod-that-uses-your-Secret"><span class="octicon octicon-link"></span></a><span>Create a Pod that uses your Secret</span></h4><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret" target="_blank" rel="noopener"><span>k8s.io</span></a></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagePullSecrets:
  - name: regcred
</code></pre><h3 id="7-Network" data-id="7-Network"><a class="anchor hidden-xs" href="#7-Network" title="7-Network"><span class="octicon octicon-link"></span></a><span>7. Network</span></h3><p><span>網路策略(Network Policy) 是用在控制Pod之間如何溝通以及如何與其他網路溝通的規範, 目的在幫K8s 實現更精細的流量控制以及租戶隔離機制.</span><br>
<span>K8s 提供一個 NetworkPolicy 供使用, 有效範圍是整個Namespace.</span></p><p><span>NetworkPolicy 會使用標籤選擇器定義一組Pod作為控制對象, 管控入站流量的是Ingress, 負責出站流量的是Egress, 兩個可以共用, 負責規範生效範圍的是 spec.policyType</span></p><p><span>Pod 預設可以接收任何來源的流量, 也可以向外部發出期望的所有流量, 一旦Nameapace 中有NetworkPolicy規範Pod, Pod就會依照NetworkPolicy的規範拒絕請求, 並且若是在spec中定義了沒有規則的Ingress或Egress就會造成拒絕相關的一切流量。</span></p><p><img src="https://i.imgur.com/vpBuGpN.png" alt="" loading="lazy"></p><hr><h4 id="Ingress-入站流量管控" data-id="Ingress-入站流量管控"><a class="anchor hidden-xs" href="#Ingress-入站流量管控" title="Ingress-入站流量管控"><span class="octicon octicon-link"></span></a><span>Ingress 入站流量管控</span></h4><div class="alert alert-warning">
<p><ins><span>Ingress 字段是一個列表, 主要有兩個字段組成:</span></ins></p>
<ol>
<li><span>from &lt;[]Object&gt; : 可訪問的Pod列表, 如果設定多項則判斷邏輯為"聯集", 是一個白名單的概念.</span></li>
<li><span>ports &lt;[]Object&gt;: 可訪問的Pod上面的允許Port列表, 有設定就就是白名單.</span></li>
</ol>
</div><p><img src="https://i.imgur.com/Y6aK1A2.png" alt="" loading="lazy"></p><ul>
<li><ins><span>拒絕所有流量配置 YAML</span></ins><br>
<span>podSelector : pod 標籤選擇器, 給空值代表全選</span><br>
<span>port : 未定義時匹配所有端口</span></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
spec:
  podSelector:{}
  policyType:["Ingress"]
</code></pre><ul>
<li><ins><span>接收所有流量 YAML</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector:{}
  policyType:["Ingress"]
  ingress:
  - {}
</code></pre><h4 id="Egress-出站流量管控" data-id="Egress-出站流量管控"><a class="anchor hidden-xs" href="#Egress-出站流量管控" title="Egress-出站流量管控"><span class="octicon octicon-link"></span></a><span>Egress 出站流量管控</span></h4><p><img src="https://i.imgur.com/C59SpVx.png" alt="" loading="lazy"></p><div class="alert alert-warning">
<p><ins><span>Egress 字段是一個列表, 主要有兩個字段組成:</span></ins></p>
<ol>
<li><span>to &lt;[]Object&gt;: 可訪問的Pod列表, 如果設定多項則判斷邏輯為 “聯集”, 是一個白名單的概念.</span></li>
<li><span>ports &lt;[]Object&gt; : 可訪問的Pod上面的允許Port列表, 有設定就就是白名單.</span></li>
</ol>
</div><ul>
<li><ins><span>拒絕所有流量配置 YAML</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
spec:
  podSelector:{}
  policyType:["Egress"]
</code></pre><ul>
<li><ins><span>接收所有流量 YAML配置</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector:{}
  policyType:["Egress"]
  egress:
  - {}
</code></pre><h3 id="8-Network-Policies" data-id="8-Network-Policies"><a class="anchor hidden-xs" href="#8-Network-Policies" title="8-Network-Policies"><span class="octicon octicon-link"></span></a><span>8. Network Policies</span></h3><pre><code>k get networkpolicie
k get netpol
k describe netpol &lt;networkpolicie-name&gt;
</code></pre><p><img src="https://i.imgur.com/6V34uN0.png" alt="" loading="lazy"></p><p><span>上面這個設定代表只允許 pod=interval 透過 8080 port 訪問 pod=payroll</span></p><p><img src="https://i.imgur.com/aM6eZdQ.png" alt="" loading="lazy"></p><h4 id="Create-a-network-policy" data-id="Create-a-network-policy"><a class="anchor hidden-xs" href="#Create-a-network-policy" title="Create-a-network-policy"><span class="octicon octicon-link"></span></a><ins><span>Create a network policy</span></ins></h4><p><img src="https://i.imgur.com/wzsw9T6.png" alt="" loading="lazy"></p><pre><code>k create -f internal.yaml
</code></pre><p><img src="https://i.imgur.com/Xkt1qWu.png" alt="" loading="lazy"></p><pre><code># internal.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              role: payroll
      ports:
        - protocol: TCP
          port: 8080
    - to:
        - podSelector:
            matchLabels:
              role: mysql
      ports:
        - protocol: TCP
          port: 3306
</code></pre><h2 id="Storage" data-id="Storage"><a class="anchor hidden-xs" href="#Storage" title="Storage"><span class="octicon octicon-link"></span></a><span>Storage</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section8-Storage/Kubernetes%2B-CKA-%2B0700%2B-%2BStorage.pdf" target="_blank" rel="noopener"><span>Kubernetes-Storage.pdf</span></a></p><p><span>要將容器中的檔案留存在主機上，Docker 有三種作法：</span></p><ol>
<li><ins><span>volume</span></ins><br>
<span>Volume 存放在主機檔案系統中由 Docker 管理的地方，在 Linux 作業系統是 /var/lib/docker/volumes/ 此路徑。非 Docker 的行程不應該修改檔案系統中的這一部分.</span><br>
<span>要在 Docker 中留存資料，volumes 是最好的方法</span></li>
<li><ins><span>bind mount</span></ins><br>
<span>可存放在主機檔案系統中的任何地方，非 Docker 行程或 Docker 容器可隨時修改。</span></li>
<li><ins><span>tmpfs mount</span></ins><span> (only 在 Linux 作業系統上的 Docker)</span><br>
<span>只存放在主機的記憶體中，不會寫入主機的檔案系統</span></li>
</ol><ul>
<li><span>這三種方式的差異可用下圖表示</span></li>
</ul><p><img src="https://i.imgur.com/JLhgr7g.png" alt="" loading="lazy"></p><hr><ul>
<li><ins><span>Volume driver &amp; Storage driver</span></ins></li>
</ul><p><img src="https://i.imgur.com/VJ6Uhts.png" alt="" loading="lazy"></p><h3 id="1-Volume" data-id="1-Volume"><a class="anchor hidden-xs" href="#1-Volume" title="1-Volume"><span class="octicon octicon-link"></span></a><span>1. Volume</span></h3><div class="alert alert-success">
<p><span>Volume 可以是一個 Node 或是 一個雲端儲存平台，是 K8s 實現儲存資料的方.</span><br>
<span>透過將 Volume mount 到Pod上，就可以實現儲存Pod的資料</span></p>
</div><h3 id="11-Volume-Type" data-id="11-Volume-Type"><a class="anchor hidden-xs" href="#11-Volume-Type" title="11-Volume-Type"><span class="octicon octicon-link"></span></a><span>1.1 Volume Type</span></h3><p><span>multinode cluster 不建議使用 hostPath, 因為每個 node 中的路徑不一定會相同</span><br>
<img src="https://i.imgur.com/sMyQSFE.png" alt="" loading="lazy"></p><p><span>K8S 提供的 Vloume類型非常多，下面舉兩個為例子:</span></p><h4 id="1-emptyDir" data-id="1-emptyDir"><a class="anchor hidden-xs" href="#1-emptyDir" title="1-emptyDir"><span class="octicon octicon-link"></span></a><ins><span>1. emptyDir</span></ins></h4><p><span>當 Pod 調度某給Node時，首先創建 emptyDir Volume，並且該Pod在該Node上運行時就存在。</span><br>
<span>顧名思義，它最初是空的。Pod中的容器都可以在emptyDir Volume中讀取和寫入相同的文件，儘管該Volume可以安裝在每個 Container 中的相同或不同路徑上。</span><br>
<span>當 Pod 從 Node 中被刪除時，emptyDir 中的數據也將被刪除。</span><br>
<span>emptyDir 的用途包含：</span></p><ol>
<li><span>臨時空間，例如用於某些應用程式執行階段所需的臨時目錄，且無須永久保存</span></li>
<li><span>長時間工作的中間過程 CheckPoint 的臨時儲存目錄</span></li>
<li><span>多容器共用目錄</span></li>
</ol><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
</code></pre><h4 id="2-hostPath" data-id="2-hostPath"><a class="anchor hidden-xs" href="#2-hostPath" title="2-hostPath"><span class="octicon octicon-link"></span></a><ins><span>2. hostPath</span></ins></h4><p><span>hostPath Volume 將 Node檔案系統中的檔案或目錄 mount到 Pod 中。</span><br>
<span>hostPath 的生命週期與 Node 相同，並不會隨著 Pod消失而消失。</span><br>
<span>hostPath的用途包含：</span></p><ol>
<li><span>容器應用程式產生的紀錄檔需要永久儲存時，可以使用Node的檔案系統進行儲存</span></li>
<li><span>需要存取Node上Docker內部資料結構的容器應用時，可以透過定義 hostPath 為 Node 的 /var/lib/docker 目錄，使容器可以直接存取 Docker的檔案系統</span></li>
</ol><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
</code></pre><h3 id="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" data-id="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC"><a class="anchor hidden-xs" href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC"><span class="octicon octicon-link"></span></a><span>2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</span></h3><p><span>PersistentVolume 是存放資源的地方,簡單想像的話就是個 Disk 空間,PersistentVolume 分為兩種:</span></p><ol>
<li><span>靜態：手動建立 PersistentVolume 稱為「靜態」綁定</span></li>
<li><span>動態：PVC在批配 PV時，若不符合規則會透過 StorageClass 自動建立新的PV，此時PV我們會稱為「動態」綁定，並且繼承 StorageClass 規定的回收政策</span></li>
</ol><hr><pre><code>kubectl get persistentvolumeclaim
kubectl delete persistentvolumeclaim &lt;my-claim&gt; --&gt; 刪除 persistentvolumeclaim
</code></pre><p><img src="https://i.imgur.com/rfXbxhu.png" alt="" loading="lazy"></p><hr><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"><span>PersistentVolumeClaim</span></a><span> 可以透過我們設定好的 Storage Class 的模板，創建出我們所需要的資源.</span></p><div class="alert alert-success">
<p><span>PVC 提供三種與 PV 中的檔案存取模式：</span><br>
<span>1.ReadWriteOnce：只可以掛載在同一個 Node 上提供讀寫功能.</span><br>
<span>2.ReadOnlyMany ：可以在多個 Node 上提供讀取功能.</span><br>
<span>3.ReadWriteMany：可以在多個 Node 上提供讀寫功能.</span></p>
</div><ul>
<li><span>PVC 該如何與 PV 進行綁定:</span>
<ul>
<li><span>透過 storageClassName 名稱，找到相同 PV.</span></li>
<li><span>透過 Label 標籤，找到相同 PV.</span></li>
</ul>
</li>
</ul><h4 id="Create-Persistent-Volumes" data-id="Create-Persistent-Volumes"><a class="anchor hidden-xs" href="#Create-Persistent-Volumes" title="Create-Persistent-Volumes"><span class="octicon octicon-link"></span></a><span>Create Persistent Volumes</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes" target="_blank" rel="noopener"><span>k8s io</span></a></p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
</code></pre><p><span>左邊是題目, 右邊是答案</span><br>
<img src="https://i.imgur.com/L9kkLox.png" alt="" loading="lazy"></p><h4 id="Create-Persistent-Volumes-Claim" data-id="Create-Persistent-Volumes-Claim"><a class="anchor hidden-xs" href="#Create-Persistent-Volumes-Claim" title="Create-Persistent-Volumes-Claim"><span class="octicon octicon-link"></span></a><span>Create Persistent Volumes Claim</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"><span>k8s io</span></a></p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
</code></pre><p><img src="https://i.imgur.com/H3jaJHB.png" alt="" loading="lazy"></p><h3 id="3-Storage-Class" data-id="3-Storage-Class"><a class="anchor hidden-xs" href="#3-Storage-Class" title="3-Storage-Class"><span class="octicon octicon-link"></span></a><span>3. Storage Class</span></h3><p><span>透過 Kubernetes 提供的 </span><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" target="_blank" rel="noopener"><span>Storage Class</span></a><span> 元件，我們可以依據需求，根據 Volumes 的提供者 (provisioner)、類型 (type)、所在地 (Region)，以及回收政策 (reclaimPolicy) 去定義不同的 Storage Class.</span></p><p><img src="https://i.imgur.com/DtbBII2.png" alt="" loading="lazy"></p><p><span>當用戶在建立 Pod 服務使用到 PVC (PersistentVolumeClaim)，這時會自動找一個符合的 PV (PersistentVolume)進行批配，</span><br>
<span>若有批配到就直接進行綁定 (此時表示與 PV 進行「靜態」批配)</span><br>
<span>但是如果沒有符合的 PV，則會透過 StorageClass 建立一個新的 PV 再和 PVC 綁定. (此時表示與PV進行「動態」批配)</span></p><p><span>系統管理人員負責建置 PV ，而開發人員則是負責建立 PVC 與 Storage Class，並交由 PVC 自動尋找合適的 PV進行綁定，或者透過StorageClass建立一個新的PV再和PVC綁定</span></p><p><img src="https://i.imgur.com/JCDMq5R.jpg" alt="" loading="lazy"></p><pre><code>kubectl get sc --&gt; 看 Storage Class
</code></pre><h4 id="Create-Storage-Class" data-id="Create-Storage-Class"><a class="anchor hidden-xs" href="#Create-Storage-Class" title="Create-Storage-Class"><span class="octicon octicon-link"></span></a><span>Create Storage Class</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource" target="_blank" rel="noopener"><span>k8s io</span></a></p><pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
</code></pre><p><img src="https://i.imgur.com/zyHx54p.png" alt="" loading="lazy"></p><h2 id="Troubleshooting" data-id="Troubleshooting"><a class="anchor hidden-xs" href="#Troubleshooting" title="Troubleshooting"><span class="octicon octicon-link"></span></a><span>Troubleshooting</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section13-Troubleshooting/Kubernetes-CKA-1000-Troubleshooting.pdf" target="_blank" rel="noopener"><span>Troubleshooting.pdf</span></a></p><p><span>這類考題通常會提供一個 K8s 集群環境，在環境中有一些 bug，例如 Pod 無法新增、kube-apiserver 故障或路徑問題等等，要求你解決這些問題.</span></p><p><a href="https://ithelp.ithome.com.tw/articles/10246043" target="_blank" rel="noopener"><span>Ref</span></a><br>
<ins><strong><span>Trobleshooting方法</span></strong></ins><br>
<span>為了追蹤和找出 K8s 集群中執行的容器應用出現的問題，經常透過以下這些查錯方法：</span></p><ol>
<li><span>檢視 K8s 目前執行的階段資訊，特別是與物件連接的Event事件.</span><br>
<span>這些事件紀錄了相關主題、發生時間、最近發生時間、發生次數及事件原因等，對於Trobleshooting很有幫助.</span><br>
<span>此外，透過檢視物件的執行階段資料，我們還可以發現參數錯誤、連結錯誤、狀態例外等問題.</span></li>
<li><span>對於服務、容器方面的問題，可能需要深入容器內部進行診斷，此時可以透過檢視容器的執行記錄檔來找出問題</span></li>
<li><span>對於某些複雜問題，例如 Pod 調度排程這類的問題，涉及到整個集群的節點，因此可能需要查找節點內的服務紀錄檔來debug.</span><br>
<span>例如蒐集 Control Plane 上的kube-apiserver、kube-schedule、kube-controler-manager，Node 上的 kubelet 及 kube-proxy 等等物件的log.</span></li>
</ol><h3 id="常見問題" data-id="常見問題"><a class="anchor hidden-xs" href="#常見問題" title="常見問題"><span class="octicon octicon-link"></span></a><span>常見問題</span></h3><ul>
<li>
<p><ins><span>Control Plane Failure</span></ins><br>
<span>這類問題包含 Pod 無法調度、認證沒通過，找不到目標檔案等等。建議解決方法可透過：</span><br>
<span>檢查 kube-system的物件</span><br>
<span>檢查 static pod路徑</span><br>
<span>要如何確認 Pod 運行在哪個Node上，可以透過kubectl get po -o wide命令，或看 Pod 名子，名子後面有接 master 的，通常代表運行在Control Plane上，可從Control Plane下手</span></p>
</li>
<li>
<p><ins><span>Pod 處於 Pending STATUS</span></ins><br>
<span>可能是 image不存在，檢查 image 的 Server (如Docker Hub)是否正常運行，或是網路狀況等等</span></p>
</li>
<li>
<p><ins><span>Service</span></ins><br>
<span>通常是 Service 的 Port mapping沒設定好，或是 Service 沒選取到Pod，可以檢查 Pod 的 label 和 Service 的 Selector</span></p>
</li>
<li>
<p><ins><span>NetworkPolicy</span></ins><br>
<span>檢查 NetworkPolicy 的 ingress和egress IP是否設定正確</span></p>
</li>
</ul><h3 id="Task-1" data-id="Task-1"><a class="anchor hidden-xs" href="#Task-1" title="Task-1"><span class="octicon octicon-link"></span></a><span>Task 1</span></h3><p><span>應用程式 port 可以通，但是 mysql-service 跟 webapp-mysql 連接上可能有些問題</span></p><p><img src="https://i.imgur.com/QUVmZ0c.png" alt="" loading="lazy"></p><pre><code>kubectl config set-context --current --namespace=alpha
curl http://localhost:30081
</code></pre><p><span>curl 打 30081 port 從 error message 出發</span></p><p><img src="https://i.imgur.com/Au9yoR1.png" alt="" loading="lazy"></p><pre><code>k describe deployments
k get svc
k edit svc mysql
k delete svc mysql
k create -f xxxx.yaml
</code></pre><p><span>發現 deployment 的 webapp-mysql container 跟 k8s 上的 service name 對不起來，那這就是錯誤所在, edit svc 內容的名稱後重啟即可</span></p><p><img src="https://i.imgur.com/FysbTRk.png" alt="" loading="lazy"></p><h2 id="Others" data-id="Others"><a class="anchor hidden-xs" href="#Others" title="Others"><span class="octicon octicon-link"></span></a><span>Others</span></h2><h3 id="22-Volume-Test" data-id="22-Volume-Test"><a class="anchor hidden-xs" href="#22-Volume-Test" title="22-Volume-Test"><span class="octicon octicon-link"></span></a><span>22. Volume-Test</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10241004" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/1co2rP7.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><p><span>1.創建一個 Pod</span><br>
<span>2.mount 到類型為emptyDir的Volume中</span></p><pre><code>1.sudo kubectl run redis-storage --image=redis --dry-run=client -o yaml &gt; q7-pod.yaml
</code></pre><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q7.yaml" target="_blank" rel="noopener"><span>q7.yaml</span></a></p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Kubectl" title="Kubectl">Kubectl</a><ul class="nav">
<li><a href="#Tips" title="Tips">Tips</a><ul class="nav">
<li><a href="#查看-K8s-所有物件和它們的縮寫" title="查看 K8s 所有物件和它們的縮寫">查看 K8s 所有物件和它們的縮寫</a></li>
<li><a href="#Pod-amp-yaml" title="Pod &amp; yaml">Pod &amp; yaml</a></li>
<li><a href="#Deployment" title="Deployment">Deployment</a></li>
<li><a href="#Service" title="Service">Service</a></li>
</ul>
</li>
<li><a href="#Core-Concepts" title="Core Concepts">Core Concepts</a><ul class="nav">
<li><a href="#1-Label" title="1. Label">1. Label</a></li>
<li><a href="#2-Deployment-amp-ReplicaSet-amp-Replication-Controller" title="2. Deployment &amp; ReplicaSet &amp; Replication Controller">2. Deployment &amp; ReplicaSet &amp; Replication Controller</a></li>
<li><a href="#3-Service" title="3. Service">3. Service</a></li>
<li><a href="#4-Namespace" title="4. Namespace">4. Namespace</a></li>
<li><a href="#5-Imperative-vs-Declarative" title="5. Imperative vs Declarative">5. Imperative vs Declarative</a></li>
</ul>
</li>
<li><a href="#Scheduling" title="Scheduling">Scheduling</a><ul class="nav">
<li><a href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0. Scheduling - nodeName &amp; nodeSelectors">0. Scheduling - nodeName &amp; nodeSelectors</a></li>
<li><a href="#1-Taints-amp-Tolerations" title="1. Taints &amp; Tolerations">1. Taints &amp; Tolerations</a></li>
<li><a href="#2-Affinity-amp-Anti-Affinity" title="2. Affinity &amp; Anti-Affinity">2. Affinity &amp; Anti-Affinity</a></li>
<li><a href="#3-Kubernetes-Resources---RequestLimit" title="3. Kubernetes Resources - Request/Limit">3. Kubernetes Resources - Request/Limit</a></li>
<li><a href="#4-Demonsets" title="4. Demonsets">4. Demonsets</a></li>
<li><a href="#5-Static-pod" title="5. Static pod">5. Static pod</a></li>
</ul>
</li>
<li><a href="#Logging-amp-Monitoring" title="Logging &amp; Monitoring">Logging &amp; Monitoring</a><ul class="nav">
<li><a href="#1-K8s-Monitor-Cluster-Components" title="1. K8s Monitor Cluster Components">1. K8s Monitor Cluster Components</a></li>
<li><a href="#2-Metric-Server" title="2. Metric Server">2. Metric Server</a></li>
</ul>
</li>
<li><a href="#Application-Lifecycle-Managment" title="Application Lifecycle Managment">Application Lifecycle Managment</a><ul class="nav">
<li><a href="#1-Rolling-Update-amp-Rollbacks" title="1. Rolling Update &amp; Rollbacks">1. Rolling Update &amp; Rollbacks</a></li>
<li><a href="#2-ConfigMap" title="2. ConfigMap">2. ConfigMap</a></li>
<li><a href="#3-Secret" title="3. Secret">3. Secret</a></li>
</ul>
</li>
<li><a href="#Cluster-Maintenance" title="Cluster Maintenance">Cluster Maintenance</a><ul class="nav">
<li><a href="#0-Kubernets-Software-Versions" title="0. Kubernets Software Versions">0. Kubernets Software Versions</a></li>
<li><a href="#1-Cluster-Upgrade" title="1. Cluster Upgrade">1. Cluster Upgrade</a></li>
<li><a href="#11-Upgrading-control-plane-nodes" title="1.1 Upgrading control plane nodes">1.1 Upgrading control plane nodes</a></li>
<li><a href="#12-Upgrade-worker-nodes" title="1.2 Upgrade worker nodes">1.2 Upgrade worker nodes</a></li>
<li><a href="#2-Node-Maintenance" title="2. Node Maintenance">2. Node Maintenance</a></li>
<li><a href="#3-Backup-amp-Restore-Methods" title="3. Backup &amp; Restore Methods">3. Backup &amp; Restore Methods</a></li>
<li><a href="#31-etcdctl-command" title="3.1 etcdctl command">3.1 etcdctl command</a></li>
</ul>
</li>
<li><a href="#Security" title="Security">Security</a><ul class="nav">
<li><a href="#1-Kubernetes-API-Server-Authentication" title="1. Kubernetes API Server Authentication">1. Kubernetes API Server Authentication</a></li>
<li><a href="#2-Authorization" title="2. Authorization">2. Authorization</a></li>
<li><a href="#3-KubeConfig" title="3. KubeConfig">3. KubeConfig</a></li>
<li><a href="#4-Role" title="4. Role">4. Role</a></li>
<li><a href="#5-Service-Account" title="5. Service Account">5. Service Account</a></li>
<li><a href="#6-Image-Sercurity" title="6. Image Sercurity">6. Image Sercurity</a></li>
<li><a href="#7-Network" title="7. Network">7. Network</a></li>
<li><a href="#8-Network-Policies" title="8. Network Policies">8. Network Policies</a></li>
</ul>
</li>
<li class=""><a href="#Storage" title="Storage">Storage</a><ul class="nav">
<li><a href="#1-Volume" title="1. Volume">1. Volume</a></li>
<li><a href="#11-Volume-Type" title="1.1 Volume Type">1.1 Volume Type</a></li>
<li><a href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)">2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</a></li>
<li><a href="#3-Storage-Class" title="3. Storage Class">3. Storage Class</a></li>
</ul>
</li>
<li><a href="#Troubleshooting" title="Troubleshooting">Troubleshooting</a><ul class="nav">
<li><a href="#常見問題" title="常見問題">常見問題</a></li>
<li><a href="#Task-1" title="Task 1">Task 1</a></li>
</ul>
</li>
<li><a href="#Others" title="Others">Others</a><ul class="nav">
<li><a href="#22-Volume-Test" title="22. Volume-Test">22. Volume-Test</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li class=""><a href="#Kubectl" title="Kubectl">Kubectl</a><ul class="nav">
<li><a href="#Tips" title="Tips">Tips</a><ul class="nav">
<li><a href="#查看-K8s-所有物件和它們的縮寫" title="查看 K8s 所有物件和它們的縮寫">查看 K8s 所有物件和它們的縮寫</a></li>
<li><a href="#Pod-amp-yaml" title="Pod &amp; yaml">Pod &amp; yaml</a></li>
<li><a href="#Deployment" title="Deployment">Deployment</a></li>
<li><a href="#Service" title="Service">Service</a></li>
</ul>
</li>
<li><a href="#Core-Concepts" title="Core Concepts">Core Concepts</a><ul class="nav">
<li><a href="#1-Label" title="1. Label">1. Label</a></li>
<li><a href="#2-Deployment-amp-ReplicaSet-amp-Replication-Controller" title="2. Deployment &amp; ReplicaSet &amp; Replication Controller">2. Deployment &amp; ReplicaSet &amp; Replication Controller</a></li>
<li><a href="#3-Service" title="3. Service">3. Service</a></li>
<li><a href="#4-Namespace" title="4. Namespace">4. Namespace</a></li>
<li><a href="#5-Imperative-vs-Declarative" title="5. Imperative vs Declarative">5. Imperative vs Declarative</a></li>
</ul>
</li>
<li><a href="#Scheduling" title="Scheduling">Scheduling</a><ul class="nav">
<li><a href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0. Scheduling - nodeName &amp; nodeSelectors">0. Scheduling - nodeName &amp; nodeSelectors</a></li>
<li><a href="#1-Taints-amp-Tolerations" title="1. Taints &amp; Tolerations">1. Taints &amp; Tolerations</a></li>
<li><a href="#2-Affinity-amp-Anti-Affinity" title="2. Affinity &amp; Anti-Affinity">2. Affinity &amp; Anti-Affinity</a></li>
<li><a href="#3-Kubernetes-Resources---RequestLimit" title="3. Kubernetes Resources - Request/Limit">3. Kubernetes Resources - Request/Limit</a></li>
<li><a href="#4-Demonsets" title="4. Demonsets">4. Demonsets</a></li>
<li><a href="#5-Static-pod" title="5. Static pod">5. Static pod</a></li>
</ul>
</li>
<li><a href="#Logging-amp-Monitoring" title="Logging &amp; Monitoring">Logging &amp; Monitoring</a><ul class="nav">
<li><a href="#1-K8s-Monitor-Cluster-Components" title="1. K8s Monitor Cluster Components">1. K8s Monitor Cluster Components</a></li>
<li><a href="#2-Metric-Server" title="2. Metric Server">2. Metric Server</a></li>
</ul>
</li>
<li><a href="#Application-Lifecycle-Managment" title="Application Lifecycle Managment">Application Lifecycle Managment</a><ul class="nav">
<li><a href="#1-Rolling-Update-amp-Rollbacks" title="1. Rolling Update &amp; Rollbacks">1. Rolling Update &amp; Rollbacks</a></li>
<li><a href="#2-ConfigMap" title="2. ConfigMap">2. ConfigMap</a></li>
<li><a href="#3-Secret" title="3. Secret">3. Secret</a></li>
</ul>
</li>
<li><a href="#Cluster-Maintenance" title="Cluster Maintenance">Cluster Maintenance</a><ul class="nav">
<li><a href="#0-Kubernets-Software-Versions" title="0. Kubernets Software Versions">0. Kubernets Software Versions</a></li>
<li><a href="#1-Cluster-Upgrade" title="1. Cluster Upgrade">1. Cluster Upgrade</a></li>
<li><a href="#11-Upgrading-control-plane-nodes" title="1.1 Upgrading control plane nodes">1.1 Upgrading control plane nodes</a></li>
<li><a href="#12-Upgrade-worker-nodes" title="1.2 Upgrade worker nodes">1.2 Upgrade worker nodes</a></li>
<li><a href="#2-Node-Maintenance" title="2. Node Maintenance">2. Node Maintenance</a></li>
<li><a href="#3-Backup-amp-Restore-Methods" title="3. Backup &amp; Restore Methods">3. Backup &amp; Restore Methods</a></li>
<li><a href="#31-etcdctl-command" title="3.1 etcdctl command">3.1 etcdctl command</a></li>
</ul>
</li>
<li><a href="#Security" title="Security">Security</a><ul class="nav">
<li><a href="#1-Kubernetes-API-Server-Authentication" title="1. Kubernetes API Server Authentication">1. Kubernetes API Server Authentication</a></li>
<li><a href="#2-Authorization" title="2. Authorization">2. Authorization</a></li>
<li><a href="#3-KubeConfig" title="3. KubeConfig">3. KubeConfig</a></li>
<li><a href="#4-Role" title="4. Role">4. Role</a></li>
<li><a href="#5-Service-Account" title="5. Service Account">5. Service Account</a></li>
<li><a href="#6-Image-Sercurity" title="6. Image Sercurity">6. Image Sercurity</a></li>
<li><a href="#7-Network" title="7. Network">7. Network</a></li>
<li><a href="#8-Network-Policies" title="8. Network Policies">8. Network Policies</a></li>
</ul>
</li>
<li class=""><a href="#Storage" title="Storage">Storage</a><ul class="nav">
<li class=""><a href="#1-Volume" title="1. Volume">1. Volume</a></li>
<li class=""><a href="#11-Volume-Type" title="1.1 Volume Type">1.1 Volume Type</a></li>
<li class=""><a href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)">2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</a></li>
<li><a href="#3-Storage-Class" title="3. Storage Class">3. Storage Class</a></li>
</ul>
</li>
<li class=""><a href="#Troubleshooting" title="Troubleshooting">Troubleshooting</a><ul class="nav">
<li><a href="#常見問題" title="常見問題">常見問題</a></li>
<li class=""><a href="#Task-1" title="Task 1">Task 1</a></li>
</ul>
</li>
<li><a href="#Others" title="Others">Others</a><ul class="nav">
<li><a href="#22-Volume-Test" title="22. Volume-Test">22. Volume-Test</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
