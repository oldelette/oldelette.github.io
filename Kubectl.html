<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Kubectl - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{background:#fff;color:#333;display:block;overflow-x:auto;padding:.5em}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{background-color:#eaffea;color:#55a532}.hljs-deletion{background-color:#ffecec;color:#bd2c00}.hljs-link{text-decoration:underline}.markdown-body{word-wrap:break-word;font-size:16px;line-height:1.5}.markdown-body:after,.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;line-height:1;margin-left:-20px;padding-right:4px}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-bottom:16px;margin-top:0}.markdown-body hr{background-color:#e7e7e7;border:0;height:.25em;margin:24px 0;padding:0}.markdown-body blockquote{border-left:.25em solid #ddd;color:#777;font-size:16px;padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{background-color:#fcfcfc;border:1px solid;border-color:#ccc #ccc #bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb;color:#555;display:inline-block;font-size:11px;line-height:10px;padding:3px 5px;vertical-align:middle}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{font-weight:600;line-height:1.25;margin-bottom:16px;margin-top:24px}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{border-bottom:1px solid #eee;padding-bottom:.3em}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:#777;font-size:.85em}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{list-style-type:none;padding:0}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-bottom:0;margin-top:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{font-size:1em;font-style:italic;font-weight:700;margin-top:16px;padding:0}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{display:block;overflow:auto;width:100%;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{border:1px solid #ddd;padding:6px 13px}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{background-color:#fff;box-sizing:initial;max-width:100%}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{background-color:initial;max-width:none;vertical-align:text-top}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{border:1px solid #ddd;display:block;float:left;margin:13px 0 0;overflow:hidden;padding:7px;width:auto}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{clear:both;color:#333;display:block;padding:5px 0 0}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{background-color:#0000000a;border-radius:3px;font-size:85%;margin:0;padding:.2em 0}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{content:"\00a0";letter-spacing:-.2em}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{background:#0000;border:0;font-size:100%;margin:0;padding:0;white-space:pre;word-break:normal}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{background-color:#f7f7f7;border-radius:3px;font-size:85%;line-height:1.45;overflow:auto;padding:16px}.markdown-body pre code,.markdown-body pre tt{word-wrap:normal;background-color:initial;border:0;display:inline;line-height:inherit;margin:0;max-width:auto;overflow:visible;padding:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{font-size:12px;line-height:1;overflow:hidden;padding:5px;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{background:#fff;border:0;padding:10px 8px 9px;text-align:right}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:#f8f8f8;border-top:0;font-weight:700}.news .alert .markdown-body blockquote{border:0;padding:0 0 0 40px}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{cursor:default!important;float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle}.markdown-body{max-width:758px;overflow:visible!important;padding-bottom:40px;padding-top:40px;position:relative}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{border-right:3px solid #6ce26c!important;box-sizing:initial;color:#afafaf!important;cursor:default;display:inline-block;min-width:20px;padding:0 8px 0 0;position:relative;text-align:right;z-index:4}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-bottom:none;border-left:none;border-top:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-collapse:inherit!important;border-spacing:0}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{margin-bottom:unset;overflow:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p:last-child{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{background-color:inherit;border-radius:0;overflow:visible;text-align:center;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{height:100%;max-width:100%}.markdown-body pre>code.wrap{word-wrap:break-word;white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap}.markdown-body .alert>p:last-child,.markdown-body .alert>ul:last-child{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{background-color:#000;background-position:50%;background-repeat:no-repeat;background-size:contain;cursor:pointer;display:table;overflow:hidden;text-align:center}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{object-fit:contain;width:100%;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{height:100%;left:0;position:absolute;top:0;width:100%}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{color:#fff;height:auto;left:50%;opacity:.3;position:absolute;top:50%;transform:translate(-50%,-50%);transition:opacity .2s;width:auto;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{bottom:0;height:100%;left:0;position:absolute;right:0;top:0;width:100%}.figma{display:table;padding-bottom:56.25%;position:relative;width:100%}.figma iframe{border:1px solid #eee;bottom:0;height:100%;left:0;position:absolute;right:0;top:0;width:100%}.markmap-container{height:300px}.markmap-container>svg{height:100%;width:100%}.MJX_Assistive_MathML{display:none}#MathJax_Message{z-index:1000!important}.ui-infobar{color:#777;margin:25px auto -25px;max-width:760px;position:relative;z-index:2}.toc .invisable-node{list-style-type:none}.ui-toc{bottom:20px;position:fixed;z-index:998}.ui-toc.both-mode{margin-left:8px}.ui-toc.both-mode .ui-toc-label{border-bottom-left-radius:0;border-top-left-radius:0;height:40px;padding:10px 4px}.ui-toc-label{background-color:#e6e6e6;border:none;color:#868686;transition:opacity .2s}.ui-toc .open .ui-toc-label{color:#fff;opacity:1;transition:opacity .2s}.ui-toc-label:focus{background-color:#ccc;color:#000;opacity:.3}.ui-toc-label:hover{background-color:#ccc;opacity:1;transition:opacity .2s}.ui-toc-dropdown{margin-bottom:20px;margin-top:20px;max-height:70vh;max-width:45vw;overflow:auto;padding-left:10px;padding-right:10px;text-align:inherit;width:25vw}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{letter-spacing:.0029em;padding-right:0}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{color:#767676;display:block;font-size:13px;font-weight:500;padding:4px 20px}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{background-color:initial;border-left:1px solid #000;color:#000;padding-left:19px;text-decoration:none}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{border-left:none;border-right:1px solid #000;padding-right:19px}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{background-color:initial;border-left:2px solid #000;color:#000;font-weight:700;padding-left:18px}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{border-left:none;border-right:2px solid #000;padding-right:18px}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{font-size:12px;font-weight:400;padding-bottom:1px;padding-left:30px;padding-top:1px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{font-size:12px;font-weight:400;padding-bottom:1px;padding-left:40px;padding-top:1px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{font-weight:500;padding-left:28px}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{font-weight:500;padding-left:38px}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{max-height:70vh;max-width:15vw;overflow:auto;position:fixed;top:0}.back-to-top,.expand-toggle,.go-to-bottom{color:#999;display:block;font-size:12px;font-weight:500;margin-left:10px;margin-top:10px;padding:4px 10px}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{background-position:50%;background-repeat:no-repeat;background-size:cover;border-radius:50%;display:block;height:20px;margin-bottom:2px;margin-right:5px;margin-top:2px;width:20px}.ui-user-icon.small{display:inline-block;height:18px;margin:0 0 .2em;vertical-align:middle;width:18px}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-more-info{color:#888;cursor:pointer;vertical-align:middle}.ui-more-info .fa{font-size:16px}.ui-connectedGithub,.ui-published-note{color:#888}.ui-connectedGithub{line-height:23px;white-space:nowrap}.ui-connectedGithub a.file-path{color:#888;padding-left:22px;text-decoration:none}.ui-connectedGithub a.file-path:active,.ui-connectedGithub a.file-path:hover{color:#888;text-decoration:underline}.ui-connectedGithub .fa{font-size:20px}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}.selectable{-webkit-user-select:text;-o-user-select:text;user-select:text}.inline-spoiler-section{cursor:pointer}.inline-spoiler-section .spoiler-text{background-color:#333;border-radius:2px}.inline-spoiler-section .spoiler-text>*{opacity:0}.inline-spoiler-section .spoiler-img{filter:blur(10px)}.inline-spoiler-section.raw{background-color:#333;border-radius:2px}.inline-spoiler-section.raw>*{opacity:0}.inline-spoiler-section.unveil{cursor:auto}.inline-spoiler-section.unveil .spoiler-text{background-color:#3333331a}.inline-spoiler-section.unveil .spoiler-text>*{opacity:1}.inline-spoiler-section.unveil .spoiler-img{filter:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{color:#222;position:relative;z-index:1}.markdown-body.slides:before{background-color:currentColor;bottom:0;box-shadow:0 0 0 50vw;content:"";display:block;left:0;position:absolute;right:0;top:0;z-index:-1}.markdown-body.slides section[data-markdown]{background-color:#fff;margin-bottom:1.5em;position:relative;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{left:1em;max-height:100%;overflow:hidden;position:absolute;right:1em;top:50%;transform:translateY(-50%)}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{border:3px solid #777;content:"";height:1.5em;position:absolute;right:1em;top:-1.5em}.site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif}html[lang=zh-tw] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}svg{text-shadow:none}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true"><h6 id="tags-Work" data-id="tags-Work"><a class="anchor hidden-xs" href="#tags-Work" title="tags-Work"><span class="octicon octicon-link"></span></a><span>tags: </span><code>Work</code></h6><h1 id="Kubectl" data-id="Kubectl"><a class="anchor hidden-xs" href="#Kubectl" title="Kubectl"><span class="octicon octicon-link"></span></a><span>Kubectl</span></h1><p><a href="https://ithelp.ithome.com.tw/articles/10234562" target="_blank" rel="noopener"><span>從題目中學習k8s</span></a></p><h2 id="Tips" data-id="Tips"><a class="anchor hidden-xs" href="#Tips" title="Tips"><span class="octicon octicon-link"></span></a><span>Tips</span></h2><p><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14937836#overview" target="_blank" rel="noopener"><span>Certification Tip!</span></a><span>  - 需登入 udemy</span><br>
<a href="https://kubernetes.io/docs/reference/kubectl/conventions/" target="_blank" rel="noopener"><span>kubectl Usage Conventions</span></a></p><h3 id="查看-K8s-所有物件和它們的縮寫" data-id="查看-K8s-所有物件和它們的縮寫"><a class="anchor hidden-xs" href="#查看-K8s-所有物件和它們的縮寫" title="查看-K8s-所有物件和它們的縮寫"><span class="octicon octicon-link"></span></a><span>查看 K8s 所有物件和它們的縮寫</span></h3><div class="alert alert-info">
<p><span>kubectl api-resources</span></p>
</div><h3 id="Pod-amp-yaml" data-id="Pod-amp-yaml"><a class="anchor hidden-xs" href="#Pod-amp-yaml" title="Pod-amp-yaml"><span class="octicon octicon-link"></span></a><span>Pod &amp; yaml</span></h3><ul>
<li><span>創建出一個基本YAML，再去修改</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl run nginx-kusc00101 --image=nginx --restart=Never --dry-run=client -o yaml&gt; xxx.pod</span></p>
</div><p><span>–dry-run=client 參數用來預覽 而不會真正提交到 cluster 集群中</span></p><p><ins><span>–dry-run</span></ins><span>: By default as soon as the command is run, the resource will be created.</span><br>
<span>If you simply want to test your command , use the </span><ins><span>–dry-run=client</span></ins><span> option.</span><br>
<span>This will not create the resource, instead, tell you whether the resource can be created and if your command is right.</span></p><p><ins><span>-o yaml</span></ins><span>: This will output the resource definition in YAML format on screen.</span></p><h3 id="Deployment" data-id="Deployment"><a class="anchor hidden-xs" href="#Deployment" title="Deployment"><span class="octicon octicon-link"></span></a><span>Deployment</span></h3><ul>
<li><span>Create a deployment</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl create deployment --image=nginx nginx</span></p>
</div><ul>
<li><span>Generate Deployment YAML file (-o yaml). Don’t create it (–dry-run)</span><br>
<span>Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run) with 4 Replicas (–replicas=4)</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml</span><br>
<span>kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml &gt; nginx-deployment.yaml</span></p>
</div><h3 id="Service" data-id="Service"><a class="anchor hidden-xs" href="#Service" title="Service"><span class="octicon octicon-link"></span></a><span>Service</span></h3><ul>
<li><span>將Pod expose出去 (創建一個Service)</span></li>
</ul><div class="alert alert-info">
<p><span>kubectl expose po &lt;pod-name&gt; --type=NodePort --name=&lt;svc-name&gt; --port=80</span><br>
<span>kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml</span></p>
</div><h2 id="Core-Concepts" data-id="Core-Concepts"><a class="anchor hidden-xs" href="#Core-Concepts" title="Core-Concepts"><span class="octicon octicon-link"></span></a><span>Core Concepts</span></h2><h3 id="1-Label" data-id="1-Label"><a class="anchor hidden-xs" href="#1-Label" title="1-Label"><span class="octicon octicon-link"></span></a><span>1. Label</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10236866" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/9pnxx4N.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>## 先透過kubectl run命令建立Pod基本YAML
kubectl run nginx-kusc00101 --image=nginx --restart=Never --dry-run=client -o yaml&gt; q1.pod
## 將YAML 修改 (新增下方圖片紅字內容)
## 創立物件
kubectl apply -f q1.pod
</code></pre><p><span>nodeSelector 是 Pod 到某 Node 上最簡單、直接的調度方式，直接利用 label選取即可</span></p><p><img src="https://i.imgur.com/sI3n5R6.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/rwzGjMY.png" alt="" loading="lazy"></p><p><span>是個Pod Scheduling 的概念題，也就是如何將 Pod運行在適合的 Node上</span></p><div class="alert alert-success">
<p><span>label 是一個 [key:value] 的結構，以 key 對應的 value 來區隔它們屬性的異同</span><br>
<span>label 可以被附加到各種 resource-objects 上，例如 Node、Pod 或 Service等…</span><br>
<span>一個 label 可以被增加到任意數量的物件上；一個物件也可以添加任意數量的 label</span></p>
<p><span>常用的 label 包含 release(版本)、environment(環境)、tier(架構)、partition(分區)、track(品質管控)等。</span></p>
</div><p><span>相關label指令</span></p><div class="alert alert-warning">
<p><span>查看Pod label</span><br>
<span>kubectl get po --show-labels</span><br>
<span>查看特定label的Pod</span><br>
<span>kubectl get po --selector &lt;key&gt;=&lt;value&gt;</span><br>
<span>kubectl get po --selector &lt;key1&gt;=&lt;value1&gt;,&lt;key2&gt;=&lt;value2&gt;</span><br>
<span>新增Pod label</span><br>
<span>kubectl label po &lt;pod-name&gt; &lt;key&gt;=&lt;value&gt;</span><br>
<span>刪除Pod label</span><br>
<span>kubectl label po &lt;pod-name&gt; &lt;key&gt;-</span></p>
</div><p><span>想要删除一個 Label，只需在命令最後指定 Label 的 key 名稱與一個減號相連即可：</span></p><p><img src="https://i.imgur.com/6RSRY2i.png" alt="" loading="lazy"></p><h3 id="2-Deployment" data-id="2-Deployment"><a class="anchor hidden-xs" href="#2-Deployment" title="2-Deployment"><span class="octicon octicon-link"></span></a><span>2. Deployment</span></h3><h4 id="Deployment部署" data-id="Deployment部署"><a class="anchor hidden-xs" href="#Deployment部署" title="Deployment部署"><span class="octicon octicon-link"></span></a><span>Deployment部署</span></h4><p><span>Deployment 可以達成以下幾件事情：</span></p><ul>
<li><span>部署一個應用服務 (application)</span></li>
<li><span>協助 applications 升級到某個特定版本</span></li>
<li><span>服務升級過程中做到無停機服務遷移 (zero downtime deployment)</span></li>
<li><span>可以 Rollback 到先前版本</span></li>
</ul><table>
<thead>
<tr>
<th><span>Deployment相關指令</span></th>
<th><span>指令功能</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>kubectl get deployments</span></td>
<td><span>取得目前Kubernetes中的deployments的資訊</span></td>
</tr>
<tr>
<td><span>kubectl get rs</span></td>
<td><span>取得目前Kubernetes中的 Replication Set 的資訊</span></td>
</tr>
<tr>
<td><span>kubectl describe deploy &lt;deployment-name&gt;</span></td>
<td><span>取得特定deployment的詳細資料</span></td>
</tr>
<tr>
<td><span>kubectl set image deploy/ &lt;deployment-name&gt; &lt;pod-name&gt;: &lt;image-path&gt;:&lt;version&gt;</span></td>
<td><span>將 deployment 管理的 pod 升級到特定image版本</span></td>
</tr>
<tr>
<td><span>kubectl edit deploy &lt;deployment-name&gt;</span></td>
<td><span>編輯特定 deployment 物件</span></td>
</tr>
<tr>
<td><span>kubectl rollout status deploy &lt;deployment-name&gt;</span></td>
<td><span>查詢目前某deployment升級狀況</span></td>
</tr>
<tr>
<td><span>kubectl rollout history deploy &lt;deployment-name&gt;</span></td>
<td><span>查詢目前某deployment升級的歷史紀錄</span></td>
</tr>
<tr>
<td><span>kubectl rollout undo deploy &lt;deployment-name&gt;</span></td>
<td><span>回滾 Pod到先前一個版本</span></td>
</tr>
<tr>
<td><span>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=n</span></td>
<td><span>回滾 Pod到某個特定版本</span></td>
</tr>
</tbody>
</table><hr><pre><code>k describe deployments &lt;deployments-name&gt;
</code></pre><p><span>kubectl describe deployment 兩項重要資訊：</span></p><ul>
<li><span>OldReplicaSets</span></li>
<li><span>NewReplicaSet</span></li>
</ul><p><span>這兩項代表著 deployments 所管理得 replicaset 物件。</span><br>
<span>如果 rollout 進行中，則兩個欄位都有資訊。</span><br>
<span>如果 rollout 完成了，則 OldReplicaSets 會顯示 資訊。</span></p><p><img src="https://i.imgur.com/zQNYl35.png" alt="" loading="lazy"></p><h4 id="Example---Deployment" data-id="Example---Deployment"><a class="anchor hidden-xs" href="#Example---Deployment" title="Example---Deployment"><span class="octicon octicon-link"></span></a><span>Example - Deployment</span></h4><p><img src="https://i.imgur.com/a9IMZxk.png" alt="" loading="lazy"></p><p><span>這題的重點是 pod scheduling</span></p><ul>
<li><ins><span>PodAntiAffinity</span></ins><br>
<span>The idea here is that we create a “Inter-pod anti-affinity” which allows us to say a Pod should only be scheduled on a node where another Pod of a specific label (here the same label) is not already running</span></li>
</ul><pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-important
  namespace: project-tiger
  labels:
    id: very-important
spec:
  replicas: 3
  selector:
    matchLabels:
      id: very-important
  template:
    metadata:
      labels:
        id: very-important
    spec:
      containers:
      - name: container1
        image: nginx:1.17.6-alpine
      containers:
      - name: container2
        image: kubernetes/pause
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: id
                operator: In
                values:
                - very-important
            topologyKey: "kubernetes.io/hostname"
</code></pre><h3 id="21-ReplicaSet-複製集" data-id="21-ReplicaSet-複製集"><a class="anchor hidden-xs" href="#21-ReplicaSet-複製集" title="21-ReplicaSet-複製集"><span class="octicon octicon-link"></span></a><span>2.1 ReplicaSet 複製集</span></h3><p><img src="https://i.imgur.com/sb9Q0FU.png" alt="" loading="lazy"><br>
<span>圖中可以看到一個 Deployment 掌管一或多個 ReplicaSet，而一個 ReplicaSet 掌管一或多個 Pod</span></p><div class="alert alert-info">
<p><span>Kubernetes 官方强烈建議避免直接使用 ReplicaSet，應該通過 Deployment 來建立 RS 和Pod</span></p>
</div><p><span>ReplicaSet 是用來確保在 k8s 中，在資源允許的前提下，指定的 pod 的數量會跟使用者所期望的一致，也就是所謂的 desired status</span></p><ul>
<li><ins><span>常用的 replicaset command</span></ins></li>
</ul><pre><code>kubectl create -f replicaset-definition.yml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset (Also deltes all underlying PODs)
kubectl replace -f replocaset-definition.yml
kubectl scale -replicas=6 -f replocaset-definition.yml (擴展 replicas)
kubectl edit rs new-replica-set (編輯 rs)
</code></pre><p><span>[Ref- ReplicaSet 介紹]</span><br>
<img src="https://i.imgur.com/DIm2ybF.png" alt="" loading="lazy"></p><pre><code># my-replica-sets.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replica-set
spec:
  replicas: 3
  selector:
    matchLabels:
      env: dev
    matchExpressions:
      - {key: env, operator: In, values: [dev]}
      - {key: env, operator: NotIn, values: [prod]}
  template:
    metadata:
      labels:
        app: hello-pod-v1
        env: dev
        version: v1
    spec:
      containers:
      - name: my-pod
        image: zxcvbnius/docker-demo
        ports:
        - containerPort: 3000
</code></pre><ul>
<li>
<p><ins><span>spec.selector.matchLabels</span></ins><br>
<span>在 Replica Set的selector 裡面提供了 matchLabels，matchLabels的用法代表著等於(equivalent)，代表Pod的labels必須與matchLabels中指定的值相同，才算符合條件。</span></p>
</li>
<li>
<p><ins><span>spec.selector.matchExpressions</span></ins><br>
<span>matchExpressions 的用法較為彈性，每一筆條件主要由三個部分組成key, operator，value.</span><br>
<span>以 my-replica-sets.yaml 中敘述為例，我們指定Pod的條件為 1) env必須為dev 2) env不能為prod。</span><br>
<span>目前operator支援4種條件: In, NotIn, Exists, DoesNotExis</span></p>
</li>
</ul><h3 id="22-Replication-Controller" data-id="22-Replication-Controller"><a class="anchor hidden-xs" href="#22-Replication-Controller" title="22-Replication-Controller"><span class="octicon octicon-link"></span></a><span>2.2 Replication Controller</span></h3><div class="alert alert-success">
<p><span>Replication Controller 就是 Kubernetes 上用來管理 Pod 的數量以及狀態的 controller</span></p>
<ul>
<li><span>每個 Replication Controller 都有屬於自己的 yaml 檔</span></li>
<li><span>在 Replication Controller 設定檔中可以指定同時有多少個相同的 Pods 運行在 Kubernetes Cluster上</span></li>
<li><span>當某一 Pod 發生 crash, failed，而終止運行時，Replication Controller會幫我們自動偵測，並且自動創建一個新的Pod，確保 Pod運行的數量與設定檔的指定的數量相同</span></li>
</ul>
</div><p><span>透過 Replica 設定我們就可以快速產生一樣內容的 Pod</span><br>
<span>舉例來說：今天設定了 replica: 2 就代表會產生兩個內容一樣的 Pod 出來。</span><br>
<img src="https://i.imgur.com/k5LOx1S.png" alt="" loading="lazy"></p><pre><code>kubectl create -f q3.yaml
kubectl get replicationcontroller
</code></pre><p><img src="https://i.imgur.com/GuGoOFi.png" alt="" loading="lazy"></p><pre><code># q3.yaml
apiVersion: v1                                                      
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: test
      name: test
    spec:
      containers:
      - image: nginx:1.11.0-alpine
        name: test
  replicas: 3
</code></pre><ul>
<li><ins><span>spec.replicas &amp; spec.selector</span></ins><br>
<span>在spec.replicas中，我們必須定義 Pod的數量，以及在spec.selector中指定我們要選擇的Pod的條件 (labels)</span></li>
<li><ins><span>spec.template</span></ins><br>
<span>在spec.template 中我們會去定義pod的資訊，包含Pod的labels以及Pod中要運行的container。</span></li>
</ul><h3 id="3-Service" data-id="3-Service"><a class="anchor hidden-xs" href="#3-Service" title="3-Service"><span class="octicon octicon-link"></span></a><span>3. Service</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10238233" target="_blank" rel="noopener"><span>Ref 從題目中學習k8s</span></a><br>
<img src="https://i.imgur.com/rxhUfNu.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>## 透過kubectl expose命令創建Service
sudo kubectl expose pod kubernetes-demo-pod --type=NodePort --port=80
</code></pre><p><a href="https://hackmd.io/@tienyulin/kubernetes-service-deployment-ingress" target="_blank" rel="noopener"><span>Ref - Pod 進階應用 : Service、Deployment、Ingress</span></a></p><div class="alert alert-success">
<p><span>Service 是一個抽象化的物件，它定義了一群的 Pod 和存取這些 Pod 的規則</span><br>
<span>簡單來說就是要讓使用者和 Kubernetes Cluster 進行隔離，使用者只需要告訴 Service 我的請求而不需要知道誰會幫我處理也不需要知道處理的過程和細節，Service 收到請求後就會依照定義的規則將請求送到對應的 Pod</span><br>
<span>Pod 和 Pod 之間的也會透過 Service 來存取。</span></p>
</div><p><img src="https://i.imgur.com/XGdNEJC.png" alt="" loading="lazy"></p><h4 id="kubectl-expose" data-id="kubectl-expose"><a class="anchor hidden-xs" href="#kubectl-expose" title="kubectl-expose"><span class="octicon octicon-link"></span></a><span>kubectl expose</span></h4><p><span>該指令可以幫我們創建一個新的 Kubernetes Service 物件，來讓Kubernetes Cluster中運行的 Pod與外部互相溝通</span></p><pre><code>kubectl expose pod &lt;pod name&gt; --type=&lt;service type&gt; --name=&lt;service name&gt; \
--port=&lt;port&gt; --target-port=&lt;target port&gt;
</code></pre><h4 id="Service-Nodeport" data-id="Service-Nodeport"><a class="anchor hidden-xs" href="#Service-Nodeport" title="Service-Nodeport"><span class="octicon octicon-link"></span></a><span>Service Nodeport</span></h4><p><span>Service 的類型:</span></p><div class="alert alert-success">
<ul>
<li><span>ClusterIP : 預設類型,使用 Cluster 內部的 IP 來暴露 Service，因此只能在 Cluster 內部存取。</span></li>
<li><span>NodePort : 使用 Node 的 IP 和 Port 來暴露 Service，在 Cluster 外可以透過 NodeIP: NodePort 來存取 Service。</span><br>
<span>而 Cluster 內部所需的 Cluster IP 會自動被建立。</span></li>
<li><span>LoadBalancer : 使用雲端供應商提供的 LoadBalancer 來開放 Service，會自動建立相對應的 NodePort 和 Cluster IP。</span></li>
<li><span>ExternalName : 將 Service 關聯到 ExternalName，也就是 Domain。</span><br>
<span>例如 : </span><a href="http://foo.sample.com" target="_blank" rel="noopener"><span>foo.sample.com</span></a><span>。</span></li>
</ul>
</div><p><img src="https://i.imgur.com/B5u9XNy.png" alt="" loading="lazy"></p><ul>
<li><ins><span>apiVersion</span></ins><br>
<span>Service使用的Kubernetes API 是 v1版本號</span></li>
<li><ins><span>spec.type</span></ins><br>
<span>可以指定 Service的型別，可以是 NodePort 或是 LoadBalancer 等等…</span></li>
<li><ins><span>spec.ports.port</span></ins><br>
<span>創建 Service 的 Cluster IP，是哪個port number去對應到targetPort</span></li>
<li><ins><span>spec.ports.nodePort</span></ins><br>
<span>可以指定 Node 物件是哪一個 port numbrt，去對應到targetPort，若是在Service的設定檔中沒有指定的話，Kubernetes會隨機幫我們選一個port number</span></li>
<li><ins><span>spec.ports.targetPort</span></ins><br>
<span>targetPort 是我們指定的 Pod 的 port number，由於我們會在Pod中運行一個 port number 3000 的 web container，所以我們指定hello-service的特定 port number都可以導到該web container</span></li>
<li><ins><span>spec.ports.protocol</span></ins><br>
<span>目前 Service 支援TCP與UDP兩種protocl，預設為TCP</span></li>
<li><ins><span>spec.selector</span></ins><br>
<span>selector 會幫忙選擇 pod (use label)</span></li>
</ul><p><a href="https://ithelp.ithome.com.tw/articles/10194344" target="_blank" rel="noopener"><span>&lt;NodePort example&gt;</span></a></p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: hello-service
spec:
  type: NodePort
  ports:
  - port: 3000
    nodePort: 30390
    protocol: TCP
    targetPort: 3000
  selector:
    app: my-deployment
</code></pre><p><img src="https://i.imgur.com/e54szMJ.png" alt="" loading="lazy"></p><p><span>相關 service 指令</span></p><div class="alert alert-warning">
<p><span>kubectl expose pod kubernetes-pod --type=NodePort</span><br>
<span>## 查看建立的 Service</span><br>
<span>kubectl get services</span><br>
<span>## 查看 Service 的詳細資訊</span><br>
<span>kubectl describe services</span></p>
</div><h3 id="4-Namespace" data-id="4-Namespace"><a class="anchor hidden-xs" href="#4-Namespace" title="4-Namespace"><span class="octicon octicon-link"></span></a><span>4. Namespace</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10238974" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/JQeAd3s.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>kubectl create ns website-frontend
kubectl run jenkins --image=jenkins --restart=Never --dry-run=client -o yaml &gt; q4.yaml
kubectl apply -f q4.yaml -n website-frontend
</code></pre><div class="alert alert-success">
<p><span>藉由 namespace 可以作到 project 的 resources 的隔離，使不同的 project 可以有相同名字的 deployment, services.</span><br>
<span>可以當作在 cluster 又在加上一層 virtual cluster 的分割，這個好處是不需要多開實體的 cluster 裡面又包含了 master node 的相關服務</span></p>
<ul>
<li><span>同一個 namespace 的資源名稱是唯一性</span></li>
<li><span>不同 namespaces 的資源名稱可以相同</span></li>
</ul>
</div><p><img src="https://i.imgur.com/6nxaHQV.png" alt="" loading="lazy"></p><p><span>K8s 預設存在的 namespace 有三個:</span></p><ul>
<li><ins><span>default</span></ins><br>
<span>K8s集群建立時自動創建，一般對K8s物件的操作都在default namespace中完成</span></li>
<li><ins><span>kube-system</span></ins><br>
<span>K8s在集群建立時會為了某些目的創建一系列的Pod和Service，例如weave net CNI、DNS Service等，通常是一些維持集群運作的重要物件</span><br>
<span>為了預防這些物件不小心被使用者刪除或更改導致集群毀損，必須將這些物件獨立於其它物件，因此會將這些物件放置於kube-system這個namespace之下</span></li>
<li><ins><span>kube-public</span></ins><br>
<span>此namespace也是由K8s自動創建，當有些resources需要讓所有user都能access到時，會利用此namespace</span></li>
</ul><p><span>相關namespace指令</span></p><div class="alert alert-warning">
<p><span>## 查看&lt;namespace-name&gt;下的pod</span><br>
<span>kubectl get po --namespace=&lt;namespace-name&gt;</span><br>
<span>kubectl get po -n &lt;namespace-name&gt;</span></p>
<p><span>## 創建namespace</span><br>
<span>kubectl create ns &lt;namespace-name&gt;</span></p>
<p><span>## 查看所有namespace下的pod</span><br>
<span>kubectl get po -A (–all-namespaces)</span></p>
<p><span>## 刪除單一 Namespaces</span><br>
<span>kubectl delete namespaces &lt;namespace-name&gt;</span></p>
<p><span>##切換 Namespaces</span><br>
<span>kubectl config set-context $(kubectl config current-context) --namespace=dev</span><br>
<span>kubectl config view | grep namespace</span></p>
</div><ul>
<li><span>切換 namespace example</span><br>
<img src="https://i.imgur.com/eKAdWlx.png" alt="" loading="lazy"></li>
</ul><pre><code># my-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
</code></pre><ul>
<li><span>刪除 example</span><br>
<img src="https://i.imgur.com/wFGPdv2.png" alt="" loading="lazy"><br>
<img src="https://i.imgur.com/4pB0jdD.png" alt="" loading="lazy"></li>
</ul><h3 id="5-Imperative-vs-Declarative" data-id="5-Imperative-vs-Declarative"><a class="anchor hidden-xs" href="#5-Imperative-vs-Declarative" title="5-Imperative-vs-Declarative"><span class="octicon octicon-link"></span></a><span>5. Imperative vs Declarative</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10240455" target="_blank" rel="noopener"><span>Ref-資源定義與使用</span></a></p><p><span>&lt;資源對象管理方式&gt;</span><br>
<span>在 kubectl 中有三種方式管理資源</span></p><ul>
<li><ins><span>Imperative commands</span></ins><br>
<span>透過指令方式管理資源, 通常於開發或測試使用</span><br>
<span>ex: kubectl edit</span></li>
<li><ins><span>Imperative object configurations</span></ins><br>
<span>ex: 建立, 刪除, 替換</span></li>
</ul><pre><code>kubectl create -f FILE.yaml
kubectl delete -f FILE.yaml
kubectl replace -f FILE.yaml
</code></pre><ul>
<li><ins><span>Declarative object configurations</span></ins><br>
<span>使用檔案描述，底層由 kubernetes 維護，也就是其他未定義欄位。因此會針對先前修改過的欄位比較，並進行刪或更新動作。</span><br>
<strong><span>相較於 create 是去創建，而 apply 則是維護</span></strong></li>
</ul><p><img src="https://i.imgur.com/N2CNscT.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/ffuHIpJ.jpg" alt="" loading="lazy"></p><h2 id="Scheduling" data-id="Scheduling"><a class="anchor hidden-xs" href="#Scheduling" title="Scheduling"><span class="octicon octicon-link"></span></a><span>Scheduling</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section3-Scheduling/Kubernetes%2B-CKA-%2B0200%2B-%2BScheduling.pdf" target="_blank" rel="noopener"><span>Scheduling.pdf</span></a></p><h3 id="0-Scheduling---nodeName-amp-nodeSelectors" data-id="0-Scheduling---nodeName-amp-nodeSelectors"><a class="anchor hidden-xs" href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0-Scheduling---nodeName-amp-nodeSelectors"><span class="octicon octicon-link"></span></a><span>0. Scheduling - nodeName &amp; nodeSelectors</span></h3><p><span>Scheduling就是決定Pod在建立時，會被安排到哪個Node上 (cluster 集群預設是 </span><strong><span>Scheduler</span></strong><span>來負責這件事)</span></p><ul>
<li><ins><span>Manual Scheduling</span></ins><br>
<span>會需要手動做 Scheduling 的情況為集群中沒有Scheduler時</span><br>
<span>如果沒有Sheduler，準備建立的Pod之狀態會停留在Pending，等待系統將其綁定到某個節點上運行</span></li>
</ul><div class="alert alert-success">
<p><span>kubectl get pods --namespace kube-system --&gt; 用這個指令看是否缺少 sheduler</span></p>
</div><h4 id="nodeName" data-id="nodeName"><a class="anchor hidden-xs" href="#nodeName" title="nodeName"><span class="octicon octicon-link"></span></a><span>nodeName</span></h4><h5 id="方法一-Binding" data-id="方法一-Binding"><a class="anchor hidden-xs" href="#方法一-Binding" title="方法一-Binding"><span class="octicon octicon-link"></span></a><span>方法一: Binding</span></h5><p><span>使用Binding來幫它綁定到某個Node上</span></p><pre><code>apiVersion: v1
kind: Binding
metadata:
  name: nginx # 會自動找這個名字的物件
target:
  apiVersion: v1
  kind: Node
  name: node01
</code></pre><p><span>這邊 </span><a href="http://metadata.name" target="_blank" rel="noopener"><span>metadata.name</span></a><span> 所設定的物件名稱，系統會自動根據這個名稱去尋找，找到這個名稱的物件並綁定到target所指定的Node</span></p><h5 id="方法二-Edit-pod-yaml" data-id="方法二-Edit-pod-yaml"><a class="anchor hidden-xs" href="#方法二-Edit-pod-yaml" title="方法二-Edit-pod-yaml"><span class="octicon octicon-link"></span></a><span>方法二: Edit pod yaml</span></h5><p><span>在原本的 YAML中，使用 </span><strong><span>nodeName</span></strong><span> 來指定Pod到某個Node.</span><br>
<span>(要注意的是，Pod其實是無法直接更新的，一定要透過刪除再重建來更新)</span></p><p><img src="https://i.imgur.com/wecbjPH.png" alt="" loading="lazy"></p><h4 id="nodeSelectors" data-id="nodeSelectors"><a class="anchor hidden-xs" href="#nodeSelectors" title="nodeSelectors"><span class="octicon octicon-link"></span></a><span>nodeSelectors</span></h4><p><span>透過設置 nodeSelector 來達成 Node Affinity 的需求</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    run: test
  name: test
spec:
  containers:
  - image: nginx
    name: test
  nodeSelector:
    size: Large 
</code></pre><h3 id="1-Taints-amp-Tolerations" data-id="1-Taints-amp-Tolerations"><a class="anchor hidden-xs" href="#1-Taints-amp-Tolerations" title="1-Taints-amp-Tolerations"><span class="octicon octicon-link"></span></a><span>1. Taints &amp; Tolerations</span></h3><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section3-Scheduling/Udemy%2BKubernetes%2Btaints-tolerations.pdf" target="_blank" rel="noopener"><span>taints-tolerations.pdf</span></a></p><p><span>taint 跟 node affinity 雖然都是屬於 scheduling 的一部份，但要達成的目的其實完全相反：</span></p><div class="alert alert-info">
<p><span>node affinity (親和性調度)：設計如何讓 pod 被分派到某個 worker node</span><br>
<span>taint (汙點)：設計讓 pod 如何不要被分派到某個 worker node</span></p>
</div><ul>
<li><ins><span>Taints (污點)</span></ins><span>: 是讓 Node 能排斥特定類型的 Pod</span></li>
<li><ins><span>Tolerations (容忍度)</span></ins><span>: 是應用在Pod上的屬性，允許(但不硬性要求)Pod 調度到帶有與之匹配的Taints 的 Node 上.</span></li>
</ul><hr><p><img src="https://i.imgur.com/Hia2ShD.png" alt="" loading="lazy"></p><p><span>pod D 有標示 Tolerations: app=blue 僅代表可以接受的 node (非強制)</span><br>
<span>以這張圖為例子, pod D 不一定只能在有標示 Taints: app=blue 上的 node1 執行, 他也可在無任何限制下的 node3 上執行</span></p><pre><code>kubectl describe nodes &lt;node-name&gt; | grep -i taints
</code></pre><p><img src="https://i.imgur.com/Nk2x2Tg.png" alt="" loading="lazy"></p><p><ins><span>Master node</span></ins><span>: 基於安全性考量情況下，k8s 中 master node是不會部署pod在上面</span><br>
<span>(可以看到有一個 Taint setting 是說 Noschedule on </span><a href="http://node-role.kubernetes.io/master" target="_blank" rel="noopener"><span>node-role.kubernetes.io/master</span></a><span>)</span></p><hr><p><span>每個 taint 都有以下 3 個屬性：</span></p><ul>
<li><span>Key</span></li>
<li><span>Value</span></li>
<li><span>Effect: NoSchedule &amp; PreferNoSchedule &amp; NoExecute</span></li>
</ul><div class="alert alert-success">
<p><span>effect 欄位是指：當 Pod 因為此 Taint 而無法調度到該節點上的時候，該怎麼處理</span></p>
</div><pre><code>#增加 taint
kubectl taint nodes node1 key=value:tain-effect
#移除 taint (在 taint 的 Key:Effect 後面加上 -)
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
</code></pre><p><img src="https://i.imgur.com/hDK3XH8.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/j1bXG49.png" alt="" loading="lazy"></p><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: test
spec:
  containers:
  - image: nginx
    name: test
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
</code></pre><ul>
<li><ins><span>operator 是操作符，一般會有這幾種</span></ins></li>
</ul><div class="alert alert-warning">
<p><span>Exists：某個Label 存在  (無須指定value)</span><br>
<span>In：Label 的值在某個列表中</span><br>
<span>NotIn：Label 的值不在某個列表中</span><br>
<span>Gt：Label 的值大於某個值</span><br>
<span>Lt：Label 的值小於某個值</span><br>
<span>DoesNotExist：某個 Label 不存在</span></p>
</div><h4 id="TaintsTolerations-and-Node-Affinity" data-id="TaintsTolerations-and-Node-Affinity"><a class="anchor hidden-xs" href="#TaintsTolerations-and-Node-Affinity" title="TaintsTolerations-and-Node-Affinity"><span class="octicon octicon-link"></span></a><span>Taints/Tolerations and Node Affinity</span></h4><p><span>有時候想要的效果需要組合 Taints/Tolerations + Node Affinity 才能辦到</span></p><hr><p><img src="https://i.imgur.com/WtcLJjT.png" alt="" loading="lazy"></p><hr><h3 id="2-Affinity-amp-Anti-Affinity" data-id="2-Affinity-amp-Anti-Affinity"><a class="anchor hidden-xs" href="#2-Affinity-amp-Anti-Affinity" title="2-Affinity-amp-Anti-Affinity"><span class="octicon octicon-link"></span></a><span>2. Affinity &amp; Anti-Affinity</span></h3><p><span>Affinity/Anti-Affinity 指的是親和性調度與反親和性調度，也就是滿足特定條件後，(不)將Pod調度到特定群集上</span><br>
<span>Node Affinity 和 Node Selector不同的是，它會有更多細緻的操作，你可以把Node Selector看成是簡易版的 Node Affinity</span></p><div class="alert alert-info">
<p><span>nodeAffinity有兩種策略</span></p>
<ul>
<li><span>1.preferredDuringSchedulingIgnoredDuringExecution: 軟策略（可以不滿足)</span></li>
<li><span>2.requiredDuringSchedulingIgnoredDuringExecution: 硬策略（一定要滿足）</span></li>
<li><span>requiredDuringSchedulingRequiredDuringExecution (planned)</span></li>
</ul>
</div><p><span>上面策略可以分成4段來看</span></p><table>
<thead>
<tr>
<th></th>
<th><span>DuringScheduling</span></th>
<th><span>DuringExecution</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>Type1</span></td>
<td><span>Required</span></td>
<td><span>Ignored</span></td>
</tr>
<tr>
<td><span>Type2</span></td>
<td><span>Preferred</span></td>
<td><span>Ignored</span></td>
</tr>
<tr>
<td><span>Type3</span></td>
<td><span>Required</span></td>
<td><span>Required</span></td>
</tr>
</tbody>
</table><ul>
<li><span>左右兩邊內容相同</span><br>
<img src="https://i.imgur.com/wuf4Bn4.png" alt="" loading="lazy"></li>
</ul><hr><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/" target="_blank" rel="noopener"><span>Schedule a Pod using required node affinity</span></a><br>
<span>做題目的時候，先利用 </span><a href="https://kubernetes.io/docs/home/" target="_blank" rel="noopener"><span>k8s.io document</span></a><span> 去搜尋 example 來改</span></p><pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
</code></pre><p><span>requiredDuringSchedulingIgnoredDuringExecution 代表了硬需求</span></p><div class="alert alert-warning">
<p><span>注意到這裡寫的 nodeSelectorTerms，在這個區塊底下，如果沒有寫 matchExpressions 的話，那麼在它底下所列出的條件，只要滿足其中一項就可以了，但在這個例子有寫 matchExpressions，所以在 matchExpressions 底下所列的條件全都要滿足才會調度</span></p>
</div><h4 id="Affinity-比較表" data-id="Affinity-比較表"><a class="anchor hidden-xs" href="#Affinity-比較表" title="Affinity-比較表"><span class="octicon octicon-link"></span></a><span>Affinity 比較表</span></h4><p><img src="https://i.imgur.com/sRbNa3n.png" alt="" loading="lazy"></p><h3 id="21-Pod-Affinity" data-id="21-Pod-Affinity"><a class="anchor hidden-xs" href="#21-Pod-Affinity" title="21-Pod-Affinity"><span class="octicon octicon-link"></span></a><span>2.1 Pod Affinity</span></h3><p><span>和nodeAffinity類似，podAffinity也有requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution兩種調度策略</span><br>
<span>唯一不同的是如果要使用互斥性，我們需要使用 </span><strong><span>podAntiAffinity字段</span></strong></p><ul>
<li><ins><span>範例說明:</span></ins><br>
<span>這個設定是要在同個名稱的 Node 底下，要部署 Label 中 app=helloworld 的Pod，而不要部署Label 中 app=node-affinity-pod 的 Pod</span></li>
</ul><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: helloworld
spec:
  containers:
  - name: k8s-demo
    image: 105552010/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - helloworld
        topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - node-affinity-pod
          topologyKey: kubernetes.io/hostname
</code></pre><hr><h3 id="3-Kubernetes-Resources---RequestLimit" data-id="3-Kubernetes-Resources---RequestLimit"><a class="anchor hidden-xs" href="#3-Kubernetes-Resources---RequestLimit" title="3-Kubernetes-Resources---RequestLimit"><span class="octicon octicon-link"></span></a><span>3. Kubernetes Resources - Request/Limit</span></h3><p><span>Kubernetes 需要考慮如何在優先度和公平性的前提下提供資源的利用率，為了實現資源被有效調度和分配時同時提高資源利用率</span><br>
<span>Kubernetes 提供了 Request/Limit 兩種限制類型讓我們對資源進行分配。</span></p><div class="alert alert-success">
<ul>
<li><ins><span>request</span></ins><br>
<span>容器使用的最小資源要求，做為容器調度時資源分配的判斷依賴</span><br>
<span>只有當前節點上可分配的資源量 &gt;= request 時才允許將容器調度到該節點上。</span></li>
<li><ins><span>limit</span></ins><br>
<span>容器能使用的最大值。</span><br>
<span>設置為 0 表示對使用的資源不做限制，可以無限使用</span></li>
</ul>
</div><p><span>下面的 Pod 含有兩個容器.</span><br>
<span>1.每個容器都有 resource requests, 0.25 cpu 以及 64MiB 記憶體</span><br>
<span>2.每個容器都有 resource limits 0.5 cpu 以及 128 MiB 記憶體</span><br>
<span>總體來說, 這個 Pod resource limits 1 cpu 以及 256 MiB 記憶體</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
</code></pre><h4 id="Exceed-limit" data-id="Exceed-limit"><a class="anchor hidden-xs" href="#Exceed-limit" title="Exceed-limit"><span class="octicon octicon-link"></span></a><span>Exceed limit</span></h4><ul>
<li><span>throttle (節流)</span></li>
<li><span>terminate (終止)</span></li>
</ul><p><img src="https://i.imgur.com/LF00LLU.png" alt="" loading="lazy"></p><h4 id="Increase-pod-resource" data-id="Increase-pod-resource"><a class="anchor hidden-xs" href="#Increase-pod-resource" title="Increase-pod-resource"><span class="octicon octicon-link"></span></a><span>Increase pod resource</span></h4><div class="alert alert-info">
<p><span>The status OOMKilled indicates that it is failing because the pod ran out of memory.</span></p>
</div><p><span>$ k describe pods elephant</span><br>
<span>下面這張圖清楚表示 pod 會死掉的原因, 因為 elephant pod 需要 15Mi 的 memory, 但是 limit 是 10Mi, 所以導致 OOMKilled.</span><br>
<img src="https://i.imgur.com/7H3gNUW.png" alt="" loading="lazy"></p><p><span>要調整 Memory Limit 到 20Mi</span></p><pre><code>(o) kubectl replace --force -f /tmp/kubectl-edit-996911536.yaml
(x) kubectl edit pod elephant --&gt; 不能直接調整 running pod resource limit
</code></pre><p><img src="https://i.imgur.com/95LbxBq.png" alt="" loading="lazy"></p><h3 id="4-Demonsets" data-id="4-Demonsets"><a class="anchor hidden-xs" href="#4-Demonsets" title="4-Demonsets"><span class="octicon octicon-link"></span></a><span>4. Demonsets</span></h3><p><span>DamonSet 會確保在所有(或是特定)節點上，一定運行著指定的一個Pod，並且每當有新的Node加入Cluster時，DaemonSet會為他們新增這指定的一個Pod，同時只要有Node被移除Cluster外，在這Node上的指定Pod也會被移除</span></p><p><img src="https://i.imgur.com/CXSKKpi.png" alt="" loading="lazy"></p><p><span>應用場景:</span></p><ul>
<li><span>Monitoring (Prometheu, Datadog)</span></li>
<li><span>Log Viewer (fluent-bi, fluentd, logstash)</span></li>
<li><span>Storage (glusterd, ceph)</span></li>
</ul><pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
</code></pre><ul>
<li><ins><span>k describe daemonsets --namespace=kube-system</span></ins><br>
<span>觀察有多少 pod scheduled by DaemonSet kube-proxy --&gt; 1個</span></li>
</ul><p><img src="https://i.imgur.com/pBA7ap7.png" alt="" loading="lazy"></p><ul>
<li><ins><span>daemonset create</span></ins></li>
</ul><pre><code>kubectl create deployment elasticsearch --namespace=kube-system \
--image=registry.k8s.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml &gt; q1.yaml
</code></pre><p><span>創建 daemonset yaml (因為 kubectl 沒有 kubectl create daemonset command 所以先用 kubectl create deployment 再去做修正)</span></p><h4 id="Example" data-id="Example"><a class="anchor hidden-xs" href="#Example" title="Example"><span class="octicon octicon-link"></span></a><span>Example</span></h4><p><img src="https://i.imgur.com/4G1XOlG.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset" target="_blank" rel="noopener"><span>k8s.io - Create a DaemonSet</span></a></p><pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ds-important
  namespace: project-tiger
  labels:
    id: ds-important
    uuid: 18426a0b-5f59-4e10-923f-c0e078e82462
spec:
  selector:
    matchLabels:
      id: ds-important
      uuid: 18426a0b-5f59-4e10-923f-c0e078e82462
  template:
    metadata:
      labels:
        id: ds-important
        uuid: 18426a0b-5f59-4e10-923f-c0e078e82462
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: ds-important
        image: httpd:2.4-alpine
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
</code></pre><hr><div class="alert alert-info">
<p><span>監控類型使用 DaemonSet（每一個Node都一定要有的服務）</span><br>
<span>有特殊需求可以使用 StatefulSet</span><br>
<span>沒任何要求就使用 Deployment，因為可以 rollback</span></p>
</div><h3 id="5-StatefulSet" data-id="5-StatefulSet"><a class="anchor hidden-xs" href="#5-StatefulSet" title="5-StatefulSet"><span class="octicon octicon-link"></span></a><span>5. StatefulSet</span></h3><p><span>透過 Deployment 部署的 POD，他都是 stateless, statefulset 跟 Deployment 的差別就在於它對對每一個 POD 產生固定的識別資訊，不會因為 POD 重啟而有所變動，所掛載的硬碟也都可以持續使用.</span></p><p><span>使用場景:</span></p><ul>
<li><span>需要持久性儲存，不因為 POD 重啟而需要重新設定硬碟給它</span></li>
</ul><h3 id="6-Static-pod" data-id="6-Static-pod"><a class="anchor hidden-xs" href="#6-Static-pod" title="6-Static-pod"><span class="octicon octicon-link"></span></a><span>6. Static pod</span></h3><p><span>Static Pod 不需依靠 Controller Plane 的物件，所以可以透過 Static Pod創建屬於自己 Node 中的 controller plane物件</span><br>
<span>ex: Control Plane 中的 controller.yaml, etcd.yaml等等，其實都算是一種 Static Pod</span></p><p><span>Pod依創建方式可分為兩種</span></p><ul>
<li><span>Static Pod</span></li>
<li><span>kube-apiserver</span></li>
</ul><div class="alert alert-success">
<p><span>kubectl get po -A --&gt; 只要Pod name後面跟著 &lt;Control Plane-Name&gt; 的，就都是Static Pod</span></p>
</div><p><span>下面這張圖有4個 static pod</span><br>
<img src="https://i.imgur.com/ewdSnzZ.png" alt="" loading="lazy"></p><h4 id="Static-Pod-Path" data-id="Static-Pod-Path"><a class="anchor hidden-xs" href="#Static-Pod-Path" title="Static-Pod-Path"><span class="octicon octicon-link"></span></a><span>Static Pod Path</span></h4><p><span>Static Pod 的創建方式是由 kubelet 定期掃描特定目錄下的 YAML file 來創建</span></p><ul>
<li><span>/var/lib/kubelet/config.yaml</span></li>
</ul><pre><code>下圖紅色圈起來的 /etc/kubernetes/manifests 路徑就是創建 Static Pod 的特定目錄
</code></pre><p><img src="https://i.imgur.com/qZ0cpLe.png" alt="" loading="lazy"></p><h4 id="Create-statis-pod" data-id="Create-statis-pod"><a class="anchor hidden-xs" href="#Create-statis-pod" title="Create-statis-pod"><span class="octicon octicon-link"></span></a><span>Create statis pod</span></h4><p><img src="https://i.imgur.com/frLT5gU.png" alt="" loading="lazy"></p><p><span>注意 --command 一定要放在命令最後面</span></p><pre><code>kubectl run static-busybox --image=busybox --dry-run=client -o yaml \ 
--command -- sleep 1000 &gt; q1.yaml
mv q1.yaml /etc/kubernetes/manifests/
</code></pre><p><span>建立好後，把 ymal 放到 Static Pod 的特定目錄, 他就會自動建立好 statis pod</span></p><p><span>注意 restartPolicy: Never 會自動重啟</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox:1.28.4
    name: static-busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
</code></pre><hr><h2 id="Logging-amp-Monitoring" data-id="Logging-amp-Monitoring"><a class="anchor hidden-xs" href="#Logging-amp-Monitoring" title="Logging-amp-Monitoring"><span class="octicon octicon-link"></span></a><span>Logging &amp; Monitoring</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section4-Loggin%20%26%20Monitoring/Kubernetes%2B-CKA-%2B0300%2B-%2BLogging-Monitoring.pdf" target="_blank" rel="noopener"><span>Logging-Monitoring.pdf</span></a></p><h3 id="1-K8s-Monitor-Cluster-Components" data-id="1-K8s-Monitor-Cluster-Components"><a class="anchor hidden-xs" href="#1-K8s-Monitor-Cluster-Components" title="1-K8s-Monitor-Cluster-Components"><span class="octicon octicon-link"></span></a><span>1. K8s Monitor Cluster Components</span></h3><p><span>K8s本身就具備一些基本的伺服器監控工具，例如：</span></p><ul>
<li><span>K8s Dashboard：插件工具，展示每個 K8s 集群上的資源利用情況，也是實現資源和環境管理與交互的主要工具。</span></li>
<li><span>Pod liveness probe：Container健康狀態診斷工具。</span></li>
<li><span>Kubelet：每個 Node 上都運行著 Kubelet，監控Container的運行情況。 Kubelet 也是 Control Plane 與各個 Node 通信的渠道。</span></li>
</ul><h3 id="2-Metric-Server" data-id="2-Metric-Server"><a class="anchor hidden-xs" href="#2-Metric-Server" title="2-Metric-Server"><span class="octicon octicon-link"></span></a><span>2. Metric Server</span></h3><p><a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener"><span>metrics-server Github</span></a></p><p><span>直接將 Github 上的 open spurce pull 到本地端並創建其中物件</span></p><pre><code>git clone https://github.com/kubernetes-sigs/metrics-server.git
or
kubectl apply -f https://github.com/kubernetes-sigs/metrics- \
server/releases/download/v0.3.6/components.yaml
</code></pre><p><span>該 YAML會於 kube-system namespace創建一個Deployment</span><br>
<img src="https://i.imgur.com/dTo7Qx1.png" alt="" loading="lazy"></p><pre><code>kubectl top no &lt;node-name&gt;
</code></pre><p><img src="https://i.imgur.com/UMx7Pol.png" alt="" loading="lazy"></p><p><span>Node 中是透過kubelet 來管理 Node 的運作，kubelet需要透過 apiserver 接收Control Plane下達的命令在 Node中 運行 Pod.</span><br>
<span>而 kubelet 中其實還包含一個 subcomponent，稱為 cAdvisor 或是 Container Advisor</span><br>
<span>cAdvisor 負責從 Pod 中擷取performance metrics，並將收集到的數據以metrics-api的形式，透過Summary API expose 給 Metric-Server</span></p><p><img src="https://i.imgur.com/lWPWbWR.png" alt="" loading="lazy"></p><ul>
<li><span>Kubernetes log</span></li>
</ul><pre><code>Kubernetes logs -f 
</code></pre><h2 id="Application-Lifecycle-Managment" data-id="Application-Lifecycle-Managment"><a class="anchor hidden-xs" href="#Application-Lifecycle-Managment" title="Application-Lifecycle-Managment"><span class="octicon octicon-link"></span></a><span>Application Lifecycle Managment</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section5-Application%20Lifecycle%20Management/Kubernetes%2B-CKA-%2B0400%2B-%2BApplication%2BLifecycle%2BManagement.pdf" target="_blank" rel="noopener"><span>Application+Lifecycle+Management.pdf</span></a></p><h3 id="1-Rolling-Update-amp-Rollbacks" data-id="1-Rolling-Update-amp-Rollbacks"><a class="anchor hidden-xs" href="#1-Rolling-Update-amp-Rollbacks" title="1-Rolling-Update-amp-Rollbacks"><span class="octicon octicon-link"></span></a><span>1. Rolling Update &amp; Rollbacks</span></h3><p><span>Deployment 這個物件裡面會包含了 ReplicaSet，然後再透過 ReplicaSet 來掌管 Pod 運行數Kubernetes 中回滾機制 (Rolling Update / Back)，就是透過 ReplicasSet 來達到</span></p><ul>
<li><span>每次 Rolling Update (更新)都會產生新的 ReplicaSet 來管控 Pod，可以把它想成產生一個新版本</span></li>
<li><span>Rollbacks (回滾) 則是把現有版本切換到上一個或指定的版本</span></li>
</ul><p><img src="https://i.imgur.com/1AZqz5W.png" alt="" loading="lazy"></p><div class="alert alert-success">
<p><span>kubectl rollout status &lt;deployment&gt;  --&gt; 查看歷史 rollout 狀態</span><br>
<span>kubectl rollout history &lt;deployment&gt;  --&gt; 查看歷史 rollout 紀錄</span><br>
<span>kubectl rollout undo &lt;deployment&gt;  --&gt; roll back 版本</span><br>
<span>kubectl rollout pause &lt;deployment&gt;  --&gt; 暫停資源</span><br>
<span>kubectl rollout resume &lt;deployment&gt;  --&gt; 恢復暫停之資源</span></p>
</div><h4 id="Deployment-Strategy" data-id="Deployment-Strategy"><a class="anchor hidden-xs" href="#Deployment-Strategy" title="Deployment-Strategy"><span class="octicon octicon-link"></span></a><span>Deployment Strategy</span></h4><p><span>K8s 中有兩種版本升級的strategies，一種是recreate，一種為rolling update.</span></p><ul>
<li><ins><span>recreate</span></ins><br>
<span>直接將所有 Pod一次升級，一次刪除所有舊版Pod，再一次創建所有新版Pod，缺點是更新過程會暫時中斷服務；</span></li>
<li><ins><span>rolling update</span></ins><br>
<span>將舊版 Pod 一個一個刪除，再一個一個創建新的，可以保證更新期間提供的服務不會中斷</span></li>
</ul><p><img src="https://i.imgur.com/ZIL9m62.png" alt="" loading="lazy"></p><h4 id="Rolling-Update" data-id="Rolling-Update"><a class="anchor hidden-xs" href="#Rolling-Update" title="Rolling-Update"><span class="octicon octicon-link"></span></a><span>Rolling Update</span></h4><p><span>有三種方式來進行滾動升級 (以升級 docker image 為例)</span><br>
<ins><span>–record 參數:</span></ins><br>
<span>這參數主要是告知 Kubernetes 紀錄此次下達的指令，能清楚不同的版本(revision) 間做了什麼操作</span></p><ul>
<li><span>set image</span></li>
</ul><pre><code>kubectl set image deployment &lt;deployment&gt; &lt;container&gt;=&lt;image&gt; --record
</code></pre><ul>
<li><span>replace</span><br>
<span>修改 xxx.yaml 內的 image 版本</span></li>
</ul><pre><code>kubectl replace -f &lt;yaml&gt; --record
</code></pre><ul>
<li><span>edit</span></li>
</ul><pre><code>kubectl edit deployment &lt;deployment&gt; --record
</code></pre><hr><p><img src="https://i.imgur.com/YaRxY5J.jpg" alt="" loading="lazy"></p><h3 id="2-Commands-and-Arguments" data-id="2-Commands-and-Arguments"><a class="anchor hidden-xs" href="#2-Commands-and-Arguments" title="2-Commands-and-Arguments"><span class="octicon octicon-link"></span></a><span>2. Commands and Arguments</span></h3><p><span>創建 Pod 時，可以為其下的 container 設置啟動時要執行的命令及 帶入參數.</span><br>
<span>如果要設置命令，就填寫在配置文件的 command 後面，如果要設置命令的帶入參數，就填寫在配置文件的 args 後面.</span><br>
<span>一但 Pod 創建完成，該命令及其帶入參數 就無法再進行更改了</span></p><h4 id="Example1" data-id="Example"><a class="anchor hidden-xs" href="#Example1" title="Example1"><span class="octicon octicon-link"></span></a><span>Example</span></h4><pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  containers:
  - image: nginx
    name: test
    resources: {}
    command: ["/bin/sh","-c"]   --&gt; 運行的命令
    args: ["echo '222'&gt;/222.txt;sleep 30"]  --&gt; 命令的參數
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre><ul>
<li><span>登入進去能看到在/ 目錄生成了一個內容是222 之 222.txt file</span></li>
</ul><p><img src="https://i.imgur.com/J3LMGHL.png" alt="" loading="lazy"></p><h3 id="3-ConfigMap" data-id="3-ConfigMap"><a class="anchor hidden-xs" href="#3-ConfigMap" title="3-ConfigMap"><span class="octicon octicon-link"></span></a><span>3. ConfigMap</span></h3><p><span>ConfigMap 與 Pod 可以單獨存在於 k8s 叢集中，當 Pod 需要使用 ConfigMap 時才需要將 ConfigMap 掛載到 Pod 內使用</span></p><div class="alert alert-info">
<p><span>Decoupled (解耦)</span></p>
<ul>
<li><span>便於管理</span></li>
<li><span>高彈性：易於掛載不同的 ConfigMap 到 Pod 內使用</span></li>
<li><span>一處編輯多處使用：同一個 ConfigMap 可掛載到多個 Pod 使用</span></li>
</ul>
</div><p><span>k8s 中的 ConfigMap/Secret 可以接受兩種來源:</span></p><ul>
<li><span>–from-literal</span></li>
</ul><pre><code>kubectl create configmap myconfig --from-literal=k1=v1 --from-literal=k2=v2
</code></pre><p><img src="https://i.imgur.com/eVf7tI7.png" alt="" loading="lazy"></p><ul>
<li><span>–from-file</span></li>
</ul><pre><code>kubectl create configmap myconfigfromkey --from-file=fromfilekey=.env
</code></pre><p><img src="https://i.imgur.com/ReZgUp4.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Configmap in Pods 有三種型式</span></li>
</ul><p><img src="https://i.imgur.com/ebjgXcy.png" alt="" loading="lazy"></p><h4 id="Configmap-in-Pods" data-id="Configmap-in-Pods"><a class="anchor hidden-xs" href="#Configmap-in-Pods" title="Configmap-in-Pods"><span class="octicon octicon-link"></span></a><span>Configmap in Pods</span></h4><p><span>將 ConfigMap 內的 key 直掛在 Pod 環境變數的例子</span></p><pre><code>apiVersion: v1
kind: Pod 
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  containers:
  - image: nginx
    name: test
    command: ["/bin/sh", "-c", "env"]
    resources: {}
    env:
    - name: MY_CONFIG_KEY  &lt;=== 指定一個新的環境變數名稱為 MY_CONFIG_KEY
      valueFrom:
        configMapKeyRef:  &lt;=== MY_CONFIG_KEY 將會參考 myconfig 內的 k1 值
          name: myconfig  &lt;=== myconfig 是上面宣告的 ConfigMap 物件
          key: k1
  dnsPolicy: ClusterFirst
  restartPolicy: Never
</code></pre><hr><h3 id="4-Secret" data-id="4-Secret"><a class="anchor hidden-xs" href="#4-Secret" title="4-Secret"><span class="octicon octicon-link"></span></a><span>4. Secret</span></h3><p><span>Secrets 是 Kubernetes 提供開發者存放敏感資訊的方式</span><br>
<span>像是密碼、OAuth tokens 及 ssh keys 等等… 將這些資訊存在放 Secret 中比直接放在 Pod YAML 或 Image中更加安全和靈活</span></p><div class="alert alert-success">
<p><span>Remember that secrets encode data in </span><strong><span>base64 format</span></strong><span>.</span><br>
<span>Anyone with the base64 encoded secret can easily decode it.</span></p>
</div><ul>
<li><ins><span>kubectl apply -f test_secret.yaml</span></ins>
<ul>
<li><span>echo -n “xx” | base64</span></li>
<li><span>echo -n “xx” | base64 --decode</span></li>
</ul>
</li>
</ul><pre><code># test_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd29yZA== 
</code></pre><h4 id="Secret-in-Pods--1" data-id="Secret-in-Pods--1"><a class="anchor hidden-xs" href="#Secret-in-Pods--1" title="Secret-in-Pods--1"><span class="octicon octicon-link"></span></a><span>Secret in Pods -1</span></h4><p><span>下面兩種方式 valueFrom &amp; envFrom</span></p><p><span>考試的時候, 可以去 </span><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/" target="_blank" rel="noopener"><span>kubernetes.io</span></a><span> document 搜尋 example</span><br>
<span>secret --&gt; Using Secrets as environment variables --&gt; </span><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data" target="_blank" rel="noopener"><span>Define container environment variables using Secret data</span></a></p><h5 id="valueFrom" data-id="valueFrom"><a class="anchor hidden-xs" href="#valueFrom" title="valueFrom"><span class="octicon octicon-link"></span></a><span>valueFrom</span></h5><ul>
<li><ins><span>將Secret作為容器的環境變數</span></ins><br>
<span>直接在 Pod 中以 spec.env.valueFrom.secretKeyRef 欄位參考到 Secret 即可</span></li>
</ul><pre><code>sudo kubectl apply -f secret_test.yaml
sudo kubectl exec -it secret-env-pod sh
</code></pre><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: secret-env-pod                                                     
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: test-secret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: test-secret
            key: password
  restartPolicy: Never
</code></pre><ul>
<li><span>創建完成後，進入pod 並echo 即可看到 Secret 的設置結果</span><br>
<img src="https://i.imgur.com/8dDcH3e.png" alt="" loading="lazy"></li>
</ul><h5 id="envFrom" data-id="envFrom"><a class="anchor hidden-xs" href="#envFrom" title="envFrom"><span class="octicon octicon-link"></span></a><span>envFrom</span></h5><ul>
<li><span>dotfile-secret  為 kubectl secret name</span></li>
</ul><pre><code>apiVersion: v1
kind: Pod 
metadata:
  name: secret-env-pod                                                         
spec:
  containers:
  - name: secret-env-pod
    image: redis
    envFrom:
      - secretRef:
          name: dotfile-secret 
</code></pre><p><img src="https://i.imgur.com/dIA1Amm.png" alt="" loading="lazy"></p><h5 id="Secret-in-Pods---Volume" data-id="Secret-in-Pods---Volume"><a class="anchor hidden-xs" href="#Secret-in-Pods---Volume" title="Secret-in-Pods---Volume"><span class="octicon octicon-link"></span></a><span>Secret in Pods - Volume</span></h5><p><span>將 Secret 製作為一個檔案，Pod 以 Volume的形式將此檔案掛載(mount)到容器上</span></p><ul>
<li><ins><span>先用 kubectl 創建一個基礎的 yaml</span></ins></li>
</ul><pre><code>sudo kubectl run nginx --image=nginx --restart=Never \ 
--dry-run=client -o yaml &gt; secret_volume.yaml
</code></pre><h4 id="Question" data-id="Question"><a class="anchor hidden-xs" href="#Question" title="Question"><span class="octicon octicon-link"></span></a><span>Question</span></h4><p><a href="https://ithelp.ithome.com.tw/articles/10239675" target="_blank" rel="noopener"><span>Ref</span></a></p><p><img src="https://i.imgur.com/fkVjs2S.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><ol>
<li><span>創建一個 Secret</span></li>
<li><span>創建兩個 Pod (kubectl run 先建立兩個基礎 pod yaml)</span></li>
<li><span>其中一個 Podmount 此 Secret，另一個 Pod 將 Secret 作為環境變數使用</span></li>
</ol><pre><code>1. sudo kubectl create secret generic super-secret \ 
--from-literal=credential=alice --from-literal=username=bob
2-1. kubectl run pod-secrets-via-file --image=redis --dry-run=client -o yaml &gt; q5-1-pod.yaml
2-2. kubectl run pod-secrets-via-env --image=redis --dry-run=client -o yaml &gt; q5-2-pod.yaml
3.kubectl apply -f q5-1-pod.yaml q5-2-pod.yaml
</code></pre><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q5.yaml" target="_blank" rel="noopener"><span>q5-1-pod.yaml</span></a><br>
<a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q5-2.yaml" target="_blank" rel="noopener"><span>q5-2-pod.yaml</span></a></p><p><img src="https://i.imgur.com/kDNbK8K.png" alt="" loading="lazy"></p><h3 id="5-Multi-container-Pod" data-id="5-Multi-container-Pod"><a class="anchor hidden-xs" href="#5-Multi-container-Pod" title="5-Multi-container-Pod"><span class="octicon octicon-link"></span></a><span>5. Multi-container Pod</span></h3><p><span>大部分的 pod 只運行一個 container, 但在某些情況下</span><br>
<span>我們必須要在一個 pod 運行兩個以上的 containers, 這種 multi-container 的模式又可以衍伸至三種 design patterns:</span></p><ol>
<li><ins><span>sidecar pattern:</span></ins><span> 最簡單的方式， 兩個container利用volume的方式share同一個檔案目錄</span></li>
<li><ins><span>adapter pattern:</span></ins><span> 利用另外一個 container 做接口， 把要輸出的資料格式化， 例如同一套 log system 便可以處理不同 pods 的 log</span></li>
<li><ins><span>ambassador pattern:</span></ins><span> 假若開發環境是在 local, 此種模式可以將 pod 要輸出的資料直接寫在 local 的資料庫等</span></li>
</ol><p><img src="https://i.imgur.com/lsMnCO9.png" alt="" loading="lazy"></p><p><a href="https://tachingchen.com/tw/blog/desigining-distributed-systems-the-sidecar-pattern-concept/" target="_blank" rel="noopener"><span>邊車模式 (The Sidecar Pattern) - 介紹</span></a></p><h4 id="example" data-id="example"><a class="anchor hidden-xs" href="#example" title="example"><span class="octicon octicon-link"></span></a><span>example</span></h4><p><img src="https://i.imgur.com/X6f8ZAU.png" alt="" loading="lazy"></p><p><span>注意只需要寫一個 container block 即可</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep","1000"]
  - image: redis
    name: gold
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre><h3 id="6-Init-Container" data-id="6-Init-Container"><a class="anchor hidden-xs" href="#6-Init-Container" title="6-Init-Container"><span class="octicon octicon-link"></span></a><span>6. Init Container</span></h3><p><span>Init Container 和 Pod Container 定義在同一個 Pod YAML 中，通常是用於幫助 Pod Container 運行的前置作業。</span><br>
<span>像是 Pod Container 需要將執行結果輸出到某一檔案，但該檔案初始並不存在，這時就可以利用Init Container 在 Pod Container 運行前先將檔案創建，以供使用。</span></p><div class="alert alert-success">
<ul>
<li><span>Init Container 是運行於 Pod Container之前的專用容器.</span></li>
<li><span>Init Conatiner 可以應用於一些不包含 setup environment 的 image.</span></li>
<li><span>Init Container 和 Pod Container 不會同時存在於同一 Pod 中，Pod Container 會等待Init Container 運行到完成狀態後才創建.</span></li>
</ul>
</div><p><span>Pod 中可以有多個 Pod Container，也可以有一個或多個 Init Container，它們在啟動 Pod Container 之前就已運行。</span></p><ul>
<li><span>它們的生命週期如下圖</span><br>
<img src="https://i.imgur.com/xi8TAa8.png" alt="" loading="lazy"></li>
</ul><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#init-containers-in-use" target="_blank" rel="noopener"><span>k8s.io -Init containers in use</span></a></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
</code></pre><hr><h2 id="Cluster-Maintenance" data-id="Cluster-Maintenance"><a class="anchor hidden-xs" href="#Cluster-Maintenance" title="Cluster-Maintenance"><span class="octicon octicon-link"></span></a><span>Cluster Maintenance</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section6-Cluster%20Maintenance/Kubernetes-CKA-0500-Cluster%2BMaintenance-v1.2.pdf" target="_blank" rel="noopener"><span>Kubernetes-CKA-0500-Cluster+Maintenance-v1.2.pdf</span></a></p><h3 id="0-Kubernets-Software-Versions" data-id="0-Kubernets-Software-Versions"><a class="anchor hidden-xs" href="#0-Kubernets-Software-Versions" title="0-Kubernets-Software-Versions"><span class="octicon octicon-link"></span></a><span>0. Kubernets Software Versions</span></h3><ul>
<li><span>檢查版本</span></li>
</ul><pre><code>kubectl version --client
</code></pre><p><span>Kubernetes的版本v1.25.2可以分為三個部分：1、25、2，我們分別稱為Major、Minor、Patch。</span></p><ul>
<li><span>Major：主要版本</span></li>
<li><span>Minor：次要，代表特點和功能上的更新</span></li>
<li><span>Patch：補丁，代表修復Bug</span></li>
</ul><p><img src="https://i.imgur.com/S7vTbad.png" alt="" loading="lazy"></p><p><span>Kubernetes的各個元件，如：kube-apiserver、Controller-manager等，彼此的版本都有相依性，也就是版本不能相差太多</span></p><p><span>Kubernetes的更新週期是三個月，也就是每三個月會推出新版本</span></p><p><img src="https://i.imgur.com/nF8pJbC.png" alt="" loading="lazy"></p><hr><p><a href="https://kodekloud.com/topic/practice-test-cluster-upgrade-process-2/" target="_blank" rel="noopener"><span>kodekloud - Practiec: Cluster Upgrade 升版練習</span></a><br>
<span>&lt;Master Node&gt;</span></p><ul>
<li><span>一定要記得 Upgrade kubelet and kubect</span></li>
<li><span>Restart the kubelet (sudo systemctl daemon-reload, sudo systemctl restart kubelet)</span></li>
</ul><p><span>&lt;Worker Node&gt;</span></p><ul>
<li><span>一定要記得 ssh 進去 node 裡面做</span></li>
</ul><hr><h3 id="1-Cluster-Upgrade" data-id="1-Cluster-Upgrade"><a class="anchor hidden-xs" href="#1-Cluster-Upgrade" title="1-Cluster-Upgrade"><span class="octicon octicon-link"></span></a><span>1. Cluster Upgrade</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10301864" target="_blank" rel="noopener"><span>Upgrade Strategy</span></a></p><p><span>Cluster Upgrade主要有三種策略，三者的共通點是</span><strong><span>必須先更新 Master Node</span></strong><span>：</span></p><ol>
<li><ins><span>Strategy-1:</span></ins><span> 一次同時更新全部的 Worker Node。</span></li>
<li><ins><span>Strategy-2:</span></ins><span> 一次更新一個 Worker Node，更新成功後再更新下一個Worker Node，直到全部Worker Node更新完成。</span></li>
<li><ins><span>Strategy-3:</span></ins><span> 一次加入一個新版本的 Worker Node，加入後再剔除一個舊版本的 Worker Node，直到全部 Worker Node更新完成。</span></li>
</ol><p><span>3和2的主要差別是加入新的Node來取代舊的Node，3是EKS目前的更新方式。</span></p><hr><div class="alert alert-success">
<p><span>分成 Upgrading control plane nodes 跟 Upgrade worker nodes 兩部分</span></p>
</div><p><span>兩種 Node 的升級步驟都可以參考 </span><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/" target="_blank" rel="noopener"><span>k8s document</span></a></p><h3 id="11-Upgrading-control-plane-nodes" data-id="11-Upgrading-control-plane-nodes"><a class="anchor hidden-xs" href="#11-Upgrading-control-plane-nodes" title="11-Upgrading-control-plane-nodes"><span class="octicon octicon-link"></span></a><span>1.1 Upgrading control plane nodes</span></h3><p><strong><ins><span>step0:</span></ins><span> Check the current version</span></strong></p><p><img src="https://i.imgur.com/3PX8baW.png" alt="" loading="lazy"></p><pre><code>apt update
apt-cache madison kubeadm
# find the latest 1.26 version in the list
# it should look like 1.26.x-00, where x is the latest patch
</code></pre><p><strong><ins><span>step1:</span></ins></strong><br>
<span>檢查可以更新到哪個版本，輸入下面的指令 kubeadm 會告訴你目前可以升級到什麼版本</span></p><pre><code>kubeadm upgrade plan
</code></pre><p><span>目前版本: v1.25.0</span><br>
<span>遠端版本 (remote version): v1.26.1</span></p><p><img src="https://i.imgur.com/3rtBcKq.png" alt="" loading="lazy"></p><p><strong><ins><span>step2:</span></ins><span> upgrage kubeadm version</span></strong></p><p><span>replace x in 1.26.0-00 with the latest patch version</span></p><pre><code>kubeadm version
apt-get update &amp;&amp; apt-get install -y kubeadm=1.26.0-00 &amp;&amp; apt-mark hold kubeadm
</code></pre><p><img src="https://i.imgur.com/BHg4ATH.png" alt="" loading="lazy"></p><pre><code>sudo kubeadm upgrade apply v1.26.0
</code></pre><p><span>成功把 kubeadm 從 v1.25.0 升級到 v1.26.0</span><br>
<img src="https://i.imgur.com/ZHLwXLz.png" alt="" loading="lazy"></p><p><strong><ins><span>step3:</span></ins><span> Upgrade kubelet and kubectl</span></strong></p><pre><code>sudo apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubelet=1.26.x-00 kubectl=1.26.x-00 &amp;&amp; apt-mark hold kubelet kubectl
</code></pre><p><img src="https://i.imgur.com/zBVNnBA.png" alt="" loading="lazy"></p><p><strong><ins><span>step4:</span></ins><span> Restart the kubelet:</span></strong><br>
<span>重啟kubelet，使其更新生效</span></p><pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre><p><strong><ins><span>step5:</span></ins><span> Check the upgrade version</span></strong></p><p><span>檢查 control plane nodes 版本是否有確實升級</span></p><p><img src="https://i.imgur.com/X1QjFOg.png" alt="" loading="lazy"></p><p><strong><ins><span>step6:</span></ins><span> Uncordon the node</span></strong></p><p><span>升級完後, 把 control plane nodes 調回 schedulable</span></p><pre><code># replace &lt;node-to-uncordon&gt; with the name of your node
kubectl uncordon &lt;node-to-uncordon&gt;
</code></pre><p><img src="https://i.imgur.com/0E5fLFC.png" alt="" loading="lazy"></p><hr><h3 id="12-Upgrade-worker-nodes" data-id="12-Upgrade-worker-nodes"><a class="anchor hidden-xs" href="#12-Upgrade-worker-nodes" title="12-Upgrade-worker-nodes"><span class="octicon octicon-link"></span></a><span>1.2 Upgrade worker nodes</span></h3><p><span>The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time, without compromising the minimum required capacity for running your workloads.</span></p><p><strong><ins><span>step0:</span></ins><span> ssh worker node</span></strong></p><pre><code>ssh &lt;woker-node-ip&gt;
</code></pre><p><img src="https://i.imgur.com/CrKsiCN.png" alt="" loading="lazy"></p><p><strong><ins><span>step1:</span></ins><span> Upgrade kubeadm</span></strong></p><pre><code>apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubeadm=1.26.0-00 &amp;&amp; apt-mark hold kubeadm
</code></pre><p><strong><span>++step2:++kubeadm upgrade</span></strong></p><p><img src="https://i.imgur.com/0lrKS2Y.png" alt="" loading="lazy"></p><p><strong><span>++step3:++Upgrade kubelet and kubectl</span></strong></p><pre><code>apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; \
apt-get install -y kubelet=1.26.0-00 kubectl=1.26.0-00 &amp;&amp; apt-mark hold kubelet kubectl
</code></pre><p><strong><ins><span>step4:</span></ins><span> Restart the kubelet:</span></strong><br>
<span>重啟kubelet，使其更新生效</span></p><pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre><p><strong><ins><span>step5:</span></ins><span> Check the upgrade version</span></strong></p><p><span>檢查 worker nodes 版本是否有確實升級</span></p><p><img src="https://i.imgur.com/X1QjFOg.png" alt="" loading="lazy"></p><p><strong><ins><span>step6:</span></ins><span> Uncordon the node</span></strong></p><p><span>升級完後, 把 worker nodes 調回 schedulable</span></p><p><img src="https://i.imgur.com/8tZnUwS.png" alt="" loading="lazy"></p><hr><h3 id="2-Node-Maintenance" data-id="2-Node-Maintenance"><a class="anchor hidden-xs" href="#2-Node-Maintenance" title="2-Node-Maintenance"><span class="octicon octicon-link"></span></a><span>2. Node Maintenance</span></h3><p><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller" target="_blank" rel="noopener"><span>Node Controller</span></a><span> 是 Kubernetes 中用來管理 Node 的一個物件</span></p><p><span>kubectl drain 代表將該 Node 狀態變更為維護模式，如此在該 Node 上面的 Pod，就會轉移到其他 Node 上.</span><br>
<span>若不是透過 Replication Controller 或是 DaemonSet 等創建好的 Pod，則該指令會失敗，除非加上 --force</span></p><ul>
<li><ins><span>從 Kubernetes Cluster 移除 node</span></ins></li>
</ul><pre><code>kubectl drain {node_name}
kubectl drain {node_name} --force

kubectl cordon {node_name}   --&gt; 標記 Node 為不可 Schedule
kubectl uncordon {node_name} --&gt; 恢復 Node
</code></pre><p><span>如果在 Node 上，有daemonset 類型的pod，不可以被刪除，需要加上下面的參數</span></p><ul>
<li><ins><span>–ignore-daemonsets</span></ins></li>
</ul><pre><code>kubectl drain node01 --ignore-daemonsets
</code></pre><p><img src="https://i.imgur.com/JSYb37l.png" alt="" loading="lazy"></p><p><span>完成後可以看到 node01 的 Status 為 </span><strong><span>SchedulingDisabled</span></strong><br>
<img src="https://i.imgur.com/y2Yvtu1.png" alt="" loading="lazy"></p><p><span>如果想要讓他恢復可被調度，使用 </span><strong><span>uncordon</span></strong><span> 後可以看到 Status 變回 Ready</span><br>
<img src="https://i.imgur.com/Uch5ONU.png" alt="" loading="lazy"></p><h4 id="cordon-vs-drain-vs-delete" data-id="cordon-vs-drain-vs-delete"><a class="anchor hidden-xs" href="#cordon-vs-drain-vs-delete" title="cordon-vs-drain-vs-delete"><span class="octicon octicon-link"></span></a><span>cordon vs drain vs delete</span></h4><p><span>k8s 命令對 node 調度 cordon，drain，delete </span><a href="https://blog.csdn.net/erhaiou2008/article/details/104986006" target="_blank" rel="noopener"><span>區別</span></a></p><ul>
<li>
<p><ins><span>cordon</span></ins><br>
<span>影響最小，只會將 node 調整為 </span><strong><span>SchedulingDisabled</span></strong><br>
<span>之後再創建 pod，不會被調度到該節點</span><br>
<span>舊有的 pod 不會受到影響，仍正常對外提供服務</span></p>
<pre><code># 恢復調度
kubectl uncordon node_name
</code></pre>
</li>
<li>
<p><ins><span>drain</span></ins><br>
<span>驅逐 node 上的 pod，其他節點重新創建.</span><br>
<span>將節點調整為 </span><strong><span>SchedulingDisabled</span></strong></p>
<pre><code># 恢復調度
kubectl uncordon node_name
</code></pre>
</li>
<li>
<p><ins><span>delete</span></ins><br>
<span>驅逐 node 上的 pod，其他節點重新創建</span><br>
<span>然後，從 master節點刪除該 node，master 對其不可見，失去對她的控制，master 不可對他恢復</span></p>
<pre><code># 恢復調度 需進入 node 節點，重啟 kubelet
systemctl restart kubelet
</code></pre>
</li>
</ul><h3 id="3-Backup-amp-Restore-Methods" data-id="3-Backup-amp-Restore-Methods"><a class="anchor hidden-xs" href="#3-Backup-amp-Restore-Methods" title="3-Backup-amp-Restore-Methods"><span class="octicon octicon-link"></span></a><span>3. Backup &amp; Restore Methods</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10301931" target="_blank" rel="noopener"><span>Ref: ETCD與集群的備份與還原</span></a><br>
<a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14298862#overview" target="_blank" rel="noopener"><span>Kodekloud: Practice Test - Backup and Restore Methods</span></a></p><p><span>Kubernetes Cluster Object 的備份方式有兩種</span></p><h4 id="1-儲存-YAML-檔" data-id="1-儲存-YAML-檔"><a class="anchor hidden-xs" href="#1-儲存-YAML-檔" title="1-儲存-YAML-檔"><span class="octicon octicon-link"></span></a><span>1. </span><ins><span>儲存 YAML 檔</span></ins></h4><p><span>備份所有 namespace 中的 deploy, service 狀態</span><br>
<code>    sudo kubectl get all -A -o yaml &gt; all-deploy-service.yaml    </code></p><h4 id="2-將ETCD-Cluster-使用etcdctl備份成快照檔" data-id="2-將ETCD-Cluster-使用etcdctl備份成快照檔"><a class="anchor hidden-xs" href="#2-將ETCD-Cluster-使用etcdctl備份成快照檔" title="2-將ETCD-Cluster-使用etcdctl備份成快照檔"><span class="octicon octicon-link"></span></a><span>2. </span><ins><span>將ETCD Cluster 使用etcdctl備份成快照檔</span></ins></h4><p><span>若發生問題需要還原時，再將快照檔還原</span></p><div class="alert alert-success">
<p><span>&lt;ETCD Cluster&gt;</span><br>
<span>Etcd 是 Kubernetes Cluster 中的一個十分重要的元件，用於保存 Cluster 所有的網路配置和對象的狀態訊息</span></p>
</div><p><span>etcd 的備份有兩種方式：</span></p><ul>
<li><ins><span>Built-in snapshot</span></ins><br>
<span>etcd 支持內建 snapshot，因此備份 etcd集群很容易。</span><br>
<span>可以使用etcdctl snapshot save 命令從集群內物件中獲取，也可以從當前未被 etcd process 使用的 etcd 資料目錄中複製 member/snap/db 文件。</span></li>
<li><ins><span>Volume snapshot</span></ins><br>
<span>如果 etcd 在支持備份的 Volume（例如Amazon Elastic Block Store）上運行，可以通過獲取存 Volume 的 snapshot來備份。</span></li>
</ul><h3 id="31-etcdctl-command" data-id="31-etcdctl-command"><a class="anchor hidden-xs" href="#31-etcdctl-command" title="31-etcdctl-command"><span class="octicon octicon-link"></span></a><span>3.1 etcdctl command</span></h3><ul>
<li><strong><ins><span>查看 etcd 的細節</span></ins></strong></li>
</ul><pre><code>k describe pod etcd-controlplane -n kube-system
cat /etc/kubernetes/manifests/etcd.yaml
</code></pre><p><span>因為 Kubernetes 集群使用 https，因此需要指定 --cert-file、–key-file和–ca-file三個參數，參數檔案都位於 /etc/kubernetes/pki/etcd目錄下</span></p><div class="alert alert-info">
<p><span>找到 advertise-client-urls, cert-file, key-file 檔案的路徑，作為之後參數使用</span><br>
<span>{$masterNodeIp} 為自己的 Master Node 的 IP</span></p>
<ul>
<li><span>–advertise-client-urls=https://{$masterNodeIp}:2379</span></li>
<li><span>–cert-file=/etc/kubernetes/pki/etcd/server.crt</span></li>
<li><span>–key-file=/etc/kubernetes/pki/etcd/server.key</span></li>
<li><span>–trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span></li>
</ul>
</div><ol>
<li><ins><span>listen-client-urls:</span></ins><br>
<span>指定 etcd server 绑定的本地地址 以接收傳入連線。</span><br>
<span>要監聽所有的 port，要使用 0.0.0.0 的IP地址。</span></li>
<li><ins><span>advertise-client-urls:</span></ins><br>
<span>建議用 Client 端的 url ，該值用在 etcd 代理 或 etcd 與node溝通, 不可以使用 localhost 這種，因為這種地址無法從遠端機器進行訪問</span></li>
<li><ins><span>trusted-ca-file:</span></ins><br>
<span>ETCD CA Certificate file located</span></li>
<li><ins><span>cert-file:</span></ins><br>
<span>ETCD server certificate located</span></li>
<li><ins><span>endpoints:</span></ins><br>
<span>不加 --endpoints 參數時，默認訪問的是 </span><a href="https://127.0.0.1:2379" target="_blank" rel="noopener"><span>https://127.0.0.1:2379</span></a></li>
</ol><ul>
<li><strong><ins><span>讓 etcdctl command 生效</span></ins></strong><br>
<span>必須先設置環境變 ETCDCTL_API=3 來使用 etcd v3, 若是覺得每次都要先宣告環境變數很麻煩，也可以使用 export 來設定環境變數，那之後的 etcdctl 指令都不需要先設置環境變數告知使用 v3了</span></li>
</ul><pre><code>ETCDCTL_API=3 etcdctl snapshot
export ETCDCTL_API=3
</code></pre><p><img src="https://i.imgur.com/To7DiXB.png" alt="" loading="lazy"></p><h3 id="32-ETCD-Store" data-id="32-ETCD-Store"><a class="anchor hidden-xs" href="#32-ETCD-Store" title="32-ETCD-Store"><span class="octicon octicon-link"></span></a><span>3.2 ETCD Store</span></h3><p><span>使用 etcdctl 保存到 /opt/snapshot-pre-boot.db</span></p><pre><code>etcdctl --endpoints 127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt \
snapshot save /opt/snapshot-pre-boot.db
--&gt; Snapshot saved at /opt/snapshot-pre-boot.db
</code></pre><p><img src="https://i.imgur.com/erRNauv.png" alt="" loading="lazy"></p><h4 id="檢查備份檔案" data-id="檢查備份檔案"><a class="anchor hidden-xs" href="#檢查備份檔案" title="檢查備份檔案"><span class="octicon octicon-link"></span></a><span>檢查備份檔案</span></h4><pre><code>etcdctl snapshot status /opt/snapshot-pre-boot.db -w table --endpoints 127.0.0.1:2379 \
--cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
--cacert=/etc/kubernetes/pki/etcd/ca.crt
</code></pre><p><img src="https://i.imgur.com/TJYvRyD.png" alt="" loading="lazy"></p><h3 id="33-ETCD-Restore-snapshot" data-id="33-ETCD-Restore-snapshot"><a class="anchor hidden-xs" href="#33-ETCD-Restore-snapshot" title="33-ETCD-Restore-snapshot"><span class="octicon octicon-link"></span></a><span>3.3 ETCD Restore snapshot</span></h3><p><span>使用保存的 /opt/snapshot-pre-boot.db snapshot 去還原</span></p><ul>
<li><ins><span>–data-dir:</span></ins><span> 代表要將 ETCD 還原的位置，這邊選擇 /var/lib/etcd-from-backup</span></li>
</ul><pre><code>etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db
</code></pre><p><img src="https://i.imgur.com/dTW8ntM.png" alt="" loading="lazy"></p><p><span>修改 /etc/kubernetes/manifests/etcd.yaml 文件內容</span><br>
<span>把 name: etcd-data 所指定的位置替換成 volume &lt;–data-dir&gt; 的 path</span></p><pre><code># 原本 /etc/kubernetes/manifests/etcd.yaml 文件內容
volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data

# 改為
volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
</code></pre><h2 id="Security" data-id="Security"><a class="anchor hidden-xs" href="#Security" title="Security"><span class="octicon octicon-link"></span></a><span>Security</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section7-Security/Kubernetes%2B-CKA-%2B0600%2B-%2BSecurity.pdf" target="_blank" rel="noopener"><span>Security.pdf</span></a></p><h3 id="1-Kubernetes-API-Server-Authentication" data-id="1-Kubernetes-API-Server-Authentication"><a class="anchor hidden-xs" href="#1-Kubernetes-API-Server-Authentication" title="1-Kubernetes-API-Server-Authentication"><span class="octicon octicon-link"></span></a><span>1. Kubernetes API Server Authentication</span></h3><p><span>kube-apiserver 是 kubernetes 的網關性質的元件，是 kubernetes cluster 資源操作的唯一入口</span><br>
<span>因此像是認證與授權等一些過程很明顯是要基於這個 kube-apiserver 元件</span></p><p><span>Kubernetes control plane 中包含了 etctd，kube-api-server，kube-scheduler，kube-controller-manager 等元件.</span><br>
<span>而這些元件之間的相互使用都是通過網路進行的。在進行網路溝通時，溝通雙方需要驗證對方的身份，以避免惡意第三方偽造身份竊取訊息 或者 對系統進行攻擊。</span><br>
<span>為了相互驗證對方的身份，溝通中的雙方 任何一方都需要做下面兩件事情：</span></p><ul>
<li><span>1.向對方提供标明自己身份的一個證書</span></li>
<li><span>2.驗證對方提供的身份證書是否合法，是否偽造的？</span></li>
</ul><p><span>三步驟</span></p><ol>
<li><span>Authentication: 認證解决的問題是辨認用户的身份</span></li>
<li><span>Authorization: 授權是明確定義 user 具有哪些權限</span></li>
<li><span>Admission Control: 准入控制是作用於 kubernetes 中的資源對象</span></li>
</ol><p><img src="https://i.imgur.com/8U2SDMN.png" alt="" loading="lazy"></p><p><span>Kubernetes 中有幾種驗證方式：</span></p><ul>
<li><span>Certificate / Token / OpenID / Web Hook</span></li>
</ul><hr><h3 id="11-Server-端證書" data-id="11-Server-端證書"><a class="anchor hidden-xs" href="#11-Server-端證書" title="11-Server-端證書"><span class="octicon octicon-link"></span></a><span>1.1 Server 端證書</span></h3><div class="alert alert-success">
<p><span>kubeadm 安裝的 cluster 中都是用 3套 CA證書來管理 和 簽發其他證書</span></p>
<ol>
<li><span>CA 给 ETCD 使用</span></li>
<li><span>kubernates 内部元件使用</span></li>
<li><span>配置 Aggregation Layer 使用的</span></li>
</ol>
</div><ul>
<li><span>kubeadm 創建的 cluster 默認證書存放路徑為 /etc/kubernetes/pki</span></li>
</ul><p><img src="https://i.imgur.com/ATLfHwx.png" alt="" loading="lazy"></p><h4 id="etcd-證書" data-id="etcd-證書"><a class="anchor hidden-xs" href="#etcd-證書" title="etcd-證書"><span class="octicon octicon-link"></span></a><ins><span>etcd 證書</span></ins></h4><p><span>etcd 證書位於 /etc/kubernetes/pki/etcd 目錄下</span></p><ul>
<li><span>etcd 對外提供服務的 server 證書及私鑰</span></li>
</ul><pre><code>server.crt  server.key
</code></pre><ul>
<li><span>etcd node 之間相互進行認證的 peer 證書、私鑰以及驗證 peer 的 CA</span></li>
</ul><pre><code>ca.crt  ca.key peer.crt  peer.key
</code></pre><ul>
<li><span>etcd 驗證訪問其服務的客户端的 CA</span></li>
</ul><pre><code>healthcheck-client.crt  healthcheck-client.key 
</code></pre><h4 id="kube-apiserver-證書" data-id="kube-apiserver-證書"><a class="anchor hidden-xs" href="#kube-apiserver-證書" title="kube-apiserver-證書"><span class="octicon octicon-link"></span></a><ins><span>kube-apiserver 證書</span></ins></h4><p><span>apiserver 證書位於 /etc/kubernetes/pki 目錄下</span></p><ul>
<li><span>訪問 etcd 的客户端證書及私鑰，這個證書是由etcd 的 CA 證書簽發，因此也需要在 apiserver 中配置 etcd 的 CA證書</span></li>
</ul><pre><code>apiserver-etcd-client.key   apiserver-etcd-client.crt   
</code></pre><ul>
<li><span>根 CA (用來簽發 k8s 中其他證書的 CA證書 及 私鑰)</span></li>
</ul><pre><code>ca.crt  ca.key
</code></pre><ul>
<li><span>apiServer 的對外提供服務的 server 證書及私鑰</span></li>
</ul><pre><code>apiserver.crt   apiserver.key 
</code></pre><h3 id="12-Client-端證書" data-id="12-Client-端證書"><a class="anchor hidden-xs" href="#12-Client-端證書" title="12-Client-端證書"><span class="octicon octicon-link"></span></a><span>1.2 Client 端證書</span></h3><p><span>Kubernetes 的元件會通過 TLS 雙向確認來驗證客户端的身份。</span><br>
<span>所有由 cluster CA 簽發的客户端證書都認為是通過認證的（Authentication)，可以正常建立HTTPS 連接，進一步 Kubernetes 會從客户端證書中讀取用户名，用户组訊息，结合 RBAC 等鑑權策略來判斷客户端是 否有對應的操作權限 (Authorization)在客户端證書中</span></p><h4 id="kubelet-client-證書" data-id="kubelet-client-證書"><a class="anchor hidden-xs" href="#kubelet-client-證書" title="kubelet-client-證書"><span class="octicon octicon-link"></span></a><ins><span>kubelet client 證書</span></ins></h4><p><img src="https://i.imgur.com/EDOVWBD.png" alt="" loading="lazy"></p><p><span>上圖 kubelet-client-2023-02-24-09-31-47.pem 表示的是 客戶端的證書, 可以查看一下證書內容.</span><br>
<span>證書之所以带日期是因为 kubelet 的證書快過期時會自動更新，因此带上時間方便區分新舊證書</span></p><div class="alert alert-info">
<ol start="0">
<li><ins><span>Serial Number</span></ins><span>: 0 (0x0) CA機構给該證書的唯一序列號碼，根證書為0 (ca.crt)</span></li>
<li><ins><span>Issuer</span></ins><span>: 證書頒發者的相關訊息</span></li>
<li><ins><span>Validity</span></ins><span>：證書生效日期 和 失效日期</span></li>
<li><ins><span>Subject</span></ins><span>: 證書持有者的相關訊息</span><br>
<span>O 表示 Organization， CN 表示 Common Name</span><br>
<span>Common Name 表示用户名，Organization 表示用户组</span></li>
<li><ins><span>Subject Public Key</span></ins><span>: server 公開的密鑰</span></li>
</ol>
</div><p><img src="https://i.imgur.com/IuyccRl.png" alt="" loading="lazy"></p><h4 id="kube-apiserver-client-證書" data-id="kube-apiserver-client-證書"><a class="anchor hidden-xs" href="#kube-apiserver-client-證書" title="kube-apiserver-client-證書"><span class="octicon octicon-link"></span></a><ins><span>kube-apiserver client 證書</span></ins></h4><p><span>kube-apiserver 使用 kubelet 接口時（執行 exec/logs 命令），kubelet 也會要求檢驗kube-apiserver 的客户端證書</span><br>
<span>該證書保存路徑為 /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></p><hr><p><span>Test 小測驗:</span></p><ol>
<li><span>Identify the certificate file used for the </span><ins><span>kube-api server</span></ins><br>
<span>–&gt; /etc/kubernetes/pki/apiserver.crt</span></li>
<li><span>Identify the Certificate file used to authenticate kube-apiserver as </span><ins><span>a client to ETCD Server</span></ins><br>
<span>–&gt; /etc/kubernetes/pki/apiserver-etcd-client.crt</span></li>
<li><span>Identify the key used to authenticate kubeapi-server to the kubelet server</span><br>
<span>–&gt; /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></li>
</ol><ul>
<li><ins><span>/etc/kubernetes/manifests/kube-apiserver.yaml</span></ins></li>
</ul><p><img src="https://i.imgur.com/HhqHNNR.png" alt="" loading="lazy"></p><ol start="4">
<li><span>What is the Common Name (CN) configured on the Kube API Server Certificate?</span><br>
<span>–&gt; kube-apiserver</span></li>
</ol><pre><code># use the feature from kubeadm to get the expiration
kubeadm certs check-expiration

# command that would renew the apiserver server certificate
kubeadm certs renew apiserver

# 證書查看 command
openssl x509 -in file-path.crt -text -noout
</code></pre><p><img src="https://i.imgur.com/mrutoM1.png" alt="" loading="lazy"></p><h3 id="2-Authorization" data-id="2-Authorization"><a class="anchor hidden-xs" href="#2-Authorization" title="2-Authorization"><span class="octicon octicon-link"></span></a><span>2. Authorization</span></h3><p><span>通過了 Authentication (身份認證)後，那僅能代表當前的使用者允許與 Kubernetes API Server 溝通，至於該使用者是否有權限(Permission)請求什麼資源，就是定義在 Authorization</span></p><p><span>Authorization Mode 有以下幾種模式：</span></p><ul>
<li><span>Node</span></li>
<li><span>ABAC (Attribute-based access control)</span></li>
<li><span>RBAC (Role-Base Access Control) -</span><strong><span>Default in k8s</span></strong></li>
<li><span>Webhook</span></li>
</ul><hr><ul>
<li><ins><span>Check the kube-apiserver settings:</span></ins><br>
<span>查看現有 cluster 的 Authorization 方法</span><br>
<span>以下圖為例, 紅色底線標記的 Node,RBAC 就是目前 cluster 的 Authorization 方法</span></li>
</ul><pre><code>cat /etc/kubernetes/manifests/kube-apiserver.yaml
</code></pre><p><img src="https://i.imgur.com/nC9N5G1.png" alt="" loading="lazy"></p><h4 id="21-Node" data-id="21-Node"><a class="anchor hidden-xs" href="#21-Node" title="21-Node"><span class="octicon octicon-link"></span></a><span>2.1 Node</span></h4><p><span>這是為了授權在每一個 node 上的 kubelet 所發出的 API request 所設計出來的，讓 kubelet 的 API request 可進行特定的權限控制</span></p><p><img src="https://i.imgur.com/ejswpYT.png" alt="" loading="lazy"></p><h4 id="22-ABAC" data-id="22-ABAC"><a class="anchor hidden-xs" href="#22-ABAC" title="22-ABAC"><span class="octicon octicon-link"></span></a><span>2.2 ABAC</span></h4><p><ins><span>Attribute-based access control</span></ins></p><p><span>此種方式就是在 master node 上保留一份 policy 文件，指定不同的使用者對於 resource 的存取權限，不彈性也不容易擴充，修改了 policy 文件之後還需要重新啟動 master node</span></p><p><img src="https://i.imgur.com/2v4K66G.png" alt="" loading="lazy"></p><h4 id="23-RBAC" data-id="23-RBAC"><a class="anchor hidden-xs" href="#23-RBAC" title="23-RBAC"><span class="octicon octicon-link"></span></a><span>2.3 RBAC</span></h4><p><ins><span>Role-Base Access Control</span></ins></p><p><span>RBAC API 中定義了 resource target，用來描述使用者以及 resource 之間的權限關係：</span></p><ul>
<li><span>Role：定義在特定 namespace 下的 resource 的存取權限</span></li>
<li><span>RoleBinding： 設定哪些使用者(or service account)與 role 綁定而擁有存取權限</span></li>
<li><span>ClusterRole：定義在整個 k8s cluster 下的 resource 的存取權限</span></li>
<li><span>ClusterRoleBinding：設定哪些使用者(or service account) 與 role 綁定而擁有存取權限</span></li>
</ul><p><img src="https://i.imgur.com/Jw38R2Y.png" alt="" loading="lazy"></p><p><img src="https://i.imgur.com/aB1ufdX.png" alt="" loading="lazy"></p><h4 id="24-Webhook" data-id="24-Webhook"><a class="anchor hidden-xs" href="#24-Webhook" title="24-Webhook"><span class="octicon octicon-link"></span></a><span>2.4 Webhook</span></h4><p><span>這個模式是管理者在外部提供 HTTPS 授權服務，並設定 API server 透過與外部服務互動的方式進行授權</span></p><p><img src="https://i.imgur.com/snqwQKD.png" alt="" loading="lazy"></p><h3 id="3-KubeConfig" data-id="3-KubeConfig"><a class="anchor hidden-xs" href="#3-KubeConfig" title="3-KubeConfig"><span class="octicon octicon-link"></span></a><span>3. KubeConfig</span></h3><p><a href="https://www.akiicat.com/2019/04/24/Kubernetes/setup-kubernetes-configuration/" target="_blank" rel="noopener"><span> </span><strong><span>Context</span></strong><span> </span></a></p><p><span>kubeconfig 是用來記錄 Cluster、User、預設 Namespace 以及 身份驗證機制的資料.</span><br>
<span>當我們使用 kubectl 這個工具在執行各種指令時，它就會透過 kubeconfig 來得知要到哪個集群，使用哪個使用者等資訊來完成這個命令。</span><br>
<span>在使用 kubeconfig 時，不需要像之前一樣建立 Object，而是可以直接建立檔案，然後讓kubectl 等工具來讀取這邊的設定</span></p><div class="alert alert-success">
<p><span>要存取某個 Kubernetes 的 cluster，必須先設定好 Kubernetes 的 context，context 裡面會描述要如何存取到你的 Kubernetes 的 cluster</span><br>
<span>在 Kubernetes 裡面，切換不同的 cluster 是以 context 為單位</span><br>
<span>一個 context 裡面必需要三個元件，分別是 User、Server、Certification.</span><br>
<span>這三個東西說起來也很直觀，有個使用者 (User) 必須要有憑證 (Certification) 才能連到某個 Cluster (Server)。</span></p>
</div><ul>
<li><span>底下是一個 Context 所包含的內容</span><br>
<img src="https://i.imgur.com/2TXQw1X.png" alt="" loading="lazy"></li>
</ul><h4 id="Config-yaml" data-id="Config-yaml"><a class="anchor hidden-xs" href="#Config-yaml" title="Config-yaml"><span class="octicon octicon-link"></span></a><span>Config yaml</span></h4><p><img src="https://i.imgur.com/A1B0YdH.png" alt="" loading="lazy"></p><p><span>設定檔會放置在 ~/.kube/config (mac or linux)，下面內容表示 kubectl 已連接到 kubernetes-admin</span></p><ul>
<li><span>列出目前 kubectl 的設定內容</span></li>
</ul><pre><code>kubectl config view --&gt; 取得設定檔
</code></pre><p><img src="https://i.imgur.com/6NIM1dS.png" alt="" loading="lazy"></p><p><span>可以分成區塊看:</span></p><ul>
<li><ins><span>Clusters</span></ins><br>
<span>我們可能會管理到很多集群，像是Production、Stagging、Development之類有著不同功能的集群，在這裡就可以記錄這些集群的資訊。</span></li>
<li><ins><span>Users</span></ins><br>
<span>我們在各個集群中可能都會擔任不同的使用者，像是我們自己建立的集群，可能自己就是 Admin。</span><br>
<span>而公司的集群，我們可能就沒有那麼大的權限。這邊就是把各個使用者列出來。</span></li>
<li><ins><span>Contexts</span></ins><br>
<span>Context 的意思就是上下文，這邊紀錄了會在哪些 Cluster 使用了哪些 User。</span></li>
</ul><hr><h4 id="常用-config-指令" data-id="常用-config-指令"><a class="anchor hidden-xs" href="#常用-config-指令" title="常用-config-指令"><span class="octicon octicon-link"></span></a><span>常用 config 指令</span></h4><pre><code>kubectl config view

# 如果加上 --kubeconfig，則可以指定要使用哪個 kubeconfig file
kubectl config view --kubeconfig=my-custom-config
</code></pre><p><span>管理多個 k8s cluster</span></p><pre><code># 改變 current-context 成 [NAME]
kubectl config use-context [NAME]

# 查詢有哪些 cluster 可以切換
kubectl config get-contexts

# 目前正在管理的 cluster
kubectl config current-context

# 取得 cluster 狀態
kubectl cluster-info

# 改變當前預設的 namespace
kubectl config set-context --current --namespace=NAMESPACE
</code></pre><ul>
<li><ins><span>kubectl cluster-info</span></ins><br>
<img src="https://i.imgur.com/Hp0ktrj.png" alt="" loading="lazy"></li>
</ul><h4 id="example1" data-id="example"><a class="anchor hidden-xs" href="#example1" title="example1"><span class="octicon octicon-link"></span></a><span>example</span></h4><p><img src="https://i.imgur.com/tg2aQAe.png" alt="" loading="lazy"></p><pre><code># To use that context, run the command
kubectl config --kubeconfig=/root/my-kube-config use-context research

# To know the current context, run the command
kubectl config --kubeconfig=/root/my-kube-config current-context
</code></pre><h3 id="4-Security-Context" data-id="4-Security-Context"><a class="anchor hidden-xs" href="#4-Security-Context" title="4-Security-Context"><span class="octicon octicon-link"></span></a><span>4. Security Context</span></h3><p><span>Pod 的安全策略，雖然 Pod 是受到 kubernetes 經過檢查確認合法才得以部署的，但是由於這些服務都會直接面向 User，若這些容器內本身的權限過高且遭受到攻擊，就會衍生出其他的安全性問題.</span><br>
<span>SecurityContext 就是用來解決這類問題的，它定義了 Pod 或 容器的特權和訪問控制設置。</span></p><div class="alert alert-warning">
<p><span>Capabilities are only supported at the container level and not at the POD level</span></p>
</div><p><span>SecurityContext 包括：</span></p><ul>
<li><ins><span>Discretionary Access Control</span></ins><br>
<span>訪問目標（如檔案）的權限基於User ID（UID）和 Group ID（GID）。</span></li>
<li><ins><span>Security Enhanced Linux (SELinux)</span></ins><br>
<span>為目標分配安全標籤</span></li>
<li><ins><span>Running as privileged or unprivileged</span></ins><br>
<span>以特權或非特權運行</span></li>
<li><ins><span>Linux Capabilities</span></ins><br>
<span>為某些process提供特權，但不是root的所有特權</span></li>
<li><ins><span>AppArmor</span></ins><br>
<span>使用程式配置文件來限制個別程式的功能。</span></li>
<li><ins><span>Seccomp</span></ins><br>
<span>過濾及篩選process的system call</span></li>
<li><ins><span>AllowPrivilegeEscalation</span></ins><br>
<span>控制process是否可以比其parent process獲得更多的特權。</span></li>
<li><ins><span>readOnlyRootFilesystem</span></ins><br>
<span>將容器的root file system mount 為 Read-Only</span></li>
</ul><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod" target="_blank" rel="noopener"><span>k8s-io Set the security context for a Pod</span></a><br>
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container" target="_blank" rel="noopener"><span>k8s-io Set capabilities for a Container </span></a></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
</code></pre><ul>
<li><ins><span>runAsUser: 1000</span></ins><br>
<span>對於 Pod 中的任何容器，所有 process 都已 User ID=1000運行</span></li>
<li><ins><span>runAsGroup: 3000</span></ins><br>
<span>Pod 的任何容器內的所有 process 指定 primary group ID 為3000。</span><br>
<span>指定 runAsGroup 時， User ID 1000 和 Group ID 3000 也將擁有所有創建的檔案。</span><br>
<span>(若省略此參數，K8s default的primary group ID是root(0))</span></li>
<li><ins><span>fsGroup: 2000</span></ins><br>
<span>容器的所有 process 也是 supplementary Group ID 2000 的一部分。</span><br>
<span>Volume /data/demo 及該 Vloume 中所有檔案的擁有者均為 Group ID 2000。</span></li>
</ul><h4 id="Example2" data-id="Example"><a class="anchor hidden-xs" href="#Example2" title="Example2"><span class="octicon octicon-link"></span></a><span>Example</span></h4><p><span>Create a new Pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time</span><br>
<span>The container should sleep for 4800 seconds</span></p><pre><code>apiVersion: v1
kind: Pod
metadata: 
    name: super-user-pod
spec: 
    containers:
    - image: busybox:1.28
      name: super-user-pod
      ## 加上securityContext參數
      securityContext:
        capabilities:
          add: ["SYS_TIME"] ## 允許設定SYS_TIME
      command: ["sleep"] ## container sleep 4800
      args: ["4800"]
    restartPolicy: Never
</code></pre><h3 id="5-Role" data-id="5-Role"><a class="anchor hidden-xs" href="#5-Role" title="5-Role"><span class="octicon octicon-link"></span></a><span>5. Role</span></h3><p><span>Role 即代表了 namespace 下的資源的權限，ClusterRole 則是 Cluster 層級資源的權限</span><br>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-example" target="_blank" rel="noopener"><span>Role example</span></a></p><pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
</code></pre><p><span>rules包含三個子欄位</span></p><ul>
<li><ins><span>apiGroups</span></ins><br>
<span>要使用的 API Group name。若此欄空白，則預設為 core API</span></li>
<li><ins><span>resources</span></ins><br>
<span>要對甚麼 resource 進行設定, 比如 pods、deployments、services、secrets 等等…</span></li>
<li><ins><span>verb</span></ins><br>
<span>允許對resources進行的操作, 比如 get、list、watch、create、delete、update 等等…</span></li>
</ul><pre><code>rules可以一次設定好幾個，以 YAML 陣列區隔即可
</code></pre><p><img src="https://i.imgur.com/H3UpDu8.png" alt="" loading="lazy"></p><h3 id="51-Binding" data-id="51-Binding"><a class="anchor hidden-xs" href="#51-Binding" title="51-Binding"><span class="octicon octicon-link"></span></a><span>5.1 Binding</span></h3><p><span>就是把定義好規則權限的 Role 绑定到指定的 Subject 上，進而賦予 Subject 相對應的一系列權限</span></p><p><span>kubernetes 中 Binding 分為兩種：</span></p><ol>
<li><ins><span>RoleBinding</span></ins><span>：绑定 Role/ClusterRole 到 Subject，生效在具體的 Namespace 範圍資源</span></li>
<li><ins><span>ClusterRoleBinding</span></ins><span> 绑定 ClusterRole 到 Subject，生效在 Cluster 範圍資源</span></li>
</ol><div class="alert alert-success">
<p><span>Kubernetes 中權限控制的範圍（Namespace/Cluster）是由 Binding 的類型决定的，而不是根據 Role 和 ClusterRole 决定的.</span></p>
</div><h4 id="Role--RoleBinding-的组合" data-id="Role--RoleBinding-的组合"><a class="anchor hidden-xs" href="#Role--RoleBinding-的组合" title="Role--RoleBinding-的组合"><span class="octicon octicon-link"></span></a><span>Role + RoleBinding 的组合</span></h4><p><span>這種方式是比較直觀的，Kubernetes 中的 Role 應該理解為 Namespace Role，它是定義在 Namespace 下面的，因此如果要實現 Namespace 範圍内的 Subject 與 Role 的绑定，無非就是通過配置一個 RoleBinding 把兩者聯繫起來.</span></p><p><img src="https://i.imgur.com/pJLWUTc.png" alt="" loading="lazy"></p><ul>
<li><span>配置範例</span></li>
</ul><pre><code>kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-reader-binding
  namespace: ns-a
subjects:
- kind: User
  name: mcleezs
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
</code></pre><h4 id="ClusterRole--RoleBinding-的组合" data-id="ClusterRole--RoleBinding-的组合"><a class="anchor hidden-xs" href="#ClusterRole--RoleBinding-的组合" title="ClusterRole--RoleBinding-的组合"><span class="octicon octicon-link"></span></a><span>ClusterRole + RoleBinding 的组合</span></h4><p><span>ClusterRole，它的本質是一個定義在全局的角色，可以作用在 Namespace 下，可以被多個 Namespace 重複使用，也可以作用在 Cluster 下。</span></p><p><span>我們現在需要實現 Namespace 範圍内绑定 Subject &amp; Role，就是將 ClusterRole 的權限範圍控制在 Namespace 層面，因此採用 RoleBinding 將 Subject 與 ClusterRole 即可實現</span></p><p><img src="https://i.imgur.com/pAfEvBN.png" alt="" loading="lazy"></p><ul>
<li><span>配置範例</span></li>
</ul><pre><code>kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-reader-binding
  namespace: ns-a
subjects:
- kind: User
  name: mcleezs
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
</code></pre><hr><h4 id="Example3" data-id="Example"><a class="anchor hidden-xs" href="#Example3" title="Example3"><span class="octicon octicon-link"></span></a><span>Example</span></h4><p><span>kube-proxy 這個 role 可以看到 configmaps 這個 object by the name [kube-proxy]</span></p><p><img src="https://i.imgur.com/YpH1ef5.png" alt="" loading="lazy"></p><p><span>想確認你可以對特定物件做甚麼操作，可以透過以下指令</span></p><pre><code>kubectl auth can-i list pod -A 
kubectl auth can-i delete pod
# 若你是管理者，你想檢查 dev-user 可否對特定物件做甚麼操作
kubectl auth can-i  list pod --as dev-user 
</code></pre><p><img src="https://i.imgur.com/I8kANen.png" alt="" loading="lazy"></p><h4 id="Question-1" data-id="Question-1"><a class="anchor hidden-xs" href="#Question-1" title="Question-1"><span class="octicon octicon-link"></span></a><span>Question 1</span></h4><p><span>接續上面 example 內容, 因為 dev-user 無法 list / delete pods, 所以這題要求創建必要的 role 根 role binding 使得 dev-user 能夠 list / delete pods</span><br>
<img src="https://i.imgur.com/TbPHWvh.png" alt="" loading="lazy"></p><pre><code>k create role developer --verb=list,create,delete --resource=pods
k describe role developer
k create rolebinding dev-user-binding --role=developer --user=dev-user
k describe rolebindings dev-user-binding
</code></pre><p><img src="https://i.imgur.com/dNLmr6D.png" alt="" loading="lazy"></p><h4 id="Question-2" data-id="Question-2"><a class="anchor hidden-xs" href="#Question-2" title="Question-2"><span class="octicon octicon-link"></span></a><span>Question 2</span></h4><p><span>需要增加角色權限 (原本為 Forbidden)</span></p><p><img src="https://i.imgur.com/hbA34WB.png" alt="" loading="lazy"></p><p><span>我們用 describe 去看，發現 developer role 可以看得 Resource Names 是 blue-app 而非題目想要我們看的資源 dark-blue-app, 所以這就是問題所在</span></p><p><img src="https://i.imgur.com/AZpdJkL.png" alt="" loading="lazy"></p><pre><code>k edit role developer -n blue
k --as dev-user get pods dark-blue-app -n blue
</code></pre><p><span>edit 進去把 rules 底下的 resourceNames 改成 dark-blue-app 即可</span><br>
<img src="https://i.imgur.com/IyV9sEG.png" alt="" loading="lazy"></p><h4 id="Question-3" data-id="Question-3"><a class="anchor hidden-xs" href="#Question-3" title="Question-3"><span class="octicon octicon-link"></span></a><span>Question 3</span></h4><p><span>Add a new rule in the existing role developer to grant the dev-user permissions to create deployments in the blue namespace.</span></p><p><span>dev-user 沒有權限在 namespace=blue 底下 create deployment, 所以這題要求改變成能夠 create deployment.</span></p><pre><code>k create deployment nginx --image=nginx -n blue --as dev-user
--&gt; error: failed to create deployment: deployments.apps is forbidden: User "dev-user" 
cannot create resource "deployments" in API group "apps" in the namespace "blue"
k edit role developer -n blue
</code></pre><p><span>增加紅圈處的部分去給 deployment.apps 權限</span></p><p><img src="https://i.imgur.com/0dYCmdO.png" alt="" loading="lazy"></p><h3 id="52-CertificateSigningRequest" data-id="52-CertificateSigningRequest"><a class="anchor hidden-xs" href="#52-CertificateSigningRequest" title="52-CertificateSigningRequest"><span class="octicon octicon-link"></span></a><span>5.2 CertificateSigningRequest</span></h3><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest" target="_blank" rel="noopener"><span>k8s-io 官網範例</span></a></p><pre><code>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: myuser
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZVzVuWld4aE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTByczhJTHRHdTYxakx2dHhWTTJSVlRWMDNHWlJTWWw0dWluVWo4RElaWjBOCnR2MUZtRVFSd3VoaUZsOFEzcWl0Qm0wMUFSMkNJVXBGd2ZzSjZ4MXF3ckJzVkhZbGlBNVhwRVpZM3ExcGswSDQKM3Z3aGJlK1o2MVNrVHF5SVBYUUwrTWM5T1Nsbm0xb0R2N0NtSkZNMUlMRVI3QTVGZnZKOEdFRjJ6dHBoaUlFMwpub1dtdHNZb3JuT2wzc2lHQ2ZGZzR4Zmd4eW8ybmlneFNVekl1bXNnVm9PM2ttT0x1RVF6cXpkakJ3TFJXbWlECklmMXBMWnoyalVnald4UkhCM1gyWnVVV1d1T09PZnpXM01LaE8ybHEvZi9DdS8wYk83c0x0MCt3U2ZMSU91TFcKcW90blZtRmxMMytqTy82WDNDKzBERHk5aUtwbXJjVDBnWGZLemE1dHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR05WdmVIOGR4ZzNvK21VeVRkbmFjVmQ1N24zSkExdnZEU1JWREkyQTZ1eXN3ZFp1L1BVCkkwZXpZWFV0RVNnSk1IRmQycVVNMjNuNVJsSXJ3R0xuUXFISUh5VStWWHhsdnZsRnpNOVpEWllSTmU3QlJvYXgKQVlEdUI5STZXT3FYbkFvczFqRmxNUG5NbFpqdU5kSGxpT1BjTU1oNndLaTZzZFhpVStHYTJ2RUVLY01jSVUyRgpvU2djUWdMYTk0aEpacGk3ZnNMdm1OQUxoT045UHdNMGM1dVJVejV4T0dGMUtCbWRSeEgvbUNOS2JKYjFRQm1HCkkwYitEUEdaTktXTU0xMzhIQXdoV0tkNjVoVHdYOWl4V3ZHMkh4TG1WQzg0L1BHT0tWQW9FNkpsYWFHdTlQVmkKdjlOSjVaZlZrcXdCd0hKbzZXdk9xVlA3SVFjZmg3d0drWm89Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
EOF
</code></pre><hr><h3 id="6-Service-Account" data-id="6-Service-Account"><a class="anchor hidden-xs" href="#6-Service-Account" title="6-Service-Account"><span class="octicon octicon-link"></span></a><span>6. Service Account</span></h3><p><span>Kubernetes 的帳號有兩種類型，分別為：</span></p><ul>
<li>
<p><ins><span>使用者帳戶 (Normal Users)</span></ins><br>
<span>任何人想要連接並存取 Kubernetes 叢集，都需要先建立一個 “使用者帳戶” 並將憑證資訊提供給用戶端 (如: kubectl)，以便通過 Kubernetes 的 API server 的認證 (Authentication)</span><br>
<span>這個名字應該稱為 User Accounts 會比較好理解，但是 Kubernetes 官網稱一般使用者 (Normal Users)</span></p>
</li>
<li>
<p><ins><span>服務帳戶 (Service Accounts)</span></ins><br>
<span>任何跑在 Pod 裡面的 container(容器) 想要存取 Kubernetes 的 API 伺服器 (kube-apiserver)，就需要先有一個 “服務帳戶” 綁定在 Pod 身上，然後以便通過 Kubernetes 的 API 伺服器的身份認證 (Authentication)</span></p>
</li>
</ul><pre><code>kubectl create serviceaccount dashboard-sa
kubectl get serviceaccount
kubectl describe serviceaccount dashboard-sa
</code></pre><h4 id="Pod-amp-ServiceAccount-by-command" data-id="Pod-amp-ServiceAccount-by-command"><a class="anchor hidden-xs" href="#Pod-amp-ServiceAccount-by-command" title="Pod-amp-ServiceAccount-by-command"><span class="octicon octicon-link"></span></a><span>Pod &amp; ServiceAccount by command</span></h4><p><span>v1.24開始，建立ServiceAccount 不會自動建立 token，所以我們需要自己建立 token 並會在約1小時後到期失效</span></p><pre><code>kubectl create token dashboard-sa
</code></pre><p><img src="https://i.imgur.com/rm1iI8R.png" alt="" loading="lazy"></p><h4 id="Pod-amp-ServiceAccount-by-yaml" data-id="Pod-amp-ServiceAccount-by-yaml"><a class="anchor hidden-xs" href="#Pod-amp-ServiceAccount-by-yaml" title="Pod-amp-ServiceAccount-by-yaml"><span class="octicon octicon-link"></span></a><span>Pod &amp; ServiceAccount by yaml</span></h4><p><span>當創建 pod 的時候，如果没有指定一個 service account，系统會自動的在與該 pod 相同的 namespace 下幫其指派一個 default service account</span><br>
<span>如果 describe pod 的原始 json 或 yaml 訊息（例如使用 kubectl get pods/podename -o yaml 命令），我們可以看到 spec.serviceAccountName 已经被設置為 automatically</span></p><pre><code>k get pods &lt;pod-name&gt; -o yaml
</code></pre><p><img src="https://i.imgur.com/h2LEhMA.png" alt="" loading="lazy"></p><p><span>這樣就可以 在 pod 中使用自動掛載的 service account 憑證來訪問 API.</span></p><ul>
<li><span>You can opt out of automounting API credentials on </span><ins><span>/var/run/secrets/kubernetes.io/serviceaccount/token</span></ins><span> for a service account by setting automountServiceAccountToken: false on the ServiceAccount</span></li>
</ul><p><span>選擇取消 service account 自訂掛載 API 憑證，只需在 service account 中設置 </span><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener"><span>automountServiceAccountToken: false</span></a></p><pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
...
</code></pre><ul>
<li><span>You can also opt out of automounting API credentials for a particular Pod:</span></li>
</ul><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
</code></pre><h4 id="Question-1---deployment-edit-serviceaccount" data-id="Question-1---deployment-edit-serviceaccount"><a class="anchor hidden-xs" href="#Question-1---deployment-edit-serviceaccount" title="Question-1---deployment-edit-serviceaccount"><span class="octicon octicon-link"></span></a><span>Question 1 - deployment edit serviceaccount</span></h4><p><a href="https://kodekloud.com/topic/practice-test-service-accounts-2/" target="_blank" rel="noopener"><span>kodekloud: practice-test-service-account</span></a></p><p><span>However currently, the default service account is mounted.</span><br>
<span>Update the deployment to use the newly created ServiceAccount</span></p><pre><code>k edit deployments.apps web-dashboard
</code></pre><p><img src="https://i.imgur.com/Z1yqKW8.png" alt="" loading="lazy"></p><h3 id="7-Image-Sercurity" data-id="7-Image-Sercurity"><a class="anchor hidden-xs" href="#7-Image-Sercurity" title="7-Image-Sercurity"><span class="octicon octicon-link"></span></a><span>7. Image Sercurity</span></h3><p><span>如果今天要從本地端去抓取一個 private container registry，我們第一件要做的事情就是 docekr login.</span><br>
<span>對於 Kubernetes 來說，其會使用 secret 的特殊型態 docker-registry 作為登入任何 private container registry 的帳號密碼來源.</span></p><p><ins><span>這邊有兩種方式可以使用</span></ins></p><ul>
<li><span>第一種是先透過 docker login 登入，之後將登入後的設定檔案送給 Kubernetes secret 物件</span></li>
<li><span>第二種則是創建 Kubernetes secret 時使用明碼的帳號密碼</span></li>
</ul><pre><code>kubectl create secret
k create secret docker-registry --&gt; 從 private harbor 拉 image
</code></pre><p><img src="https://i.imgur.com/3PkNc8g.png" alt="" loading="lazy"></p><h4 id="Commandline-create-secret-object" data-id="Commandline-create-secret-object"><a class="anchor hidden-xs" href="#Commandline-create-secret-object" title="Commandline-create-secret-object"><span class="octicon octicon-link"></span></a><span>Commandline create secret object</span></h4><p><img src="https://i.imgur.com/lB2YhtN.png" alt="" loading="lazy"></p><pre><code>kubectl create secret docker-registry -h

k create secret docker-registry private-reg-cred \ 
--docker-server=myprivateregistry.com:5000 --docker-username=dock_user \ 
--docker-password=dock_password --docker-email=dock_user@myprivateregistry.com
</code></pre><p><img src="https://i.imgur.com/nI0u0P8.png" alt="" loading="lazy"></p><h4 id="Create-a-Pod-that-uses-your-Secret" data-id="Create-a-Pod-that-uses-your-Secret"><a class="anchor hidden-xs" href="#Create-a-Pod-that-uses-your-Secret" title="Create-a-Pod-that-uses-your-Secret"><span class="octicon octicon-link"></span></a><span>Create a Pod that uses your Secret</span></h4><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret" target="_blank" rel="noopener"><span>k8s.io</span></a></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagePullSecrets:
  - name: regcred
</code></pre><h3 id="8-Network" data-id="8-Network"><a class="anchor hidden-xs" href="#8-Network" title="8-Network"><span class="octicon octicon-link"></span></a><span>8. Network</span></h3><p><span>網路策略(Network Policy) 是用在控制Pod之間如何溝通以及如何與其他網路溝通的規範, 目的在幫K8s 實現更精細的流量控制以及租戶隔離機制.</span><br>
<span>K8s 提供一個 NetworkPolicy 供使用, 有效範圍是整個Namespace.</span></p><p><span>NetworkPolicy 會使用標籤選擇器定義一組Pod作為控制對象, 管控入站流量的是Ingress, 負責出站流量的是Egress, 兩個可以共用, 負責規範生效範圍的是 spec.policyType</span></p><p><span>Pod 預設可以接收任何來源的流量, 也可以向外部發出期望的所有流量, 一旦Nameapace 中有NetworkPolicy規範Pod, Pod就會依照NetworkPolicy的規範拒絕請求, 並且若是在spec中定義了沒有規則的Ingress或Egress就會造成拒絕相關的一切流量。</span></p><p><img src="https://i.imgur.com/vpBuGpN.png" alt="" loading="lazy"></p><hr><h4 id="Ingress-入站流量管控" data-id="Ingress-入站流量管控"><a class="anchor hidden-xs" href="#Ingress-入站流量管控" title="Ingress-入站流量管控"><span class="octicon octicon-link"></span></a><span>Ingress 入站流量管控</span></h4><div class="alert alert-warning">
<p><ins><span>Ingress 字段是一個列表, 主要有兩個字段組成:</span></ins></p>
<ol>
<li><span>from &lt;[]Object&gt; : 可訪問的Pod列表, 如果設定多項則判斷邏輯為"聯集", 是一個白名單的概念.</span></li>
<li><span>ports &lt;[]Object&gt;: 可訪問的Pod上面的允許Port列表, 有設定就就是白名單.</span></li>
</ol>
</div><p><img src="https://i.imgur.com/Y6aK1A2.png" alt="" loading="lazy"></p><ul>
<li><ins><span>拒絕所有流量配置 YAML</span></ins><br>
<span>podSelector : pod 標籤選擇器, 給空值代表全選</span><br>
<span>port : 未定義時匹配所有端口</span></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
spec:
  podSelector:{}
  policyType:["Ingress"]
</code></pre><ul>
<li><ins><span>接收所有流量 YAML</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector:{}
  policyType:["Ingress"]
  ingress:
  - {}
</code></pre><h4 id="Egress-出站流量管控" data-id="Egress-出站流量管控"><a class="anchor hidden-xs" href="#Egress-出站流量管控" title="Egress-出站流量管控"><span class="octicon octicon-link"></span></a><span>Egress 出站流量管控</span></h4><p><img src="https://i.imgur.com/C59SpVx.png" alt="" loading="lazy"></p><div class="alert alert-warning">
<p><ins><span>Egress 字段是一個列表, 主要有兩個字段組成:</span></ins></p>
<ol>
<li><span>to &lt;[]Object&gt;: 可訪問的Pod列表, 如果設定多項則判斷邏輯為 “聯集”, 是一個白名單的概念.</span></li>
<li><span>ports &lt;[]Object&gt; : 可訪問的Pod上面的允許Port列表, 有設定就就是白名單.</span></li>
</ol>
</div><ul>
<li><ins><span>拒絕所有流量配置 YAML</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
spec:
  podSelector:{}
  policyType:["Egress"]
</code></pre><ul>
<li><ins><span>接收所有流量 YAML配置</span></ins></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector:{}
  policyType:["Egress"]
  egress:
  - {}
</code></pre><h3 id="9-Network-Policies" data-id="9-Network-Policies"><a class="anchor hidden-xs" href="#9-Network-Policies" title="9-Network-Policies"><span class="octicon octicon-link"></span></a><span>9. Network Policies</span></h3><pre><code>k get networkpolicie
k get netpol
k describe netpol &lt;networkpolicie-name&gt;
</code></pre><p><img src="https://i.imgur.com/6V34uN0.png" alt="" loading="lazy"></p><p><span>上面這個設定代表只允許 pod=interval 透過 8080 port 訪問 pod=payroll</span></p><p><img src="https://i.imgur.com/aM6eZdQ.png" alt="" loading="lazy"></p><h4 id="Create-a-network-policy" data-id="Create-a-network-policy"><a class="anchor hidden-xs" href="#Create-a-network-policy" title="Create-a-network-policy"><span class="octicon octicon-link"></span></a><ins><span>Create a network policy</span></ins></h4><p><img src="https://i.imgur.com/wzsw9T6.png" alt="" loading="lazy"></p><pre><code>k create -f internal.yaml
</code></pre><p><img src="https://i.imgur.com/Xkt1qWu.png" alt="" loading="lazy"></p><pre><code># internal.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              role: payroll
      ports:
        - protocol: TCP
          port: 8080
    - to:
        - podSelector:
            matchLabels:
              role: mysql
      ports:
        - protocol: TCP
          port: 3306
</code></pre><h2 id="Storage" data-id="Storage"><a class="anchor hidden-xs" href="#Storage" title="Storage"><span class="octicon octicon-link"></span></a><span>Storage</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section8-Storage/Kubernetes%2B-CKA-%2B0700%2B-%2BStorage.pdf" target="_blank" rel="noopener"><span>Kubernetes-Storage.pdf</span></a></p><p><span>要將容器中的檔案留存在主機上，Docker 有三種作法：</span></p><ol>
<li><ins><span>volume</span></ins><br>
<span>Volume 存放在主機檔案系統中由 Docker 管理的地方，在 Linux 作業系統是 /var/lib/docker/volumes/ 此路徑。非 Docker 的行程不應該修改檔案系統中的這一部分.</span><br>
<span>要在 Docker 中留存資料，volumes 是最好的方法</span></li>
<li><ins><span>bind mount</span></ins><br>
<span>可存放在主機檔案系統中的任何地方，非 Docker 行程或 Docker 容器可隨時修改。</span></li>
<li><ins><span>tmpfs mount</span></ins><span> (only 在 Linux 作業系統上的 Docker)</span><br>
<span>只存放在主機的記憶體中，不會寫入主機的檔案系統</span></li>
</ol><ul>
<li><span>這三種方式的差異可用下圖表示</span></li>
</ul><p><img src="https://i.imgur.com/JLhgr7g.png" alt="" loading="lazy"></p><hr><ul>
<li><ins><span>Volume driver &amp; Storage driver</span></ins></li>
</ul><p><img src="https://i.imgur.com/VJ6Uhts.png" alt="" loading="lazy"></p><h3 id="1-Volume" data-id="1-Volume"><a class="anchor hidden-xs" href="#1-Volume" title="1-Volume"><span class="octicon octicon-link"></span></a><span>1. Volume</span></h3><div class="alert alert-success">
<p><span>Volume 可以是一個 Node 或是 一個雲端儲存平台，是 K8s 實現儲存資料的方.</span><br>
<span>透過將 Volume mount 到Pod上，就可以實現儲存Pod的資料</span></p>
</div><h3 id="11-Volume-Type" data-id="11-Volume-Type"><a class="anchor hidden-xs" href="#11-Volume-Type" title="11-Volume-Type"><span class="octicon octicon-link"></span></a><span>1.1 Volume Type</span></h3><p><span>multinode cluster 不建議使用 hostPath, 因為每個 node 中的路徑不一定會相同</span><br>
<img src="https://i.imgur.com/sMyQSFE.png" alt="" loading="lazy"></p><p><span>K8S 提供的 Vloume類型非常多，下面舉兩個為例子:</span></p><h4 id="1-emptyDir" data-id="1-emptyDir"><a class="anchor hidden-xs" href="#1-emptyDir" title="1-emptyDir"><span class="octicon octicon-link"></span></a><ins><span>1. emptyDir</span></ins></h4><p><span>當 Pod 調度某給Node時，首先創建 emptyDir Volume，並且該Pod在該Node上運行時就存在。</span><br>
<span>顧名思義，它最初是空的。Pod中的容器都可以在emptyDir Volume中讀取和寫入相同的文件，儘管該Volume可以安裝在每個 Container 中的相同或不同路徑上。</span><br>
<span>當 Pod 從 Node 中被刪除時，emptyDir 中的數據也將被刪除。</span><br>
<span>emptyDir 的用途包含：</span></p><ol>
<li><span>臨時空間，例如用於某些應用程式執行階段所需的臨時目錄，且無須永久保存</span></li>
<li><span>長時間工作的中間過程 CheckPoint 的臨時儲存目錄</span></li>
<li><span>多容器共用目錄</span></li>
</ol><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
</code></pre><h4 id="2-hostPath" data-id="2-hostPath"><a class="anchor hidden-xs" href="#2-hostPath" title="2-hostPath"><span class="octicon octicon-link"></span></a><ins><span>2. hostPath</span></ins></h4><p><span>hostPath Volume 將 Node檔案系統中的檔案或目錄 mount到 Pod 中。</span><br>
<span>hostPath 的生命週期與 Node 相同，並不會隨著 Pod消失而消失。</span><br>
<span>hostPath的用途包含：</span></p><ol>
<li><span>容器應用程式產生的紀錄檔需要永久儲存時，可以使用Node的檔案系統進行儲存</span></li>
<li><span>需要存取Node上Docker內部資料結構的容器應用時，可以透過定義 hostPath 為 Node 的 /var/lib/docker 目錄，使容器可以直接存取 Docker的檔案系統</span></li>
</ol><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
</code></pre><div class="alert alert-success">
<p><span>帶來的問題：發布環境侷限不能進行遷移 → 要遷移就要修改文件</span></p>
</div><h3 id="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" data-id="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC"><a class="anchor hidden-xs" href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC"><span class="octicon octicon-link"></span></a><span>2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</span></h3><p><span>PersistentVolume 是存放資源的地方,簡單想像的話就是個 Disk 空間,PersistentVolume 分為兩種:</span></p><ol>
<li><span>靜態：手動建立 PersistentVolume 稱為「靜態」綁定</span></li>
<li><span>動態：PVC在批配 PV時，若不符合規則會透過 StorageClass 自動建立新的PV，此時PV我們會稱為「動態」綁定，並且繼承 StorageClass 規定的回收政策</span></li>
</ol><p><img src="https://i.imgur.com/0dtZy2s.png" alt="" loading="lazy"></p><p><a href="https://medium.com/k8s%E7%AD%86%E8%A8%98/kubernetes-k8s-pv-pvc-%E5%84%B2%E5%AD%98%E5%A4%A7%E5%B0%8F%E4%BA%8B%E4%BA%A4%E7%B5%A6pv-pvc%E7%AE%A1%E7%90%86-4d412b8bafb5" target="_blank" rel="noopener"><span>解決</span></a><span> Pod 與具體存儲 Volume 耦合 → 引入了 PV/ PVC 進行解藕</span></p><hr><pre><code>kubectl get persistentvolumeclaim
kubectl delete persistentvolumeclaim &lt;my-claim&gt; --&gt; 刪除 persistentvolumeclaim
</code></pre><p><img src="https://i.imgur.com/rfXbxhu.png" alt="" loading="lazy"></p><hr><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"><span>PersistentVolumeClaim</span></a><span> 可以透過我們設定好的 Storage Class 的模板，創建出我們所需要的資源.</span></p><div class="alert alert-success">
<p><span>每個 PV 都有自己的一組 accessModes (訪問模式)，用於描述該特定 PV 的功能.</span><br>
<span>分為三種:</span></p>
<ol>
<li><span>ReadWriteOnce：只可以掛載在同一個 Node 上提供讀寫功能.</span></li>
<li><span>ReadOnlyMany ：可以在多個 Node 上提供讀取功能.</span></li>
<li><span>ReadWriteMany：可以在多個 Node 上提供讀寫功能.</span></li>
</ol>
</div><p><span>當 PV 和 PVC 創建完成後，K8s 會根據 PVC 的 request 和 PV 的 properties，將最合適的 PV 及 PVC bind 起來，</span><strong><span>每個 PVC 只會 bound 一個 PV</span></strong><br>
<span>在 binding 過程中，K8s 會為 PVC 挑選最合適的 PV，除了容量外，也會考量 accessModes、vloumeModes 和 storage class 等等…</span></p><ul>
<li><span>PVC 該如何與 PV 進行綁定:</span>
<ul>
<li><span>透過 Label 標籤，找到相同 PV. (客製 binding 到特別的 PV)</span></li>
<li><span>透過 storageClassName 名稱，找到相同 PV.</span></li>
</ul>
</li>
</ul><h4 id="Create-Persistent-Volumes" data-id="Create-Persistent-Volumes"><a class="anchor hidden-xs" href="#Create-Persistent-Volumes" title="Create-Persistent-Volumes"><span class="octicon octicon-link"></span></a><span>Create Persistent Volumes</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes" target="_blank" rel="noopener"><span>k8s io</span></a></p><ul>
<li><ins><span>capacity</span></ins><span>: 定義這個 PV 的 storage 大小</span></li>
</ul><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
</code></pre><div class="alert alert-info">
<p><span>PV 如果處於 Available Status，代表目前沒有 PVC 存在，或是沒有合適的 PVC 可以 bind 此 PV，那我們就需要自己建一個</span></p>
</div><p><span>左邊是題目, 右邊是答案</span><br>
<img src="https://i.imgur.com/L9kkLox.png" alt="" loading="lazy"></p><h4 id="Create-Persistent-Volumes-Claim" data-id="Create-Persistent-Volumes-Claim"><a class="anchor hidden-xs" href="#Create-Persistent-Volumes-Claim" title="Create-Persistent-Volumes-Claim"><span class="octicon octicon-link"></span></a><span>Create Persistent Volumes Claim</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"><span>k8s io</span></a></p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
</code></pre><div class="alert alert-info">
<p><span>當創建 PVC 後，可能會看見它處於 Pending Status，這是因為沒有找到合適配對的 PV.</span><br>
<span>如果有配對到合適的 PV 後，K8s 會自動將兩者 binding 起來，就可以看見 PVC 是 Bound Status.</span></p>
</div><p><img src="https://i.imgur.com/H3jaJHB.png" alt="" loading="lazy"></p><h3 id="3-Storage-Class" data-id="3-Storage-Class"><a class="anchor hidden-xs" href="#3-Storage-Class" title="3-Storage-Class"><span class="octicon octicon-link"></span></a><span>3. Storage Class</span></h3><p><span>透過 Kubernetes 提供的 </span><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" target="_blank" rel="noopener"><span>Storage Class</span></a><span> 元件，我們可以依據需求，根據 Volumes 的提供者 (provisioner)、類型 (type)、所在地 (Region)，以及回收政策 (reclaimPolicy) 去定義不同的 Storage Class.</span></p><p><img src="https://i.imgur.com/DtbBII2.png" alt="" loading="lazy"></p><p><span>當用戶在建立 Pod 服務使用到 PVC (PersistentVolumeClaim)，這時會自動找一個符合的 PV (PersistentVolume)進行批配，</span><br>
<span>若有批配到就直接進行綁定 (此時表示與 PV 進行「靜態」批配)</span><br>
<span>但是如果沒有符合的 PV，則會透過 StorageClass 建立一個新的 PV 再和 PVC 綁定. (此時表示與PV進行「動態」批配)</span></p><p><span>系統管理人員負責建置 PV ，而開發人員則是負責建立 PVC 與 Storage Class，並交由 PVC 自動尋找合適的 PV進行綁定，或者透過StorageClass建立一個新的PV再和PVC綁定</span></p><p><img src="https://i.imgur.com/JCDMq5R.jpg" alt="" loading="lazy"></p><pre><code>kubectl get sc --&gt; 看 Storage Class
</code></pre><h4 id="Create-Storage-Class" data-id="Create-Storage-Class"><a class="anchor hidden-xs" href="#Create-Storage-Class" title="Create-Storage-Class"><span class="octicon octicon-link"></span></a><span>Create Storage Class</span></h4><p><span>先從官網拉基本 sample 下來再去改 </span><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource" target="_blank" rel="noopener"><span>k8s io</span></a></p><pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
</code></pre><p><img src="https://i.imgur.com/zyHx54p.png" alt="" loading="lazy"></p><h2 id="Troubleshooting" data-id="Troubleshooting"><a class="anchor hidden-xs" href="#Troubleshooting" title="Troubleshooting"><span class="octicon octicon-link"></span></a><span>Troubleshooting</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/Udemy/Section13-Troubleshooting/Kubernetes-CKA-1000-Troubleshooting.pdf" target="_blank" rel="noopener"><span>Troubleshooting.pdf</span></a></p><p><span>這類考題通常會提供一個 K8s 集群環境，在環境中有一些 bug，例如 Pod 無法新增、kube-apiserver 故障或路徑問題等等，要求你解決這些問題.</span></p><p><a href="https://ithelp.ithome.com.tw/articles/10246043" target="_blank" rel="noopener"><span>Ref</span></a><br>
<ins><strong><span>Trobleshooting方法</span></strong></ins><br>
<span>為了追蹤和找出 K8s 集群中執行的容器應用出現的問題，經常透過以下這些查錯方法：</span></p><ol>
<li><span>檢視 K8s 目前執行的階段資訊，特別是與物件連接的Event事件.</span><br>
<span>這些事件紀錄了相關主題、發生時間、最近發生時間、發生次數及事件原因等，對於Trobleshooting很有幫助.</span><br>
<span>此外，透過檢視物件的執行階段資料，我們還可以發現參數錯誤、連結錯誤、狀態例外等問題.</span></li>
<li><span>對於服務、容器方面的問題，可能需要深入容器內部進行診斷，此時可以透過檢視容器的執行記錄檔來找出問題</span></li>
<li><span>對於某些複雜問題，例如 Pod 調度排程這類的問題，涉及到整個集群的節點，因此可能需要查找節點內的服務紀錄檔來debug.</span><br>
<span>例如蒐集 Control Plane 上的kube-apiserver、kube-schedule、kube-controler-manager，Node 上的 kubelet 及 kube-proxy 等等物件的log.</span></li>
</ol><h3 id="常見問題" data-id="常見問題"><a class="anchor hidden-xs" href="#常見問題" title="常見問題"><span class="octicon octicon-link"></span></a><span>常見問題</span></h3><ul>
<li>
<p><ins><span>Control Plane Failure</span></ins><br>
<span>這類問題包含 Pod 無法調度、認證沒通過，找不到目標檔案等等。建議解決方法可透過：</span><br>
<span>檢查 kube-system的物件</span><br>
<span>檢查 static pod路徑</span><br>
<span>要如何確認 Pod 運行在哪個Node上，可以透過kubectl get po -o wide命令，或看 Pod 名子，名子後面有接 master 的，通常代表運行在Control Plane上，可從Control Plane下手</span></p>
</li>
<li>
<p><ins><span>Pod 處於 Pending STATUS</span></ins><br>
<span>可能是 image不存在，檢查 image 的 Server (如Docker Hub)是否正常運行，或是網路狀況等等</span></p>
</li>
<li>
<p><ins><span>Service</span></ins><br>
<span>通常是 Service 的 Port mapping沒設定好，或是 Service 沒選取到Pod，可以檢查 Pod 的 label 和 Service 的 Selector</span></p>
</li>
<li>
<p><ins><span>NetworkPolicy</span></ins><br>
<span>檢查 NetworkPolicy 的 ingress和egress IP是否設定正確</span></p>
</li>
</ul><h3 id="Application-Failure" data-id="Application-Failure"><a class="anchor hidden-xs" href="#Application-Failure" title="Application-Failure"><span class="octicon octicon-link"></span></a><span>Application Failure</span></h3><ul>
<li><ins><span>Check ControlPlane Service</span></ins></li>
</ul><pre><code>service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
service kubelet status
service kube-proxy status
</code></pre><ul>
<li><ins><span>Check Service Logs</span></ins></li>
</ul><pre><code>kubectl logs kube-apiserver-master -n kube-system
sudo journalctl -u kube-apiserver
</code></pre><h4 id="Task-1--" data-id="Task-1--"><a class="anchor hidden-xs" href="#Task-1--" title="Task-1--"><span class="octicon octicon-link"></span></a><span>Task 1 -</span></h4><p><span>應用程式 port 可以通，但是 mysql-service 跟 webapp-mysql 連接上可能有些問題</span></p><p><img src="https://i.imgur.com/QUVmZ0c.png" alt="" loading="lazy"></p><pre><code>kubectl config set-context --current --namespace=alpha
curl http://localhost:30081
</code></pre><p><span>curl 打 30081 port 從 error message 出發</span></p><p><img src="https://i.imgur.com/Au9yoR1.png" alt="" loading="lazy"></p><pre><code>k describe deployments
k get svc
k edit svc mysql
k delete svc mysql
k create -f xxxx.yaml
</code></pre><p><span>發現 deployment 的 webapp-mysql container 跟 k8s 上的 service name 對不起來，那這就是錯誤所在, edit svc 內容的名稱後重啟即可</span></p><p><img src="https://i.imgur.com/FysbTRk.png" alt="" loading="lazy"></p><h3 id="Control-Plane-Failure" data-id="Control-Plane-Failure"><a class="anchor hidden-xs" href="#Control-Plane-Failure" title="Control-Plane-Failure"><span class="octicon octicon-link"></span></a><span>Control Plane Failure</span></h3><h4 id="Task-1---deploy-check" data-id="Task-1---deploy-check"><a class="anchor hidden-xs" href="#Task-1---deploy-check" title="Task-1---deploy-check"><span class="octicon octicon-link"></span></a><span>Task 1 - deploy check</span></h4><hr><p><span>The cluster is broken. We tried deploying an application but it’s not working. Troubleshoot and fix the issue.</span><br>
<span>Start looking at the deployments.</span></p><hr><p><img src="https://i.imgur.com/XRRgIoM.png" alt="" loading="lazy"></p><p><span>檢查 pod 的時候，發現 kube-system 底下有個 kube-scheduler-controlplane 的 pod 壞掉, 所以再深入去看他就發現是 kubelet exec command Error.</span></p><pre><code>k describe pod kube-scheduler-controlplane -n kube-system
</code></pre><p><img src="https://i.imgur.com/kNIwlMh.png" alt="" loading="lazy"></p><p><span>因為 kube-scheduler 是個 static pod, 所以要去 /etc/kubernetes/manifests 路徑去搜尋他的定義文件並且修改.</span></p><h4 id="Task-2---Scale-deployment" data-id="Task-2---Scale-deployment"><a class="anchor hidden-xs" href="#Task-2---Scale-deployment" title="Task-2---Scale-deployment"><span class="octicon octicon-link"></span></a><span>Task 2 - Scale deployment</span></h4><hr><p><span>Q1: Scale the deployment app to 2 pods.</span></p><p><span>Q2: Even though the deployment was scaled to 2, the number of PODs does not seem to increase. Investigate and fix the issue.</span><br>
<span>Inspect the component responsible for managing deployments and replicasets.</span></p><hr><p><span>參考 </span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment" target="_blank" rel="noopener"><span>k8s.io-Scaling a Deployment</span></a></p><pre><code>k scale deployment app --replicas=2
</code></pre><p><span>開始檢查為什麼 pods 數量沒有跟隨 deployment scale 2 而變成兩個, 發現 kube-system 底下有 pod CrashLoopBackOff.</span><br>
<span>但是這次 describe 沒有發現甚麼有用的訊息，所以改用 logs 去看, 就發現 error message 了.</span></p><pre><code>k get pods -A
k describe pod kube-controller-manager-controlplane -n kube-system
k logs kube-controller-manager-controlplane -n kube-system
</code></pre><p><img src="https://i.imgur.com/3HSnFK6.png" alt="" loading="lazy"></p><p><span>因為 kube-controller-manager-controlplane 是個 static pod, 所以要去 /etc/kubernetes/manifests 路徑去搜尋他的定義文件並且修改.</span></p><pre><code>vim /etc/kubernetes/manifests/kube-controller-manager.yaml
k get pods -n kube-system --watch
k get deployments -n kube-system
</code></pre><h4 id="Task-3---Scale-deployment_2" data-id="Task-3---Scale-deployment_2"><a class="anchor hidden-xs" href="#Task-3---Scale-deployment_2" title="Task-3---Scale-deployment_2"><span class="octicon octicon-link"></span></a><span>Task 3 - Scale deployment_2</span></h4><hr><p><span>Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it’s not happening.</span><br>
<span>Investigate and fix the issue.</span></p><hr><pre><code>k get pods -A
k logs kube-controller-manager-controlplane -n kube-system
</code></pre><p><img src="https://i.imgur.com/NsXi41D.png" alt="" loading="lazy"><br>
<span>這次觀察出是憑證問題, error log 說 “/etc/kubernetes/pki/ca.crt: no such file or directory”, 但是實際去看是有的.</span></p><pre><code>vim /etc/kubernetes/manifests/kube-controller-manager.yaml
</code></pre><p><span>看證書的 volumeMounts 名稱為 k8s-certs, 再往下找到 error (紅色箭頭處)</span><br>
<img src="https://i.imgur.com/iqpvsZ1.png" alt="" loading="lazy"></p><h3 id="Worker-Node-Failure" data-id="Worker-Node-Failure"><a class="anchor hidden-xs" href="#Worker-Node-Failure" title="Worker-Node-Failure"><span class="octicon octicon-link"></span></a><span>Worker Node Failure</span></h3><h2 id="Others" data-id="Others"><a class="anchor hidden-xs" href="#Others" title="Others"><span class="octicon octicon-link"></span></a><span>Others</span></h2><h3 id="Deplyment" data-id="Deplyment"><a class="anchor hidden-xs" href="#Deplyment" title="Deplyment"><span class="octicon octicon-link"></span></a><span>Deplyment</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10237456" target="_blank" rel="noopener"><span>Ref1 從題目中學習k8s</span></a><br>
<a href="https://stackoverflow.com/questions/73814500/record-has-been-deprecated-then-what-is-the-alternative" target="_blank" rel="noopener"><span>Ref2 --record has been deprecated, then what is the alternative</span></a></p><p><img src="https://i.imgur.com/JnZHgeD.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><pre><code>sudo kubectl create deploy nginx-app --image=nginx:1.11.0-alpine --replicas 3
sudo kubectl set image deployment nginx-app nginx=nginx:1.11.3-alpine --&gt; 升版
sudo kubectl annotate deployment nginx-app kubernetes.io/change-cause="version change to 1.11.0 to 1.11.3" --overwrite=true
sudo kubectl rollout history deployment nginx-app
sudo kubectl rollout undo deployment nginx-app --&gt; 退版
</code></pre><p><img src="https://i.imgur.com/leII1mB.png" alt="" loading="lazy"></p><p><span>考點是 rolling update 和 rollout，也就是K8s中重要的版本控制、版本升級與版本回滾 (版本是指 image 的版本)</span></p><p><span>在K8s中有兩種版本升級的 strategies</span></p><ul>
<li><ins><span>recreate</span></ins><br>
<span>recreate方法很直覺，就是直接將所有Pod一次升級，一次刪除所有舊版Pod，再一次創建所有新版Pod，缺點是更新過程會暫時中斷服務</span></li>
<li><ins><span>rolling update</span></ins><br>
<span>rolling update則是將舊版Pod一個一個刪除，再一個一個創建新的，可以保證更新期間提供的服務不會中斷。</span></li>
</ul><div class="alert alert-success">
<p><span>創建 Deployment 時不必特別指定 update strategy type</span><br>
<span>K8s中 default的策略就是 rolling update (可以用 kubectl describe deploy 命令查看)</span></p>
</div><h3 id="22-Volume-Test" data-id="22-Volume-Test"><a class="anchor hidden-xs" href="#22-Volume-Test" title="22-Volume-Test"><span class="octicon octicon-link"></span></a><span>22. Volume-Test</span></h3><p><a href="https://ithelp.ithome.com.tw/articles/10241004" target="_blank" rel="noopener"><span>Ref</span></a><br>
<img src="https://i.imgur.com/1co2rP7.png" alt="" loading="lazy"></p><hr><ul>
<li><span>Answer</span></li>
</ul><p><span>1.創建一個 Pod</span><br>
<span>2.mount 到類型為emptyDir的Volume中</span></p><pre><code>1.sudo kubectl run redis-storage --image=redis --dry-run=client -o yaml &gt; q7-pod.yaml
</code></pre><p><a href="https://github.com/oldelette/oldelette.github.io/blob/master/secret/q7.yaml" target="_blank" rel="noopener"><span>q7.yaml</span></a></p><h2 data-id="Mock-Exam" id="Mock-Exam"><a class="anchor hidden-xs" href="#Mock-Exam" title="Mock-Exam"><span class="octicon octicon-link"></span></a><span>Mock Exam</span></h2><p><a href="https://www.youtube.com/@xiaomoinfo/videos?view=2&amp;sort=dd&amp;live_view=503&amp;shelf_id=0" target="_blank" rel="noopener"><span>小莫 CKA 講解</span></a></p><h2 id="Mock-Exam---1" data-id="Mock-Exam---1"><a class="anchor hidden-xs" href="#Mock-Exam---1" title="Mock-Exam---1"><span class="octicon octicon-link"></span></a><span>Mock Exam - 1</span></h2><p><span>2023.2.25</span><br>
<img src="https://i.imgur.com/qaji2FR.png" alt="" loading="lazy"></p><p><a href="https://github.com/oldelette/oldelette.github.io/tree/master/Udemy/mock_exam/1" target="_blank" rel="noopener"><span>Answer mock_exam_1</span></a></p><h3 id="Q5" data-id="Q5"><a class="anchor hidden-xs" href="#Q5" title="Q5"><span class="octicon octicon-link"></span></a><span>Q5</span></h3><p><img src="https://i.imgur.com/05HUJEG.png" alt="" loading="lazy"></p><h3 id="Q7" data-id="Q7"><a class="anchor hidden-xs" href="#Q7" title="Q7"><span class="octicon octicon-link"></span></a><span>Q7</span></h3><p><img src="https://i.imgur.com/fR3kboK.png" alt="" loading="lazy"></p><h3 id="Q8" data-id="Q8"><a class="anchor hidden-xs" href="#Q8" title="Q8"><span class="octicon octicon-link"></span></a><span>Q8</span></h3><p><img src="https://i.imgur.com/KjQ2ec2.png" alt="" loading="lazy"></p><h3 id="Q10" data-id="Q10"><a class="anchor hidden-xs" href="#Q10" title="Q10"><span class="octicon octicon-link"></span></a><span>Q10</span></h3><p><img src="https://i.imgur.com/VnVrvrl.png" alt="" loading="lazy"></p><h3 id="Q11" data-id="Q11"><a class="anchor hidden-xs" href="#Q11" title="Q11"><span class="octicon octicon-link"></span></a><span>Q11</span></h3><p><img src="https://i.imgur.com/PkWKMGP.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/reference/kubectl/jsonpath/" target="_blank" rel="noopener"><span>k8s.io - JSONPath Support</span></a></p><pre><code>k get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}'
</code></pre><h3 id="Q12" data-id="Q12"><a class="anchor hidden-xs" href="#Q12" title="Q12"><span class="octicon octicon-link"></span></a><span>Q12</span></h3><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes" target="_blank" rel="noopener"><span>k8s.io - Persistent Volumes</span></a></p><p><img src="https://i.imgur.com/6wnmdxr.png" alt="" loading="lazy"></p><h2 id="Mock-Exam---2" data-id="Mock-Exam---2"><a class="anchor hidden-xs" href="#Mock-Exam---2" title="Mock-Exam---2"><span class="octicon octicon-link"></span></a><span>Mock Exam - 2</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/tree/master/Udemy/mock_exam/2" target="_blank" rel="noopener"><span>Answer mock_exam_2</span></a></p><h3 id="Q3-Security" data-id="Q3-Security"><a class="anchor hidden-xs" href="#Q3-Security" title="Q3-Security"><span class="octicon octicon-link"></span></a><span>Q3 Security</span></h3><p><img src="https://i.imgur.com/cWqQX30.png" alt="" loading="lazy"></p><p><a href="https://ithelp.ithome.com.tw/articles/10241665" target="_blank" rel="noopener"><span>這題</span></a><span>的重點是在 </span><strong><span>Allow the pod to be able to set system_time</span></strong></p><h3 id="Q4-PV-amp-PVC" data-id="Q4-PV-amp-PVC"><a class="anchor hidden-xs" href="#Q4-PV-amp-PVC" title="Q4-PV-amp-PVC"><span class="octicon octicon-link"></span></a><span>Q4 PV &amp; PVC</span></h3><p><img src="https://i.imgur.com/1vi89ne.png" alt="" loading="lazy"></p><h3 id="Q6-CSR---Certificate-Signing-Request" data-id="Q6-CSR---Certificate-Signing-Request"><a class="anchor hidden-xs" href="#Q6-CSR---Certificate-Signing-Request" title="Q6-CSR---Certificate-Signing-Request"><span class="octicon octicon-link"></span></a><span>Q6 CSR - Certificate Signing Request</span></h3><p><img src="https://i.imgur.com/FEcGZeB.png" alt="" loading="lazy"></p><pre><code># Create CertificateSigningRequest
cat CKA/john.csr | base64 | tr -d "\n"
k create -f CKA/q6-csr.yaml
--&gt; certificatesigningrequest.certificates.k8s.io/john-developer created
k get csr
k certificate approve john-developer
</code></pre><p><img src="https://i.imgur.com/bCWpTKt.png" alt="" loading="lazy"></p><p><span>這題創建必要的 role 根 role binding 使得 john 能夠 list / delete / create / delete / get pods</span></p><pre><code>k create -f q6.yaml
k describe role -n development
k auth can-i list pods -n=development --as john --&gt; no (rolebind 尚未有權限)

k create -f q6-1.yaml
k describe rolebindings.rbac.authorization.k8s.io -n development
k auth can-i list pods -n=development --as john --&gt; yes (rolebind 完就可以看到有權限了)
</code></pre><p><img src="https://i.imgur.com/NO7nzTQ.png" alt="" loading="lazy"></p><h3 id="Q7-nslookup" data-id="Q7-nslookup"><a class="anchor hidden-xs" href="#Q7-nslookup" title="Q7-nslookup"><span class="octicon octicon-link"></span></a><span>Q7 nslookup</span></h3><p><img src="https://i.imgur.com/38h3btB.png" alt="" loading="lazy"></p><pre><code>k run nginx-resolver --image=nginx
k expose pod nginx-resolver --name=nginx-resolver-service --port=80
k run busybox --image=busybox:1.28 -- sleep 4000

k exec busybox -- nslookup nginx-resolver-service
</code></pre><div class="alert alert-info">
<p><span>K8s 內有自己的網路, 相同 name space 內的服務都可以透過內部網路直接溝通, 也有自己的 DNS 可以將我們建立的 service name 轉成內部的 IP.</span><br>
<span>因此內部 DNS 如果無法正常運作, 就會造成內部使用 service name 進行溝通的服務出現問題</span></p>
</div><ul>
<li><ins><span>svc</span></ins><br>
<span>檢測 K8s 內的 DNS</span></li>
</ul><pre><code>k exec busybox -- nslookup nginx-resolver-service &gt; /root/CKA/nginx.svc
</code></pre><p><img src="https://i.imgur.com/PKGnoQF.png" alt="" loading="lazy"></p><ul>
<li><ins><span>pod</span></ins></li>
</ul><pre><code>k exec busybox -- nslookup 10-244-192-2.default.pod.cluster.local &gt; /root/CKA/nginx.pod
</code></pre><p><img src="https://i.imgur.com/IuF3yFx.png" alt="" loading="lazy"></p><h4 id="DNS-for-Pod" data-id="DNS-for-Pod"><a class="anchor hidden-xs" href="#DNS-for-Pod" title="DNS-for-Pod"><span class="octicon octicon-link"></span></a><span>DNS for Pod</span></h4><ul>
<li><ins><span>A record</span></ins><br>
<span>跟 service 相同，每個 pod 在產生的時候也會以下述的格式分配一個 DNS A record 在 k8s DNS service 中：</span></li>
</ul><div class="alert alert-success">
<p><span>[pod-ip-address].[namespace-name].pod.cluster.local</span></p>
</div><p><span>因此假設 pod 的 ip 為 1.2.3.4，namespace 為 default，且 cluster domain 為 cluster.local，那就會有 1-2-3-4.default.pod.cluster.local 這筆 A record 產生</span></p><ul>
<li><ins><span>SRV records</span></ins><br>
<span>除了 A record 之外，k8s DNS 還會額外建立相對應的 SRV record，並且用以下的命名規則來產生：</span></li>
</ul><div class="alert alert-success">
<p><span>[_my-port-name].[_my-port-protocol].[svc_name].[namespace_name].svc.cluster.local</span></p>
</div><h3 id="Q8-Static-pod-on-worker-node" data-id="Q8-Static-pod-on-worker-node"><a class="anchor hidden-xs" href="#Q8-Static-pod-on-worker-node" title="Q8-Static-pod-on-worker-node"><span class="octicon octicon-link"></span></a><span>Q8 Static pod on worker node</span></h3><p><img src="https://i.imgur.com/KEF3C8H.png" alt="" loading="lazy"></p><p><span>這題的重點是要在 woker node 上執行 pod, 所以我們要先在 controlplane 上生出 pod 的 yaml file, 在 scp  送進去 worker node 上的 static pod path: /etc/kubernetes/manifests</span></p><pre><code>kubectl run nginx-critical --image=nginx --dry-run=client -o yaml &gt; static.yaml
</code></pre><ul>
<li><span>Copy the contents of this file or use scp command to transfer this file from controlplane to node01 node.</span></li>
</ul><pre><code>scp static.yaml node01:/root/
kubectl get nodes -o wide
# On node01 node
ssh node01
cp /root/static.yaml /etc/kubernetes/manifests/
</code></pre><h2 id="Mock-Exam---3" data-id="Mock-Exam---3"><a class="anchor hidden-xs" href="#Mock-Exam---3" title="Mock-Exam---3"><span class="octicon octicon-link"></span></a><span>Mock Exam - 3</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/tree/master/Udemy/mock_exam/3" target="_blank" rel="noopener"><span>Answer mock_exam_3</span></a></p><p><span>2023.3.1</span><br>
<img src="https://i.imgur.com/v1APc0a.png" alt="" loading="lazy"></p><h3 id="Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding" data-id="Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding"><a class="anchor hidden-xs" href="#Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding" title="Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding"><span class="octicon octicon-link"></span></a><span>Q1 - Service account / Cluster Role &amp; Cluster RoleBinding</span></h3><p><img src="https://i.imgur.com/WCGQYLF.png" alt="" loading="lazy"></p><pre><code>k create serviceaccount pvviewer
k describe clusterrole pvviewer-role
k describe clusterrolebindings pvviewer-role-binding
</code></pre><p><img src="https://i.imgur.com/zxlcysh.png" alt="" loading="lazy"></p><h3 id="Q2---json-path" data-id="Q2---json-path"><a class="anchor hidden-xs" href="#Q2---json-path" title="Q2---json-path"><span class="octicon octicon-link"></span></a><span>Q2 - json path</span></h3><p><img src="https://i.imgur.com/C8gUjIu.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/#viewing-and-finding-resources" target="_blank" rel="noopener"><span>k8s-io: Viewing and finding resources</span></a></p><pre><code> k get nodes -A -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' &gt; /root/CKA/node_ips
</code></pre><h3 id="Q3---Environment-Variables" data-id="Q3---Environment-Variables"><a class="anchor hidden-xs" href="#Q3---Environment-Variables" title="Q3---Environment-Variables"><span class="octicon octicon-link"></span></a><span>Q3 - Environment Variables</span></h3><p><img src="https://i.imgur.com/d7ny2Wm.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#define-an-environment-variable-for-a-container" target="_blank" rel="noopener"><span>k8s-io: Define an environment variable for a container</span></a></p><h3 id="Q4---Security" data-id="Q4---Security"><a class="anchor hidden-xs" href="#Q4---Security" title="Q4---Security"><span class="octicon octicon-link"></span></a><span>Q4 - Security</span></h3><p><img src="https://i.imgur.com/A4vsWaM.png" alt="" loading="lazy"></p><h3 id="Q5---NetworkPolicy" data-id="Q5---NetworkPolicy"><a class="anchor hidden-xs" href="#Q5---NetworkPolicy" title="Q5---NetworkPolicy"><span class="octicon octicon-link"></span></a><span>Q5 - NetworkPolicy</span></h3><p><img src="https://i.imgur.com/WAvSVfm.png" alt="" loading="lazy"></p><p><span>利用 curl image 去測試 service: np-test-service 有沒有通, 會發現不通(卡住), 所以要建立 networkpolicies</span></p><pre><code>k run curl --image=alpine/curl --rm -it -- sh
curl np-test-service
</code></pre><p><img src="https://i.imgur.com/MdKwTgz.png" alt="" loading="lazy"></p><h3 id="Q6---Taint--Toleration" data-id="Q6---Taint--Toleration"><a class="anchor hidden-xs" href="#Q6---Taint--Toleration" title="Q6---Taint--Toleration"><span class="octicon octicon-link"></span></a><span>Q6 - Taint / Toleration</span></h3><p><img src="https://i.imgur.com/0HVGWZp.png" alt="" loading="lazy"></p><pre><code>k taint node node01 env_type=production:NoSchedule
</code></pre><h3 id="Q7---Namespace" data-id="Q7---Namespace"><a class="anchor hidden-xs" href="#Q7---Namespace" title="Q7---Namespace"><span class="octicon octicon-link"></span></a><span>Q7 - Namespace</span></h3><p><img src="https://i.imgur.com/NuTEqBi.png" alt="" loading="lazy"></p><pre><code>k create namespace hr
</code></pre><h3 id="Q8---Kubeconfig" data-id="Q8---Kubeconfig"><a class="anchor hidden-xs" href="#Q8---Kubeconfig" title="Q8---Kubeconfig"><span class="octicon octicon-link"></span></a><span>Q8 - Kubeconfig</span></h3><p><img src="https://i.imgur.com/CSzN4Z5.png" alt="" loading="lazy"></p><p><span>用 --kubeconfig 指定要使用自己的 kubeconfig file 去 check 正確性</span></p><pre><code>k get nodes --kubeconfig CKA/super.kubeconfig
</code></pre><h3 id="Q9---Troubleshooting" data-id="Q9---Troubleshooting"><a class="anchor hidden-xs" href="#Q9---Troubleshooting" title="Q9---Troubleshooting"><span class="octicon octicon-link"></span></a><span>Q9 - Troubleshooting</span></h3><p><img src="https://i.imgur.com/4AxMlrW.png" alt="" loading="lazy"></p><pre><code>k edit deployments.apps nginx-deploy
</code></pre><p><span>發現沒有正常擴展到 replica=3, 去看 controller-manager.</span><br>
<span>(因為 Deployment 是通過控制 ReplicaSet，ReplicaSet 再控制 Pod，最终由 controller-manager 驅動達到期望狀態)</span></p><pre><code>k describe pod kube-contro1ler-manager-controlplane -n kube-system
vim /etc/kubernetes/manifests/kube-controller-manager.yaml
</code></pre><p><img src="https://i.imgur.com/8a40q0w.png" alt="" loading="lazy"></p><p><span>contro1ler 修正成 controller 即可 (有5個地方錯)</span></p><h2 id="Lightning-Lab" data-id="Lightning-Lab"><a class="anchor hidden-xs" href="#Lightning-Lab" title="Lightning-Lab"><span class="octicon octicon-link"></span></a><span>Lightning Lab</span></h2><p><a href="https://github.com/oldelette/oldelette.github.io/tree/master/Udemy/mock_exam/Lightning%20Lab" target="_blank" rel="noopener"><span>Answer mock_exam_Lightning Lab</span></a></p><h3 id="Q1-Upgrade-amp-Taint" data-id="Q1-Upgrade-amp-Taint"><a class="anchor hidden-xs" href="#Q1-Upgrade-amp-Taint" title="Q1-Upgrade-amp-Taint"><span class="octicon octicon-link"></span></a><span>Q1 Upgrade &amp; Taint</span></h3><p><img src="https://i.imgur.com/STO246l.png" alt="" loading="lazy"></p><p><span>重點在 Pods for gold-nginx should run on the controlplane node</span></p><pre><code>kubectl drain controlplane --ignore-daemonsets
kubectl uncordon controlplane
</code></pre><p><span>Before draining node01, we need to remove the taint from the controlplane node.</span></p><pre><code># Identify the taint first. 
root@controlplane:~# kubectl describe node controlplane | grep -i taint

# Remove the taint with help of "kubectl taint" command.
root@controlplane:~# kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-

# Verify it, the taint has been removed successfully.  
root@controlplane:~# kubectl describe node controlplane | grep -i taint
</code></pre><p><span>drain the node01</span></p><pre><code>kubectl drain node01 --ignore-daemonsets
kubectl uncordon node01
kubectl get pods -o wide | grep gold (make sure this is scheduled on node)
</code></pre><h3 id="Q2---json-path1" data-id="Q2---json-path"><a class="anchor hidden-xs" href="#Q2---json-path1" title="Q2---json-path1"><span class="octicon octicon-link"></span></a><span>Q2 - json path</span></h3><p><img src="https://i.imgur.com/GqcZWzn.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/reference/kubectl/#custom-columns" target="_blank" rel="noopener"><span>k8s-io Custom columns</span></a><br>
<a href="https://kubernetes.io/docs/reference/kubectl/#syntax-2" target="_blank" rel="noopener"><span>To print a list of pods sorted by name</span></a></p><pre><code>k get deployments.apps -n admin2406 -o custom-columns=DEPLOYMENT:.metadata.name,\
CONTAINER_IMAGE:.spec.template.spec.containers[].image,\
READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name &gt; /opt/admin2406_data
</code></pre><h3 id="Q3---KubeConfig-Troubleshoot" data-id="Q3---KubeConfig-Troubleshoot"><a class="anchor hidden-xs" href="#Q3---KubeConfig-Troubleshoot" title="Q3---KubeConfig-Troubleshoot"><span class="octicon octicon-link"></span></a><span>Q3 - KubeConfig (Troubleshoot)</span></h3><p><img src="https://i.imgur.com/Ez2Skpu.png" alt="" loading="lazy"></p><h3 id="Q4---Rolling-update" data-id="Q4---Rolling-update"><a class="anchor hidden-xs" href="#Q4---Rolling-update" title="Q4---Rolling-update"><span class="octicon octicon-link"></span></a><span>Q4 - Rolling update</span></h3><p><img src="https://i.imgur.com/rrTzcg5.png" alt="" loading="lazy"></p><pre><code>k edit deployments.apps nginx-deploy --record
k rollout history deployment nginx-deploy
</code></pre><h3 id="Q5---PVC-Troubleshoot" data-id="Q5---PVC-Troubleshoot"><a class="anchor hidden-xs" href="#Q5---PVC-Troubleshoot" title="Q5---PVC-Troubleshoot"><span class="octicon octicon-link"></span></a><span>Q5 - PVC (Troubleshoot)</span></h3><p><img src="https://i.imgur.com/ee23gz0.png" alt="" loading="lazy"></p><pre><code>k describe deployments alpha-mysql -n alpha
k describe pvc -n alpha
</code></pre><p><img src="https://i.imgur.com/CFWwTNI.png" alt="" loading="lazy"></p><p><span>要特別注意 PV Capacity 跟 PVC Request</span></p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-alpha-pvc
  namespace: alpha
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: slow
</code></pre><h3 id="Q6---ETCD-Save" data-id="Q6---ETCD-Save"><a class="anchor hidden-xs" href="#Q6---ETCD-Save" title="Q6---ETCD-Save"><span class="octicon octicon-link"></span></a><span>Q6 - ETCD Save</span></h3><p><img src="https://i.imgur.com/MmDhfDN.png" alt="" loading="lazy"></p><pre><code>cat /etc/kubernetes/manifests/etcd.yaml --&gt; 找必要檔案的路徑

export ETCDCTL_API=3
ETCDCTL_API=3 etcdctl --endpoints=https://192.31.5.9:2379\
--cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --\
key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-backup.db
--&gt;Snapshot saved at /opt/etcd-backup.db
</code></pre><h3 id="Q7---Secret" data-id="Q7---Secret"><a class="anchor hidden-xs" href="#Q7---Secret" title="Q7---Secret"><span class="octicon octicon-link"></span></a><span>Q7 - Secret</span></h3><p><img src="https://i.imgur.com/PyUUVjM.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/#restriction-secret-must-exist" target="_blank" rel="noopener"><span>k8s-io - Optional Secrets</span></a><br>
<span>將 Secret 製作為一個檔案，Pod 以 Volume 的形式將此檔案掛載 (mount) 到容器上.</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: secret-1401
  name: secret-1401
  namespace: admin1401
spec:
  containers:
  - image: busybox
    name: secret-admin
    resources: {}
    command: ["/bin/sh"]
    args: ["-c", "sleep 4800"]
    volumeMounts:
    - name: secret-volume
      mountPath: "/etc/secret-volume"
      readOnly: true
  volumes:
  - name: secret-volume
    secret:
      secretName: dotfile-secret
      optional: true
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre><h2 data-id="真題" id="真題"><a class="anchor hidden-xs" href="#真題" title="真題"><span class="octicon octicon-link"></span></a><span>真題</span></h2><h3 data-id="1-RBAC" id="1-RBAC"><a class="anchor hidden-xs" href="#1-RBAC" title="1-RBAC"><span class="octicon octicon-link"></span></a><span>1. RBAC</span></h3><p><img src="https://i.imgur.com/ZqO8UkH.png" alt="" loading="lazy"></p><div class="alert alert-danger">
<p><span>role / rolebinding  有 ns</span><br>
<span>clusterrole / clusterrolebinding  無 ns</span><br>
<span>serviceaccount  有 ns</span></p>
<p><span>pv 無 ns</span><br>
<span>pvc 有 ns</span></p>
</div><pre><code>kubectl create clusterrole deployment-clusterrole --verb=create \
--resource=deployments,daemonsets,statefulsets

kubectl create serviceaccount cicd-token -n app-team1

kubectl create rolebinding cicd-token-rolebinding --serviceaccount=app-team1:cicd-token \
--clusterrole=deployment-clusterrole -n app-team1
</code></pre><p><img src="https://i.imgur.com/l1rp3x6.png" alt="" loading="lazy"></p><h3 id="2-CPU" data-id="2-CPU"><a class="anchor hidden-xs" href="#2-CPU" title="2-CPU"><span class="octicon octicon-link"></span></a><span>2. CPU</span></h3><p><img src="https://i.imgur.com/CVJw3y4.png" alt="" loading="lazy"></p><pre><code>k top pod -l name=cpu-utilizer -A --sort-by cpu &gt; /opt/KUTR00401/KUTR00401.txt
</code></pre><h3 id="3-Networkpolicy" data-id="3-Networkpolicy"><a class="anchor hidden-xs" href="#3-Networkpolicy" title="3-Networkpolicy"><span class="octicon octicon-link"></span></a><span>3. Networkpolicy</span></h3><p><img src="https://i.imgur.com/hYYkdDT.png" alt="" loading="lazy"></p><pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: my-app
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: big-corp # 訪問者的 ns labels
      ports:
        - protocol: TCP
          port: 8080
</code></pre><h4 id="檢查" data-id="檢查"><a class="anchor hidden-xs" href="#檢查" title="檢查"><span class="octicon octicon-link"></span></a><span>檢查:</span></h4><p><span>allow-port-from-namespace</span></p><p><img src="https://i.imgur.com/bHH7uSo.png" alt="" loading="lazy"></p><h3 id="4-Service" data-id="4-Service"><a class="anchor hidden-xs" href="#4-Service" title="4-Service"><span class="octicon octicon-link"></span></a><span>4. Service</span></h3><p><img src="https://i.imgur.com/yg3M6Kk.png" alt="" loading="lazy"></p><p><span>deployment 修改 pod container port</span></p><pre><code>k edit deployment front-end

...
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:                #　增加
        - containerPort: 80　　#　增加
          name: http　　　　　　#　增加
          
kubectl expose deployment front-end --port=80 --target-port=80 ＼
--type=NodePort --name=front-end-svc
</code></pre><h4 id="檢查1" data-id="檢查"><a class="anchor hidden-xs" href="#檢查1" title="檢查1"><span class="octicon octicon-link"></span></a><span>檢查</span></h4><pre><code>1. curl &lt;node&gt;:&lt;Nodeport&gt;
--&gt; curl cluster-node2:32566
2. curl &lt;svc&gt;:80
--&gt; curl 10.108.194.230:80
</code></pre><p><img src="https://i.imgur.com/5iku3CA.png" alt="" loading="lazy"></p><h3 id="5-Ingress" data-id="5-Ingress"><a class="anchor hidden-xs" href="#5-Ingress" title="5-Ingress"><span class="octicon octicon-link"></span></a><span>5. Ingress</span></h3><p><img src="https://i.imgur.com/TmKsgR8.png" alt="" loading="lazy"></p><ul>
<li><span>-k：跳過 SSL 證書檢測</span></li>
<li><span>-L：跟随跳轉，比如網站做了重定向，不加這個選項的话只會看到一個 302 的訪問 code 就结束了，加上的話會看到完整的跳轉情况</span></li>
</ul><pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pong
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 5678
</code></pre><h4 id="檢查2" data-id="檢查"><a class="anchor hidden-xs" href="#檢查2" title="檢查2"><span class="octicon octicon-link"></span></a><span>檢查</span></h4><pre><code>kubectl get ingress -ning-internal
curl -kL internal_IP/hello
</code></pre><h3 id="6-Deployment-scale" data-id="6-Deployment-scale"><a class="anchor hidden-xs" href="#6-Deployment-scale" title="6-Deployment-scale"><span class="octicon octicon-link"></span></a><span>6. Deployment scale</span></h3><p><img src="https://i.imgur.com/WoI8ARW.png" alt="" loading="lazy"></p><pre><code>k scale deployment loadbalancer --replicas 2
</code></pre><h3 id="7-nodeSelector" data-id="7-nodeSelector"><a class="anchor hidden-xs" href="#7-nodeSelector" title="7-nodeSelector"><span class="octicon octicon-link"></span></a><span>7. nodeSelector</span></h3><p><img src="https://i.imgur.com/GGYzQGp.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/#using-pods" target="_blank" rel="noopener"><span>k8s-io Using Pods</span></a><span> 再手動新增 </span><strong><span>nodeSelector 欄位</span></strong></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc00401
spec:
  containers:
  - name: nginx-kusc00401
    image: nginx
  nodeSelector: #這個是和 containers 同一级别的。
    disk: ssd
</code></pre><h3 id="8-Node-Scheduled" data-id="8-Node-Scheduled"><a class="anchor hidden-xs" href="#8-Node-Scheduled" title="8-Node-Scheduled"><span class="octicon octicon-link"></span></a><span>8. Node Scheduled</span></h3><p><img src="https://i.imgur.com/5IpxBFc.png" alt="" loading="lazy"></p><pre><code>k describe node | grep Taint | grep -vc NoSchedule &gt; /opt/KUSC00402/kusc00402.txt
</code></pre><ul>
<li><span>-c 代表統計個數</span></li>
<li><span>-v 代表排除</span></li>
</ul><h4 id="檢查3" data-id="檢查"><a class="anchor hidden-xs" href="#檢查3" title="檢查3"><span class="octicon octicon-link"></span></a><span>檢查</span></h4><pre><code>cat /opt/KUSC00402/kusc00402.txt
--&gt; 2
</code></pre><h3 id="9-Multi-container" data-id="9-Multi-container"><a class="anchor hidden-xs" href="#9-Multi-container" title="9-Multi-container"><span class="octicon octicon-link"></span></a><span>9. Multi-container</span></h3><p><img src="https://i.imgur.com/VutY8Tk.png" alt="" loading="lazy"><br>
<a href="https://kubernetes.io/docs/concepts/workloads/pods/#using-pods" target="_blank" rel="noopener"><span>k8s-io Using Pods</span></a><span> 在手動增加多個 container</span></p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: kucc4
spec:
  containers:
  - name: nginx
    image: nginx
  - name: redis
    image: redis
  - name: memcached
    image: memcached
</code></pre><h4 id="檢查4" data-id="檢查"><a class="anchor hidden-xs" href="#檢查4" title="檢查4"><span class="octicon octicon-link"></span></a><span>檢查</span></h4><pre><code>k get pods 確認有 3/3
</code></pre><p><img src="https://i.imgur.com/sK54hYd.png" alt="" loading="lazy"></p><h3 id="10-PV" data-id="10-PV"><a class="anchor hidden-xs" href="#10-PV" title="10-PV"><span class="octicon octicon-link"></span></a><span>10. PV</span></h3><p><img src="https://i.imgur.com/zIL4D3U.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes" target="_blank" rel="noopener"><span>k8s-io Persistent Volumes</span></a><span> 再去修正成題目要的</span></p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /srv/app-data
</code></pre><h3 id="11-PVC" data-id="11-PVC"><a class="anchor hidden-xs" href="#11-PVC" title="11-PVC"><span class="octicon octicon-link"></span></a><span>11. PVC</span></h3><p><img src="https://i.imgur.com/CJx0FbT.png" alt="" loading="lazy"></p><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"><span>k8s-io PersistentVolumeClaims</span></a><span> 再去修正成題目要的</span></p><pre><code># PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Mi
  storageClassName: ci-hostpath-sc
</code></pre><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes" target="_blank" rel="noopener"><span>k8s-io Claims As Volumes </span></a><span> 再去修正成題目要的</span></p><pre><code># Pod
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: pv-volume
</code></pre><p><span>更改 pvc 容量</span></p><pre><code>kubectl edit pvc pv-volume
</code></pre><p><img src="https://i.imgur.com/Ey8ttem.png" alt="" loading="lazy"></p><h3 id="12-Pod-log" data-id="12-Pod-log"><a class="anchor hidden-xs" href="#12-Pod-log" title="12-Pod-log"><span class="octicon octicon-link"></span></a><span>12. Pod log</span></h3><p><img src="https://i.imgur.com/sPb2EZe.png" alt="" loading="lazy"></p><pre><code>k logs bar | grep "file-not-found" &gt; /opt/KUTR00101/bar
</code></pre><h3 id="13-Sidecar" data-id="13-Sidecar"><a class="anchor hidden-xs" href="#13-Sidecar" title="13-Sidecar"><span class="octicon octicon-link"></span></a><span>13. Sidecar</span></h3><p><img src="https://i.imgur.com/EeVt4ek.png" alt="" loading="lazy"></p><p><span>sidecar 邊車容器不是作為主容器，而是輔助主容器做一些功能</span></p><pre><code># 先把運行中的 legacy-app pod 倒出來成 yaml file, 然後備份
kubectl get pods leagcy-app -o yaml &gt; sidecar.yaml
cp sidecar.yaml sidecar.yaml.bkg

# 修改好後
kubectl delet -f sidecar.yaml
kubectl create -f sidecar.yaml
</code></pre><p><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-logging-agent" target="_blank" rel="noopener"><span>k8s-io Using a sidecar container with the logging agent</span></a><span> 再去修正成題目要的</span></p><ul>
<li><span>增加之內容</span></li>
</ul><pre><code>... 
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox:1.28
    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
</code></pre><h3 id="14-Upgrade" data-id="14-Upgrade"><a class="anchor hidden-xs" href="#14-Upgrade" title="14-Upgrade"><span class="octicon octicon-link"></span></a><span>14. Upgrade</span></h3><p><a href="https://www.youtube.com/watch?v=LTvaD8aWTiM&amp;ab_channel=%E5%B0%8F%E8%8E%AB" target="_blank" rel="noopener"><span>CKA真题14</span></a></p><p><img src="https://i.imgur.com/kqNGyHf.png" alt="" loading="lazy"></p><p><strong><span>题目要求不升级 etcd，這裡手動補上</span></strong></p><pre><code>1、切換環境
kubectl config use-context mk8s

2、配置
#升级kueadm
kubectl drain mk8s-master-0 --ignore=daemonsets

ssh mk8s-master-0
sudo -i 

 apt-mark unhold kubeadm &amp;&amp; \
 apt-get update &amp;&amp; apt-get install -y kubeadm=1.26.0-00 &amp;&amp; \
 apt-mark hold kubeadm

kubeadm upgrade apply v1.26.0 --etcd-upgrade=false

#升级 kubelt
apt-mark unhold kubelet kubectl &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubelet=1.26.0-00 kubectl=1.26.0-00 &amp;&amp; \
apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet #這裡要重啟 kubelt 的

exit
exit

kubectl uncordon mk8s-master-0

3、驗證
kubectl get node -o wide
kubectl --version
kubelet --version
</code></pre><h3 data-id="15-ETCD-backup-amp-Restore" id="15-ETCD-backup-amp-Restore"><a class="anchor hidden-xs" href="#15-ETCD-backup-amp-Restore" title="15-ETCD-backup-amp-Restore"><span class="octicon octicon-link"></span></a><span>15. ETCD backup &amp; Restore</span></h3><p><a href="https://www.youtube.com/watch?v=ZNOMtTUU7XM&amp;ab_channel=%E5%B0%8F%E8%8E%AB" target="_blank" rel="noopener"><span>CKA真题15</span></a></p><p><img src="https://i.imgur.com/yGnWxub.png" alt="" loading="lazy"></p><pre><code>1、確定當前環境
kubectl get node
2. 配置
export ETCDCTL_API=3
etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; \
  snapshot save /var/lib/backup/etcd-snapshot.db
  
# Restoring an etcd cluster
ll /data/backup/etcd-snapshot-previous.db
# (不確定對不對) 直接還原, 不要加 --data-dir
etcdctl snapshot restore /var/lib/backup/etcd-snapshot-previous.db
</code></pre><pre><code># 正常流程, 還要去修正 /etc/kubernetes/manifests/etcd.yaml 裡面的 hostPath: 路徑
etcdctl snapshot restore --data-dir &lt;data-dir-location&gt; \
/var/lib/backup/etcd-snapshot-previous.db
vim /etc/kubernetes/manifests/etcd.yaml
</code></pre><ul>
<li><span>–data-dir: 自己定義要將 ETCD 還原的位置 &lt;data-dir-location&gt; ，會自動幫創建資料夾</span></li>
</ul><p><span>以下應該正確 restore</span></p><pre><code>mv /etc/kubernetes/manifests /etc/kubernetes/manifests_bkg
ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db
cp -r /etc/kubernetes/manifests_bkg /etc/kubernetes/manifests
# 裡面的 hostPath 路徑修正
vim /etc/kubernetes/manifests/etcd.yaml 
#　
</code></pre><p><img src="https://i.imgur.com/T76Ky8q.png" alt="" loading="lazy"></p><h3 id="16-Troubleshoot" data-id="16-Troubleshoot"><a class="anchor hidden-xs" href="#16-Troubleshoot" title="16-Troubleshoot"><span class="octicon octicon-link"></span></a><span>16. Troubleshoot</span></h3><p><a href="https://www.youtube.com/watch?v=H5Qtns6Y3UU&amp;ab_channel=%E5%B0%8F%E8%8E%AB" target="_blank" rel="noopener"><span>CKA真题16</span></a></p><p><img src="https://i.imgur.com/G0Mwkqo.png" alt="" loading="lazy"></p><p><span>通過 get nodes 查看異常節點，登入節點查看 kubelet 等组件的 status 並判斷原因.</span><br>
<span>真實考试时，這個異常節點的 kubelet 服務没有啟動導致的.</span><br>
<span>journalctl -u kubelet</span></p><pre><code>k config use-context wk8s
k get node
ssh wk8s-node-0
sudo -i
systemctl status kubelet
systemctl start kubelet
systemctl enable kubelet
</code></pre><h3 id="17-Drain-amp-Cordon" data-id="17-Drain-amp-Cordon"><a class="anchor hidden-xs" href="#17-Drain-amp-Cordon" title="17-Drain-amp-Cordon"><span class="octicon octicon-link"></span></a><span>17. Drain &amp; Cordon</span></h3><p><a href="https://www.youtube.com/watch?v=jx1R0EzMWEA&amp;ab_channel=%E5%B0%8F%E8%8E%AB" target="_blank" rel="noopener"><span>CKA真题17</span></a></p><p><img src="https://i.imgur.com/hHSa1AZ.png" alt="" loading="lazy"></p><pre><code>k config use-context wk8s
kubectl cordon ek8s-node-1
kubectl drain ek8s-node-1 --ignore-daemonsets

# 如果 --ignore-daemonsets 報錯, 使用
kubectl drain ek8s-node-1 --ignore-daemonsets --delete-local-data --force
kubectl drain ek8s-node-1 --ignore-daemonsets --delete-emptydir-data --force

# 驗證
kubectl get node
</code></pre><p><span>正常來說已經沒有 pod 會在 ek8s-node-1 上了, 但是 daemonsets 模式的 pod 還是會顯示在 ek8s-node-1 上 (因為上面加了 --ignore-daemonsets)</span></p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Kubectl" title="Kubectl">Kubectl</a><ul class="nav">
<li><a href="#Tips" title="Tips">Tips</a><ul class="nav">
<li><a href="#查看-K8s-所有物件和它們的縮寫" title="查看 K8s 所有物件和它們的縮寫">查看 K8s 所有物件和它們的縮寫</a></li>
<li><a href="#Pod-amp-yaml" title="Pod &amp; yaml">Pod &amp; yaml</a></li>
<li><a href="#Deployment" title="Deployment">Deployment</a></li>
<li><a href="#Service" title="Service">Service</a></li>
</ul>
</li>
<li><a href="#Core-Concepts" title="Core Concepts">Core Concepts</a><ul class="nav">
<li><a href="#1-Label" title="1. Label">1. Label</a></li>
<li><a href="#2-Deployment" title="2. Deployment">2. Deployment</a></li>
<li><a href="#21-ReplicaSet-複製集" title="2.1 ReplicaSet 複製集">2.1 ReplicaSet 複製集</a></li>
<li><a href="#22-Replication-Controller" title="2.2 Replication Controller">2.2 Replication Controller</a></li>
<li><a href="#3-Service" title="3. Service">3. Service</a></li>
<li><a href="#4-Namespace" title="4. Namespace">4. Namespace</a></li>
<li><a href="#5-Imperative-vs-Declarative" title="5. Imperative vs Declarative">5. Imperative vs Declarative</a></li>
</ul>
</li>
<li><a href="#Scheduling" title="Scheduling">Scheduling</a><ul class="nav">
<li><a href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0. Scheduling - nodeName &amp; nodeSelectors">0. Scheduling - nodeName &amp; nodeSelectors</a></li>
<li><a href="#1-Taints-amp-Tolerations" title="1. Taints &amp; Tolerations">1. Taints &amp; Tolerations</a></li>
<li><a href="#2-Affinity-amp-Anti-Affinity" title="2. Affinity &amp; Anti-Affinity">2. Affinity &amp; Anti-Affinity</a></li>
<li><a href="#21-Pod-Affinity" title="2.1 Pod Affinity">2.1 Pod Affinity</a></li>
<li><a href="#3-Kubernetes-Resources---RequestLimit" title="3. Kubernetes Resources - Request/Limit">3. Kubernetes Resources - Request/Limit</a></li>
<li><a href="#4-Demonsets" title="4. Demonsets">4. Demonsets</a></li>
<li><a href="#5-StatefulSet" title="5. StatefulSet">5. StatefulSet</a></li>
<li><a href="#6-Static-pod" title="6. Static pod">6. Static pod</a></li>
</ul>
</li>
<li><a href="#Logging-amp-Monitoring" title="Logging &amp; Monitoring">Logging &amp; Monitoring</a><ul class="nav">
<li><a href="#1-K8s-Monitor-Cluster-Components" title="1. K8s Monitor Cluster Components">1. K8s Monitor Cluster Components</a></li>
<li><a href="#2-Metric-Server" title="2. Metric Server">2. Metric Server</a></li>
</ul>
</li>
<li><a href="#Application-Lifecycle-Managment" title="Application Lifecycle Managment">Application Lifecycle Managment</a><ul class="nav">
<li><a href="#1-Rolling-Update-amp-Rollbacks" title="1. Rolling Update &amp; Rollbacks">1. Rolling Update &amp; Rollbacks</a></li>
<li><a href="#2-Commands-and-Arguments" title="2. Commands and Arguments">2. Commands and Arguments</a></li>
<li><a href="#3-ConfigMap" title="3. ConfigMap">3. ConfigMap</a></li>
<li><a href="#4-Secret" title="4. Secret">4. Secret</a></li>
<li><a href="#5-Multi-container-Pod" title="5. Multi-container Pod">5. Multi-container Pod</a></li>
<li><a href="#6-Init-Container" title="6. Init Container">6. Init Container</a></li>
</ul>
</li>
<li><a href="#Cluster-Maintenance" title="Cluster Maintenance">Cluster Maintenance</a><ul class="nav">
<li><a href="#0-Kubernets-Software-Versions" title="0. Kubernets Software Versions">0. Kubernets Software Versions</a></li>
<li><a href="#1-Cluster-Upgrade" title="1. Cluster Upgrade">1. Cluster Upgrade</a></li>
<li><a href="#11-Upgrading-control-plane-nodes" title="1.1 Upgrading control plane nodes">1.1 Upgrading control plane nodes</a></li>
<li><a href="#12-Upgrade-worker-nodes" title="1.2 Upgrade worker nodes">1.2 Upgrade worker nodes</a></li>
<li><a href="#2-Node-Maintenance" title="2. Node Maintenance">2. Node Maintenance</a></li>
<li><a href="#3-Backup-amp-Restore-Methods" title="3. Backup &amp; Restore Methods">3. Backup &amp; Restore Methods</a></li>
<li><a href="#31-etcdctl-command" title="3.1 etcdctl command">3.1 etcdctl command</a></li>
<li><a href="#32-ETCD-Store" title="3.2 ETCD Store">3.2 ETCD Store</a></li>
<li><a href="#33-ETCD-Restore-snapshot" title="3.3 ETCD Restore snapshot">3.3 ETCD Restore snapshot</a></li>
</ul>
</li>
<li><a href="#Security" title="Security">Security</a><ul class="nav">
<li><a href="#1-Kubernetes-API-Server-Authentication" title="1. Kubernetes API Server Authentication">1. Kubernetes API Server Authentication</a></li>
<li><a href="#11-Server-端證書" title="1.1 Server 端證書">1.1 Server 端證書</a></li>
<li><a href="#12-Client-端證書" title="1.2 Client 端證書">1.2 Client 端證書</a></li>
<li><a href="#2-Authorization" title="2. Authorization">2. Authorization</a></li>
<li><a href="#3-KubeConfig" title="3. KubeConfig">3. KubeConfig</a></li>
<li><a href="#4-Security-Context" title="4. Security Context">4. Security Context</a></li>
<li><a href="#5-Role" title="5. Role">5. Role</a></li>
<li><a href="#51-Binding" title="5.1 Binding">5.1 Binding</a></li>
<li><a href="#52-CertificateSigningRequest" title="5.2 CertificateSigningRequest">5.2 CertificateSigningRequest</a></li>
<li><a href="#6-Service-Account" title="6. Service Account">6. Service Account</a></li>
<li><a href="#7-Image-Sercurity" title="7. Image Sercurity">7. Image Sercurity</a></li>
<li><a href="#8-Network" title="8. Network">8. Network</a></li>
<li><a href="#9-Network-Policies" title="9. Network Policies">9. Network Policies</a></li>
</ul>
</li>
<li><a href="#Storage" title="Storage">Storage</a><ul class="nav">
<li><a href="#1-Volume" title="1. Volume">1. Volume</a></li>
<li><a href="#11-Volume-Type" title="1.1 Volume Type">1.1 Volume Type</a></li>
<li><a href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)">2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</a></li>
<li><a href="#3-Storage-Class" title="3. Storage Class">3. Storage Class</a></li>
</ul>
</li>
<li><a href="#Troubleshooting" title="Troubleshooting">Troubleshooting</a><ul class="nav">
<li><a href="#常見問題" title="常見問題">常見問題</a></li>
<li><a href="#Application-Failure" title="Application Failure">Application Failure</a></li>
<li><a href="#Control-Plane-Failure" title="Control Plane Failure">Control Plane Failure</a></li>
<li><a href="#Worker-Node-Failure" title="Worker Node Failure">Worker Node Failure</a></li>
</ul>
</li>
<li><a href="#Others" title="Others">Others</a><ul class="nav">
<li><a href="#Deplyment" title="Deplyment">Deplyment</a></li>
<li><a href="#22-Volume-Test" title="22. Volume-Test">22. Volume-Test</a></li>
</ul>
</li>
<li><a href="#Mock-Exam" title="Mock Exam">Mock Exam</a></li>
<li><a href="#Mock-Exam---1" title="Mock Exam - 1">Mock Exam - 1</a><ul class="nav">
<li><a href="#Q5" title="Q5">Q5</a></li>
<li><a href="#Q7" title="Q7">Q7</a></li>
<li><a href="#Q8" title="Q8">Q8</a></li>
<li><a href="#Q10" title="Q10">Q10</a></li>
<li><a href="#Q11" title="Q11">Q11</a></li>
<li><a href="#Q12" title="Q12">Q12</a></li>
</ul>
</li>
<li><a href="#Mock-Exam---2" title="Mock Exam - 2">Mock Exam - 2</a><ul class="nav">
<li><a href="#Q3-Security" title="Q3 Security">Q3 Security</a></li>
<li><a href="#Q4-PV-amp-PVC" title="Q4 PV &amp; PVC">Q4 PV &amp; PVC</a></li>
<li><a href="#Q6-CSR---Certificate-Signing-Request" title="Q6 CSR - Certificate Signing Request">Q6 CSR - Certificate Signing Request</a></li>
<li><a href="#Q7-nslookup" title="Q7 nslookup">Q7 nslookup</a></li>
<li><a href="#Q8-Static-pod-on-worker-node" title="Q8 Static pod on worker node">Q8 Static pod on worker node</a></li>
</ul>
</li>
<li><a href="#Mock-Exam---3" title="Mock Exam - 3">Mock Exam - 3</a><ul class="nav">
<li><a href="#Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding" title="Q1 - Service account / Cluster Role &amp; Cluster RoleBinding">Q1 - Service account / Cluster Role &amp; Cluster RoleBinding</a></li>
<li><a href="#Q2---json-path" title="Q2 - json path">Q2 - json path</a></li>
<li><a href="#Q3---Environment-Variables" title="Q3 - Environment Variables">Q3 - Environment Variables</a></li>
<li><a href="#Q4---Security" title="Q4 - Security">Q4 - Security</a></li>
<li><a href="#Q5---NetworkPolicy" title="Q5 - NetworkPolicy">Q5 - NetworkPolicy</a></li>
<li><a href="#Q6---Taint--Toleration" title="Q6 - Taint / Toleration">Q6 - Taint / Toleration</a></li>
<li><a href="#Q7---Namespace" title="Q7 - Namespace">Q7 - Namespace</a></li>
<li><a href="#Q8---Kubeconfig" title="Q8 - Kubeconfig">Q8 - Kubeconfig</a></li>
<li><a href="#Q9---Troubleshooting" title="Q9 - Troubleshooting">Q9 - Troubleshooting</a></li>
</ul>
</li>
<li><a href="#Lightning-Lab" title="Lightning Lab">Lightning Lab</a><ul class="nav">
<li><a href="#Q1-Upgrade-amp-Taint" title="Q1 Upgrade &amp; Taint">Q1 Upgrade &amp; Taint</a></li>
<li><a href="#Q2---json-path1" title="Q2 - json path">Q2 - json path</a></li>
<li><a href="#Q3---KubeConfig-Troubleshoot" title="Q3 - KubeConfig (Troubleshoot)">Q3 - KubeConfig (Troubleshoot)</a></li>
<li><a href="#Q4---Rolling-update" title="Q4 - Rolling update">Q4 - Rolling update</a></li>
<li><a href="#Q5---PVC-Troubleshoot" title="Q5 - PVC (Troubleshoot)">Q5 - PVC (Troubleshoot)</a></li>
<li><a href="#Q6---ETCD-Save" title="Q6 - ETCD Save">Q6 - ETCD Save</a></li>
<li><a href="#Q7---Secret" title="Q7 - Secret">Q7 - Secret</a></li>
</ul>
</li>
<li class=""><a href="#真題" title="真題">真題</a><ul class="nav">
<li><a href="#1-RBAC" title="1. RBAC">1. RBAC</a></li>
<li><a href="#2-CPU" title="2. CPU">2. CPU</a></li>
<li><a href="#3-Networkpolicy" title="3. Networkpolicy">3. Networkpolicy</a></li>
<li><a href="#4-Service" title="4. Service">4. Service</a></li>
<li><a href="#5-Ingress" title="5. Ingress">5. Ingress</a></li>
<li><a href="#6-Deployment-scale" title="6. Deployment scale">6. Deployment scale</a></li>
<li><a href="#7-nodeSelector" title="7. nodeSelector">7. nodeSelector</a></li>
<li><a href="#8-Node-Scheduled" title="8. Node Scheduled">8. Node Scheduled</a></li>
<li><a href="#9-Multi-container" title="9. Multi-container">9. Multi-container</a></li>
<li><a href="#10-PV" title="10. PV">10. PV</a></li>
<li><a href="#11-PVC" title="11. PVC">11. PVC</a></li>
<li><a href="#12-Pod-log" title="12. Pod log">12. Pod log</a></li>
<li><a href="#13-Sidecar" title="13. Sidecar">13. Sidecar</a></li>
<li><a href="#14-Upgrade" title="14. Upgrade">14. Upgrade</a></li>
<li><a href="#15-ETCD-backup-amp-Restore" title="15. ETCD backup &amp; Restore">15. ETCD backup &amp; Restore</a></li>
<li class=""><a href="#16-Troubleshoot" title="16. Troubleshoot">16. Troubleshoot</a></li>
<li><a href="#17-Drain-amp-Cordon" title="17. Drain &amp; Cordon">17. Drain &amp; Cordon</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li class=""><a href="#Kubectl" title="Kubectl">Kubectl</a><ul class="nav">
<li><a href="#Tips" title="Tips">Tips</a><ul class="nav">
<li><a href="#查看-K8s-所有物件和它們的縮寫" title="查看 K8s 所有物件和它們的縮寫">查看 K8s 所有物件和它們的縮寫</a></li>
<li><a href="#Pod-amp-yaml" title="Pod &amp; yaml">Pod &amp; yaml</a></li>
<li><a href="#Deployment" title="Deployment">Deployment</a></li>
<li><a href="#Service" title="Service">Service</a></li>
</ul>
</li>
<li><a href="#Core-Concepts" title="Core Concepts">Core Concepts</a><ul class="nav">
<li><a href="#1-Label" title="1. Label">1. Label</a></li>
<li><a href="#2-Deployment" title="2. Deployment">2. Deployment</a></li>
<li><a href="#21-ReplicaSet-複製集" title="2.1 ReplicaSet 複製集">2.1 ReplicaSet 複製集</a></li>
<li><a href="#22-Replication-Controller" title="2.2 Replication Controller">2.2 Replication Controller</a></li>
<li><a href="#3-Service" title="3. Service">3. Service</a></li>
<li><a href="#4-Namespace" title="4. Namespace">4. Namespace</a></li>
<li><a href="#5-Imperative-vs-Declarative" title="5. Imperative vs Declarative">5. Imperative vs Declarative</a></li>
</ul>
</li>
<li><a href="#Scheduling" title="Scheduling">Scheduling</a><ul class="nav">
<li><a href="#0-Scheduling---nodeName-amp-nodeSelectors" title="0. Scheduling - nodeName &amp; nodeSelectors">0. Scheduling - nodeName &amp; nodeSelectors</a></li>
<li><a href="#1-Taints-amp-Tolerations" title="1. Taints &amp; Tolerations">1. Taints &amp; Tolerations</a></li>
<li><a href="#2-Affinity-amp-Anti-Affinity" title="2. Affinity &amp; Anti-Affinity">2. Affinity &amp; Anti-Affinity</a></li>
<li><a href="#21-Pod-Affinity" title="2.1 Pod Affinity">2.1 Pod Affinity</a></li>
<li><a href="#3-Kubernetes-Resources---RequestLimit" title="3. Kubernetes Resources - Request/Limit">3. Kubernetes Resources - Request/Limit</a></li>
<li><a href="#4-Demonsets" title="4. Demonsets">4. Demonsets</a></li>
<li><a href="#5-StatefulSet" title="5. StatefulSet">5. StatefulSet</a></li>
<li><a href="#6-Static-pod" title="6. Static pod">6. Static pod</a></li>
</ul>
</li>
<li><a href="#Logging-amp-Monitoring" title="Logging &amp; Monitoring">Logging &amp; Monitoring</a><ul class="nav">
<li><a href="#1-K8s-Monitor-Cluster-Components" title="1. K8s Monitor Cluster Components">1. K8s Monitor Cluster Components</a></li>
<li><a href="#2-Metric-Server" title="2. Metric Server">2. Metric Server</a></li>
</ul>
</li>
<li><a href="#Application-Lifecycle-Managment" title="Application Lifecycle Managment">Application Lifecycle Managment</a><ul class="nav">
<li><a href="#1-Rolling-Update-amp-Rollbacks" title="1. Rolling Update &amp; Rollbacks">1. Rolling Update &amp; Rollbacks</a></li>
<li><a href="#2-Commands-and-Arguments" title="2. Commands and Arguments">2. Commands and Arguments</a></li>
<li><a href="#3-ConfigMap" title="3. ConfigMap">3. ConfigMap</a></li>
<li><a href="#4-Secret" title="4. Secret">4. Secret</a></li>
<li><a href="#5-Multi-container-Pod" title="5. Multi-container Pod">5. Multi-container Pod</a></li>
<li><a href="#6-Init-Container" title="6. Init Container">6. Init Container</a></li>
</ul>
</li>
<li><a href="#Cluster-Maintenance" title="Cluster Maintenance">Cluster Maintenance</a><ul class="nav">
<li><a href="#0-Kubernets-Software-Versions" title="0. Kubernets Software Versions">0. Kubernets Software Versions</a></li>
<li><a href="#1-Cluster-Upgrade" title="1. Cluster Upgrade">1. Cluster Upgrade</a></li>
<li><a href="#11-Upgrading-control-plane-nodes" title="1.1 Upgrading control plane nodes">1.1 Upgrading control plane nodes</a></li>
<li><a href="#12-Upgrade-worker-nodes" title="1.2 Upgrade worker nodes">1.2 Upgrade worker nodes</a></li>
<li><a href="#2-Node-Maintenance" title="2. Node Maintenance">2. Node Maintenance</a></li>
<li><a href="#3-Backup-amp-Restore-Methods" title="3. Backup &amp; Restore Methods">3. Backup &amp; Restore Methods</a></li>
<li><a href="#31-etcdctl-command" title="3.1 etcdctl command">3.1 etcdctl command</a></li>
<li><a href="#32-ETCD-Store" title="3.2 ETCD Store">3.2 ETCD Store</a></li>
<li><a href="#33-ETCD-Restore-snapshot" title="3.3 ETCD Restore snapshot">3.3 ETCD Restore snapshot</a></li>
</ul>
</li>
<li><a href="#Security" title="Security">Security</a><ul class="nav">
<li><a href="#1-Kubernetes-API-Server-Authentication" title="1. Kubernetes API Server Authentication">1. Kubernetes API Server Authentication</a></li>
<li><a href="#11-Server-端證書" title="1.1 Server 端證書">1.1 Server 端證書</a></li>
<li><a href="#12-Client-端證書" title="1.2 Client 端證書">1.2 Client 端證書</a></li>
<li><a href="#2-Authorization" title="2. Authorization">2. Authorization</a></li>
<li><a href="#3-KubeConfig" title="3. KubeConfig">3. KubeConfig</a></li>
<li><a href="#4-Security-Context" title="4. Security Context">4. Security Context</a></li>
<li><a href="#5-Role" title="5. Role">5. Role</a></li>
<li><a href="#51-Binding" title="5.1 Binding">5.1 Binding</a></li>
<li><a href="#52-CertificateSigningRequest" title="5.2 CertificateSigningRequest">5.2 CertificateSigningRequest</a></li>
<li><a href="#6-Service-Account" title="6. Service Account">6. Service Account</a></li>
<li><a href="#7-Image-Sercurity" title="7. Image Sercurity">7. Image Sercurity</a></li>
<li><a href="#8-Network" title="8. Network">8. Network</a></li>
<li><a href="#9-Network-Policies" title="9. Network Policies">9. Network Policies</a></li>
</ul>
</li>
<li><a href="#Storage" title="Storage">Storage</a><ul class="nav">
<li><a href="#1-Volume" title="1. Volume">1. Volume</a></li>
<li><a href="#11-Volume-Type" title="1.1 Volume Type">1.1 Volume Type</a></li>
<li><a href="#2-PersistentVolumePV-amp-Persistent-Volume-Claim-PVC" title="2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)">2. PersistentVolume(PV) &amp; Persistent Volume Claim (PVC)</a></li>
<li><a href="#3-Storage-Class" title="3. Storage Class">3. Storage Class</a></li>
</ul>
</li>
<li><a href="#Troubleshooting" title="Troubleshooting">Troubleshooting</a><ul class="nav">
<li><a href="#常見問題" title="常見問題">常見問題</a></li>
<li><a href="#Application-Failure" title="Application Failure">Application Failure</a></li>
<li><a href="#Control-Plane-Failure" title="Control Plane Failure">Control Plane Failure</a></li>
<li><a href="#Worker-Node-Failure" title="Worker Node Failure">Worker Node Failure</a></li>
</ul>
</li>
<li><a href="#Others" title="Others">Others</a><ul class="nav">
<li><a href="#Deplyment" title="Deplyment">Deplyment</a></li>
<li><a href="#22-Volume-Test" title="22. Volume-Test">22. Volume-Test</a></li>
</ul>
</li>
<li><a href="#Mock-Exam" title="Mock Exam">Mock Exam</a></li>
<li><a href="#Mock-Exam---1" title="Mock Exam - 1">Mock Exam - 1</a><ul class="nav">
<li><a href="#Q5" title="Q5">Q5</a></li>
<li><a href="#Q7" title="Q7">Q7</a></li>
<li><a href="#Q8" title="Q8">Q8</a></li>
<li><a href="#Q10" title="Q10">Q10</a></li>
<li><a href="#Q11" title="Q11">Q11</a></li>
<li><a href="#Q12" title="Q12">Q12</a></li>
</ul>
</li>
<li><a href="#Mock-Exam---2" title="Mock Exam - 2">Mock Exam - 2</a><ul class="nav">
<li><a href="#Q3-Security" title="Q3 Security">Q3 Security</a></li>
<li><a href="#Q4-PV-amp-PVC" title="Q4 PV &amp; PVC">Q4 PV &amp; PVC</a></li>
<li><a href="#Q6-CSR---Certificate-Signing-Request" title="Q6 CSR - Certificate Signing Request">Q6 CSR - Certificate Signing Request</a></li>
<li><a href="#Q7-nslookup" title="Q7 nslookup">Q7 nslookup</a></li>
<li><a href="#Q8-Static-pod-on-worker-node" title="Q8 Static pod on worker node">Q8 Static pod on worker node</a></li>
</ul>
</li>
<li><a href="#Mock-Exam---3" title="Mock Exam - 3">Mock Exam - 3</a><ul class="nav">
<li><a href="#Q1---Service-account--Cluster-Role-amp-Cluster-RoleBinding" title="Q1 - Service account / Cluster Role &amp; Cluster RoleBinding">Q1 - Service account / Cluster Role &amp; Cluster RoleBinding</a></li>
<li><a href="#Q2---json-path" title="Q2 - json path">Q2 - json path</a></li>
<li><a href="#Q3---Environment-Variables" title="Q3 - Environment Variables">Q3 - Environment Variables</a></li>
<li><a href="#Q4---Security" title="Q4 - Security">Q4 - Security</a></li>
<li><a href="#Q5---NetworkPolicy" title="Q5 - NetworkPolicy">Q5 - NetworkPolicy</a></li>
<li><a href="#Q6---Taint--Toleration" title="Q6 - Taint / Toleration">Q6 - Taint / Toleration</a></li>
<li><a href="#Q7---Namespace" title="Q7 - Namespace">Q7 - Namespace</a></li>
<li><a href="#Q8---Kubeconfig" title="Q8 - Kubeconfig">Q8 - Kubeconfig</a></li>
<li><a href="#Q9---Troubleshooting" title="Q9 - Troubleshooting">Q9 - Troubleshooting</a></li>
</ul>
</li>
<li><a href="#Lightning-Lab" title="Lightning Lab">Lightning Lab</a><ul class="nav">
<li><a href="#Q1-Upgrade-amp-Taint" title="Q1 Upgrade &amp; Taint">Q1 Upgrade &amp; Taint</a></li>
<li><a href="#Q2---json-path1" title="Q2 - json path">Q2 - json path</a></li>
<li><a href="#Q3---KubeConfig-Troubleshoot" title="Q3 - KubeConfig (Troubleshoot)">Q3 - KubeConfig (Troubleshoot)</a></li>
<li><a href="#Q4---Rolling-update" title="Q4 - Rolling update">Q4 - Rolling update</a></li>
<li><a href="#Q5---PVC-Troubleshoot" title="Q5 - PVC (Troubleshoot)">Q5 - PVC (Troubleshoot)</a></li>
<li><a href="#Q6---ETCD-Save" title="Q6 - ETCD Save">Q6 - ETCD Save</a></li>
<li><a href="#Q7---Secret" title="Q7 - Secret">Q7 - Secret</a></li>
</ul>
</li>
<li class=""><a href="#真題" title="真題">真題</a><ul class="nav">
<li><a href="#1-RBAC" title="1. RBAC">1. RBAC</a></li>
<li><a href="#2-CPU" title="2. CPU">2. CPU</a></li>
<li><a href="#3-Networkpolicy" title="3. Networkpolicy">3. Networkpolicy</a></li>
<li><a href="#4-Service" title="4. Service">4. Service</a></li>
<li><a href="#5-Ingress" title="5. Ingress">5. Ingress</a></li>
<li><a href="#6-Deployment-scale" title="6. Deployment scale">6. Deployment scale</a></li>
<li><a href="#7-nodeSelector" title="7. nodeSelector">7. nodeSelector</a></li>
<li><a href="#8-Node-Scheduled" title="8. Node Scheduled">8. Node Scheduled</a></li>
<li><a href="#9-Multi-container" title="9. Multi-container">9. Multi-container</a></li>
<li><a href="#10-PV" title="10. PV">10. PV</a></li>
<li><a href="#11-PVC" title="11. PVC">11. PVC</a></li>
<li><a href="#12-Pod-log" title="12. Pod log">12. Pod log</a></li>
<li><a href="#13-Sidecar" title="13. Sidecar">13. Sidecar</a></li>
<li><a href="#14-Upgrade" title="14. Upgrade">14. Upgrade</a></li>
<li><a href="#15-ETCD-backup-amp-Restore" title="15. ETCD backup &amp; Restore">15. ETCD backup &amp; Restore</a></li>
<li class=""><a href="#16-Troubleshoot" title="16. Troubleshoot">16. Troubleshoot</a></li>
<li><a href="#17-Drain-amp-Cordon" title="17. Drain &amp; Cordon">17. Drain &amp; Cordon</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
